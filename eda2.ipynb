{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Environment checks\n",
    "print(f\"Python executable: {sys.executable}\")  \n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"\\nFiles in data directory:\", os.listdir('data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions for Data Loading and Cleaning\n",
    "def load_and_clean_shifts(df):\n",
    "    \"\"\"\n",
    "    Load and clean shifts dataset\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Raw shifts dataframe\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned shifts dataframe with proper datatypes\n",
    "        \n",
    "    Notes:\n",
    "        - Makes a copy to avoid modifying original data\n",
    "        - Converts datetime columns\n",
    "        - Handles potential errors in datetime conversion\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert datetime columns with error handling\n",
    "    datetime_cols = ['Start', 'End', 'Created At']\n",
    "    for col in datetime_cols:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col], format='mixed')\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {col} to datetime: {str(e)}\")\n",
    "                # Log problematic rows for investigation\n",
    "                problematic_rows = df[pd.to_datetime(df[col], format='mixed', errors='coerce').isna()]\n",
    "                if not problematic_rows.empty:\n",
    "                    print(f\"Problematic rows in {col}:\")\n",
    "                    print(problematic_rows[col].head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_and_clean_bookings(df):\n",
    "    \"\"\"\n",
    "    Load and clean booking logs dataset\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Raw bookings dataframe\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned bookings dataframe with proper datatypes\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Convert datetime columns\n",
    "    try:\n",
    "        df['Created At'] = pd.to_datetime(df['Created At'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting Created At: {str(e)}\")\n",
    "    return df\n",
    "\n",
    "def load_and_clean_cancellations(df):\n",
    "    \"\"\"\n",
    "    Load and clean cancellation logs dataset\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Raw cancellations dataframe\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned cancellations dataframe with proper datatypes\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Convert datetime columns with flexible parsing\n",
    "    try:\n",
    "        df['Created At'] = pd.to_datetime(df['Created At'], format='mixed')\n",
    "        df['Shift Start Logs'] = pd.to_datetime(df['Shift Start Logs'], format='mixed')\n",
    "    except Exception as e:\n",
    "        print(f\"Error in datetime conversion: {str(e)}\")\n",
    "        # Try to identify problematic rows\n",
    "        prob_rows = df[pd.to_datetime(df['Shift Start Logs'], format='mixed', errors='coerce').isna()]\n",
    "        if not prob_rows.empty:\n",
    "            print(\"\\nSample of problematic date formats:\")\n",
    "            print(prob_rows['Shift Start Logs'].head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def categorize_lead_time(hours):\n",
    "    \"\"\"\n",
    "    Categorize lead times based on business rules.\n",
    "    \n",
    "    Parameters:\n",
    "        hours (float): Lead time in hours\n",
    "        \n",
    "    Returns:\n",
    "        str: Category of lead time\n",
    "    \"\"\"\n",
    "    if hours < 0:\n",
    "        return 'No-Show'  # Cancelled after shift start\n",
    "    elif hours < 4:\n",
    "        return 'Late (<4hrs)'\n",
    "    elif hours < 24:\n",
    "        return 'Same Day'\n",
    "    elif hours < 72:\n",
    "        return 'Advance (<3 days)'\n",
    "    return 'Early (3+ days)'\n",
    "\n",
    "def clean_lead_times(cancellations_df):\n",
    "    \"\"\"\n",
    "    Clean and categorize lead times in cancellation data\n",
    "    \n",
    "    Parameters:\n",
    "        cancellations_df (pd.DataFrame): Raw cancellations dataframe\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned cancellations data with categorized lead times\n",
    "        pd.Series: Statistics about removed records for quality control\n",
    "    \"\"\"\n",
    "    df = cancellations_df.copy()\n",
    "    \n",
    "    # Track data quality issues\n",
    "    quality_stats = {\n",
    "        'original_rows': len(df),\n",
    "        'null_lead_times': df['Lead Time'].isnull().sum(),\n",
    "        'infinite_values': (~np.isfinite(df['Lead Time'])).sum()\n",
    "    }\n",
    "    \n",
    "    # Only remove truly invalid data\n",
    "    mask = df['Lead Time'].notnull() & np.isfinite(df['Lead Time'])\n",
    "    df = df[mask]\n",
    "    \n",
    "    # Add cleaned lead time without filtering extremes\n",
    "    df['clean_lead_time'] = df['Lead Time']\n",
    "    \n",
    "    # Categorize all lead times\n",
    "    df['cancellation_category'] = df['clean_lead_time'].apply(categorize_lead_time)\n",
    "    \n",
    "    # Add flags for extreme values for analysis\n",
    "    df['is_extreme_negative'] = df['Lead Time'] < -72  # Flag cancellations >3 days after\n",
    "    df['is_extreme_positive'] = df['Lead Time'] > 1000 # Flag cancellations >41 days before\n",
    "    \n",
    "    quality_stats['final_rows'] = len(df)\n",
    "    quality_stats['removed_rows'] = quality_stats['original_rows'] - quality_stats['final_rows']\n",
    "    \n",
    "    return df, pd.Series(quality_stats)\n",
    "\n",
    "# Data Summary Storage Class\n",
    "class DataSummary:\n",
    "    \"\"\"Class to store and manage analysis results\"\"\"\n",
    "    def __init__(self):\n",
    "        self.summaries = {}\n",
    "    \n",
    "    def add_summary(self, dataset_name, summary_type, data):\n",
    "        \"\"\"Add summary statistics to storage\"\"\"\n",
    "        if dataset_name not in self.summaries:\n",
    "            self.summaries[dataset_name] = {}\n",
    "        self.summaries[dataset_name][summary_type] = data\n",
    "    \n",
    "    def get_summary(self, dataset_name, summary_type=None):\n",
    "        \"\"\"Retrieve stored summary statistics\"\"\"\n",
    "        if summary_type:\n",
    "            return self.summaries.get(dataset_name, {}).get(summary_type)\n",
    "        return self.summaries.get(dataset_name)\n",
    "    \n",
    "    def print_summary(self, dataset_name):\n",
    "        \"\"\"Print stored summaries for a dataset\"\"\"\n",
    "        if dataset_name in self.summaries:\n",
    "            print(f\"\\nSummary for {dataset_name}:\")\n",
    "            for summary_type, data in self.summaries[dataset_name].items():\n",
    "                print(f\"\\n{summary_type}:\")\n",
    "                print(data)\n",
    "\n",
    "# Initialize summary storage\n",
    "summary = DataSummary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare shifts data\n",
    "print(\"Loading shifts data...\")\n",
    "shifts_df = pd.read_csv('data/cleveland_shifts_large.csv')\n",
    "shifts_df = load_and_clean_shifts(shifts_df)\n",
    "\n",
    "# Initial data exploration\n",
    "print(\"\\n=== Initial Data Overview ===\")\n",
    "print(\"Dataset Shape:\", shifts_df.shape)\n",
    "print(\"\\nColumns:\", shifts_df.columns.tolist())\n",
    "print(\"\\nData Types:\\n\", shifts_df.dtypes)\n",
    "\n",
    "# Missing value analysis\n",
    "missing_values = shifts_df.isnull().sum()\n",
    "print(\"\\nMissing Values:\\n\", missing_values)\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(shifts_df.head())\n",
    "\n",
    "# Store initial findings\n",
    "summary.add_summary('shifts', 'shape', shifts_df.shape)\n",
    "summary.add_summary('shifts', 'dtypes', shifts_df.dtypes)\n",
    "summary.add_summary('shifts', 'missing_values', missing_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare cancellations data\n",
    "print(\"Loading cancellations data...\")\n",
    "cancellations_df = pd.read_csv('data/cancel_logs_large.csv')\n",
    "cancellations_df = load_and_clean_cancellations(cancellations_df)\n",
    "\n",
    "# Clean lead times and get quality stats\n",
    "clean_cancellations, quality_stats = clean_lead_times(cancellations_df)\n",
    "\n",
    "print(\"=== Data Quality Statistics ===\")\n",
    "print(quality_stats)\n",
    "\n",
    "print(\"\\n=== Cancellation Categories ===\")\n",
    "print(clean_cancellations['cancellation_category'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n=== Extreme Values Analysis ===\")\n",
    "print(f\"Very Late Cancellations (>3 days after): {clean_cancellations['is_extreme_negative'].sum()}\")\n",
    "print(f\"Very Early Cancellations (>41 days before): {clean_cancellations['is_extreme_positive'].sum()}\")\n",
    "\n",
    "# Distribution of lead times for extreme cases\n",
    "if clean_cancellations['is_extreme_negative'].any():\n",
    "    print(\"\\nVery Late Cancellation Stats:\")\n",
    "    print(clean_cancellations[clean_cancellations['is_extreme_negative']]['Lead Time'].describe())\n",
    "\n",
    "if clean_cancellations['is_extreme_positive'].any():\n",
    "    print(\"\\nVery Early Cancellation Stats:\")\n",
    "    print(clean_cancellations[clean_cancellations['is_extreme_positive']]['Lead Time'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Numerical Analysis ===\n",
    "print(\"\\n=== Numerical Analysis ===\")\n",
    "# Basic statistics for numerical columns\n",
    "numeric_stats = shifts_df[['Charge', 'Time']].describe()\n",
    "print(\"\\nNumerical Statistics:\")\n",
    "print(numeric_stats)\n",
    "\n",
    "# Additional numeric insights\n",
    "print(\"\\nCharge Rate Analysis:\")\n",
    "print(f\"Shifts with zero charge: {(shifts_df['Charge'] == 0).sum()}\")\n",
    "print(f\"Average charge by agent type:\")\n",
    "print(shifts_df.groupby('Agent Req')['Charge'].mean().round(2))\n",
    "\n",
    "# === Categorical Analysis ===\n",
    "print(\"\\n=== Categorical Analysis ===\")\n",
    "# Shift type distribution\n",
    "print(\"\\nShift Type Distribution:\")\n",
    "shift_type_dist = shifts_df['Shift Type'].value_counts(dropna=True)\n",
    "print(shift_type_dist)\n",
    "\n",
    "# Agent requirements\n",
    "print(\"\\nAgent Requirement Distribution:\")\n",
    "agent_req_dist = shifts_df['Agent Req'].value_counts(dropna=True)\n",
    "print(agent_req_dist)\n",
    "\n",
    "# Cross-tabulation of shift types and agent requirements\n",
    "print(\"\\nShift Types by Agent Requirements:\")\n",
    "print(pd.crosstab(shifts_df['Shift Type'], shifts_df['Agent Req']))\n",
    "\n",
    "# === Data Completeness Analysis ===\n",
    "print(\"\\n=== Data Completeness Analysis ===\")\n",
    "complete_rows = shifts_df.dropna().shape[0]\n",
    "print(f\"Complete rows: {complete_rows} out of {shifts_df.shape[0]}\")\n",
    "print(f\"Completion rate: {(complete_rows/shifts_df.shape[0]*100):.2f}%\")\n",
    "\n",
    "# === Time-Based Analysis ===\n",
    "print(\"\\n=== Time-Based Analysis ===\")\n",
    "# Extract time components\n",
    "shifts_df['Hour'] = shifts_df['Start'].dt.hour\n",
    "shifts_df['Day'] = shifts_df['Start'].dt.day_name()\n",
    "shifts_df['Month'] = shifts_df['Start'].dt.month\n",
    "shifts_df['Shift_Length'] = (shifts_df['End'] - shifts_df['Start']).dt.total_seconds() / 3600\n",
    "# Time patterns\n",
    "print(\"\\nShifts by Hour:\")\n",
    "hour_dist = shifts_df['Hour'].value_counts().sort_index()\n",
    "print(hour_dist)\n",
    "\n",
    "print(\"\\nShifts by Day of Week:\")\n",
    "day_dist = shifts_df['Day'].value_counts()\n",
    "print(day_dist)\n",
    "\n",
    "print(\"\\nShift Length Distribution:\")\n",
    "print(shifts_df['Shift_Length'].describe().round(2))\n",
    "\n",
    "# === Facility Analysis ===\n",
    "print(\"\\n=== Facility Analysis ===\")\n",
    "facility_stats = shifts_df.groupby('Facility ID').agg({\n",
    "    'ID': 'count',\n",
    "    'Charge': 'mean',\n",
    "    'Time': 'mean'\n",
    "}).rename(columns={\n",
    "    'ID': 'Number of Shifts',\n",
    "    'Charge': 'Average Charge',\n",
    "    'Time': 'Average Shift Length'\n",
    "})\n",
    "print(\"\\nFacility Statistics:\")\n",
    "print(facility_stats.head())\n",
    "print(f\"\\nTotal unique facilities: {shifts_df['Facility ID'].nunique()}\")\n",
    "\n",
    "# Store all results\n",
    "summary.add_summary('shifts', 'numeric_stats', numeric_stats)\n",
    "summary.add_summary('shifts', 'shift_types', shift_type_dist.to_dict())\n",
    "summary.add_summary('shifts', 'agent_types', agent_req_dist.to_dict())\n",
    "summary.add_summary('shifts', 'hour_distribution', hour_dist.to_dict())\n",
    "summary.add_summary('shifts', 'day_distribution', day_dist.to_dict())\n",
    "summary.add_summary('shifts', 'facility_stats', facility_stats.to_dict())\n",
    "\n",
    "# Optional: Create visualizations\n",
    "# We can add matplotlib/seaborn plots here if you'd like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's fix the data loading to ensure consistency\n",
    "# Load the full datasets\n",
    "shifts_df = pd.read_csv('data/cleveland_shifts_large.csv')\n",
    "bookings_df = pd.read_csv('data/booking_logs_large.csv')\n",
    "cancellations_df = pd.read_csv('data/cancel_logs_large.csv')\n",
    "\n",
    "# Clean and prepare the data\n",
    "shifts_df = load_and_clean_shifts(shifts_df)\n",
    "bookings_df = load_and_clean_bookings(bookings_df)\n",
    "cancellations_df = load_and_clean_cancellations(cancellations_df)\n",
    "\n",
    "# Basic validation\n",
    "for name, df in [('Shifts', shifts_df), ('Bookings', bookings_df), ('Cancellations', cancellations_df)]:\n",
    "    print(f\"\\n=== {name} Dataset ===\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Date Range: {pd.to_datetime(df['Created At']).min()} to {pd.to_datetime(df['Created At']).max()}\")\n",
    "    print(\"\\nSample of unique IDs:\")\n",
    "    print(df['ID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship analysis\n",
    "# First, let's see how many shifts had cancellations\n",
    "shifts_with_cancellations = len(set(shifts_df['ID']) & set(cancellations_df['Shift ID']))\n",
    "print(f\"Shifts with cancellations: {shifts_with_cancellations}\")\n",
    "print(f\"Percentage of shifts cancelled: {(shifts_with_cancellations/len(shifts_df))*100:.2f}%\")\n",
    "\n",
    "# Analyze cancellation lead times\n",
    "cancellations_df['Lead Time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze lead times\n",
    "# Remove implausible lead times\n",
    "clean_cancellations = cancellations_df[cancellations_df['Lead Time'] > 0]\n",
    "\n",
    "\n",
    "clean_cancellations['cancellation_category'] = clean_cancellations['Lead Time'].apply(categorize_lead_time)\n",
    "\n",
    "# Analyze patterns\n",
    "cancellation_distribution = clean_cancellations['cancellation_category'].value_counts()\n",
    "print(\"\\nCancellation Distribution by Lead Time:\")\n",
    "print(cancellation_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cancellation types\n",
    "cancellation_types = cancellations_df['Action'].value_counts()\n",
    "print(\"\\nCancellation Types:\")\n",
    "print(cancellation_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Quality Assessment & Cleaning ===\n",
    "def clean_lead_times(cancellations_df):\n",
    "    \"\"\"\n",
    "    Clean and categorize lead times in cancellation data\n",
    "    \n",
    "    Parameters:\n",
    "        cancellations_df (pd.DataFrame): Raw cancellations dataframe\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned cancellations data with categorized lead times\n",
    "        pd.Series: Statistics about removed records for quality control\n",
    "    \"\"\"\n",
    "    df = cancellations_df.copy()\n",
    "    \n",
    "    # Track data quality issues\n",
    "    quality_stats = {\n",
    "        'original_rows': len(df),\n",
    "        'null_lead_times': df['Lead Time'].isnull().sum(),\n",
    "        'infinite_values': (~np.isfinite(df['Lead Time'])).sum()\n",
    "    }\n",
    "    \n",
    "    # Only remove truly invalid data\n",
    "    mask = df['Lead Time'].notnull() & np.isfinite(df['Lead Time'])\n",
    "    df = df[mask]\n",
    "    \n",
    "    # Add cleaned lead time without filtering extremes\n",
    "    df['clean_lead_time'] = df['Lead Time']\n",
    "    \n",
    "    # Categorize lead times including all valid values\n",
    "    df['cancellation_category'] = df['clean_lead_time'].apply(categorize_lead_time)\n",
    "    \n",
    "    # Add flags for extreme values for analysis\n",
    "    df['is_extreme_negative'] = df['Lead Time'] < -72  # Flag cancellations >3 days after shift\n",
    "    df['is_extreme_positive'] = df['Lead Time'] > 1000 # Flag cancellations >41 days before\n",
    "    \n",
    "    quality_stats['final_rows'] = len(df)\n",
    "    quality_stats['removed_rows'] = quality_stats['original_rows'] - quality_stats['final_rows']\n",
    "    \n",
    "    return df, pd.Series(quality_stats)\n",
    "\n",
    "# Apply cleaning and show results\n",
    "clean_cancellations, quality_stats = clean_lead_times(cancellations_df)\n",
    "\n",
    "print(\"=== Data Quality Statistics ===\")\n",
    "print(quality_stats)\n",
    "\n",
    "print(\"\\n=== Cancellation Categories ===\")\n",
    "print(clean_cancellations['cancellation_category'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n=== Extreme Values Analysis ===\")\n",
    "print(f\"Very Late Cancellations (>3 days after): {clean_cancellations['is_extreme_negative'].sum()}\")\n",
    "print(f\"Very Early Cancellations (>41 days before): {clean_cancellations['is_extreme_positive'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Detailed Cancellation Analysis ===\n",
    "def analyze_cancellation_patterns(clean_cancellations, shifts_df):\n",
    "    \"\"\"\n",
    "    Analyze patterns in cancellations, including:\n",
    "    - Types of cancellations (NCNS vs Regular)\n",
    "    - Timing patterns\n",
    "    - Role-based patterns\n",
    "    \n",
    "    Parameters:\n",
    "        clean_cancellations (pd.DataFrame): Cleaned cancellations data\n",
    "        shifts_df (pd.DataFrame): Shifts data for cross-reference\n",
    "    \"\"\"\n",
    "    print(\"=== Cancellation Action Analysis ===\")\n",
    "    action_counts = clean_cancellations['Action'].value_counts()\n",
    "    print(\"\\nCancellation Types:\")\n",
    "    print(action_counts)\n",
    "    print(f\"\\nNo-Call-No-Show Rate: {(action_counts.get('NO_CALL_NO_SHOW', 0)/len(clean_cancellations))*100:.2f}%\")\n",
    "    \n",
    "    # Merge with shifts to analyze by role\n",
    "    cancellations_with_shifts = pd.merge(\n",
    "        clean_cancellations,\n",
    "        shifts_df[['ID', 'Agent Req', 'Shift Type', 'Charge']],\n",
    "        left_on='Shift ID',\n",
    "        right_on='ID',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Cancellations by Role ===\")\n",
    "    role_cancels = pd.crosstab(\n",
    "        cancellations_with_shifts['Agent Req'],\n",
    "        cancellations_with_shifts['cancellation_category'],\n",
    "        normalize='index'\n",
    "    ).round(3) * 100\n",
    "    print(role_cancels)\n",
    "    \n",
    "    print(\"\\n=== Cancellations by Shift Type ===\")\n",
    "    shift_cancels = pd.crosstab(\n",
    "        cancellations_with_shifts['Shift Type'],\n",
    "        cancellations_with_shifts['cancellation_category'],\n",
    "        normalize='index'\n",
    "    ).round(3) * 100\n",
    "    print(shift_cancels)\n",
    "    \n",
    "    # Store results in summary\n",
    "    summary.add_summary('cancellations', 'action_types', action_counts.to_dict())\n",
    "    summary.add_summary('cancellations', 'role_patterns', role_cancels.to_dict())\n",
    "    summary.add_summary('cancellations', 'shift_patterns', shift_cancels.to_dict())\n",
    "    \n",
    "    return cancellations_with_shifts\n",
    "\n",
    "# Run the analysis\n",
    "cancellations_with_shifts = analyze_cancellation_patterns(clean_cancellations, shifts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Booking Pattern Analysis ===\n",
    "def analyze_booking_patterns(bookings_df, shifts_df, clean_cancellations):\n",
    "    \"\"\"\n",
    "    Analyze patterns in shift bookings, including:\n",
    "    - Time from posting to booking\n",
    "    - Successful vs cancelled bookings\n",
    "    - Rebooking patterns after cancellations\n",
    "    \n",
    "    Parameters:\n",
    "        bookings_df (pd.DataFrame): Booking logs data\n",
    "        shifts_df (pd.DataFrame): Shifts data\n",
    "        clean_cancellations (pd.DataFrame): Cleaned cancellations data\n",
    "    \"\"\"\n",
    "    print(\"=== Booking Success Analysis ===\")\n",
    "\n",
    "    # Calculate time to fill (from shift creation to booking)\n",
    "    bookings_with_shifts = pd.merge(\n",
    "        bookings_df,\n",
    "        shifts_df[['ID', 'Created At', 'Agent Req', 'Shift Type', 'Charge']],\n",
    "        left_on='Shift ID',\n",
    "        right_on='ID',\n",
    "        how='left',\n",
    "        suffixes=('_booking', '_shift')\n",
    "    )\n",
    "    \n",
    "    bookings_with_shifts['time_to_fill'] = (\n",
    "        pd.to_datetime(bookings_with_shifts['Created At_booking']) - \n",
    "        pd.to_datetime(bookings_with_shifts['Created At_shift'])\n",
    "    ).dt.total_seconds() / 3600  # Convert to hours\n",
    "    \n",
    "    print(\"\\nTime to Fill Statistics (hours):\")\n",
    "    print(bookings_with_shifts['time_to_fill'].describe().round(2))\n",
    "    \n",
    "    # Analyze bookings by role and shift type\n",
    "    print(\"\\n=== Bookings by Role ===\")\n",
    "    role_bookings = bookings_with_shifts.groupby('Agent Req').agg({\n",
    "        'Shift ID': 'count',  # Changed from 'ID' to 'Shift ID'\n",
    "        'time_to_fill': 'mean',\n",
    "        'Charge': 'mean'\n",
    "    }).round(2)\n",
    "    role_bookings.columns = ['Number of Bookings', 'Avg Time to Fill', 'Avg Charge']\n",
    "    print(role_bookings)\n",
    "    \n",
    "    # Look at shifts that got cancelled and rebooked\n",
    "    rebooked_cancellations = clean_cancellations['Shift ID'].value_counts()\n",
    "    \n",
    "    print(\"\\n=== Rebooking Analysis ===\")\n",
    "    print(f\"Shifts cancelled multiple times: {(rebooked_cancellations > 1).sum()}\")\n",
    "    print(f\"Maximum cancellations for a single shift: {rebooked_cancellations.max()}\")\n",
    "    \n",
    "    # Additional timing analysis\n",
    "    print(\"\\n=== Booking Time Patterns ===\")\n",
    "    bookings_with_shifts['booking_hour'] = pd.to_datetime(bookings_with_shifts['Created At_booking']).dt.hour\n",
    "    bookings_with_shifts['booking_day'] = pd.to_datetime(bookings_with_shifts['Created At_booking']).dt.day_name()\n",
    "    \n",
    "    print(\"\\nBookings by Hour of Day:\")\n",
    "    print(bookings_with_shifts['booking_hour'].value_counts().sort_index())\n",
    "    \n",
    "    print(\"\\nBookings by Day of Week:\")\n",
    "    print(bookings_with_shifts['booking_day'].value_counts())\n",
    "    \n",
    "    # Store results\n",
    "    summary.add_summary('bookings', 'time_to_fill', \n",
    "                       bookings_with_shifts['time_to_fill'].describe().to_dict())\n",
    "    summary.add_summary('bookings', 'role_patterns', role_bookings.to_dict())\n",
    "    summary.add_summary('bookings', 'rebooking_stats', {\n",
    "        'multiple_cancellations': (rebooked_cancellations > 1).sum(),\n",
    "        'max_cancellations': rebooked_cancellations.max()\n",
    "    })\n",
    "    \n",
    "    return bookings_with_shifts\n",
    "\n",
    "# Run the analysis\n",
    "bookings_with_shifts = analyze_booking_patterns(bookings_df, shifts_df, clean_cancellations)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Economic Impact Analysis ===\n",
    "def analyze_economic_impact(shifts_df, cancellations_with_shifts):\n",
    "    \"\"\"\n",
    "    Analyze economic impact of cancellations, including:\n",
    "    - Revenue loss from cancellations\n",
    "    - Impact by facility and role type\n",
    "    - Patterns in high-cost cancellations\n",
    "    \"\"\"\n",
    "    print(\"=== Economic Impact Analysis ===\")\n",
    "    \n",
    "    # First, ensure we have all needed columns by merging with shifts data if needed\n",
    "    if 'Time' not in cancellations_with_shifts.columns:\n",
    "        cancellations_with_shifts = pd.merge(\n",
    "            cancellations_with_shifts,\n",
    "            shifts_df[['ID', 'Time', 'Charge']],\n",
    "            left_on='Shift ID',\n",
    "            right_on='ID',\n",
    "            how='left',\n",
    "            suffixes=('', '_shift')\n",
    "        )\n",
    "\n",
    "    # Calculate baseline metrics\n",
    "    total_revenue = (shifts_df['Charge'] * shifts_df['Time']).sum()\n",
    "    avg_hourly_revenue = shifts_df['Charge'].mean()\n",
    "    \n",
    "    # Analyze cancelled shifts\n",
    "    cancelled_revenue = (cancellations_with_shifts['Charge'] * \n",
    "                        cancellations_with_shifts['Time']).sum()\n",
    "    \n",
    "    print(\"\\nBaseline Metrics:\")\n",
    "    print(f\"Total Potential Revenue: ${total_revenue:,.2f}\")\n",
    "    print(f\"Average Hourly Rate: ${avg_hourly_revenue:.2f}\")\n",
    "    print(f\"Lost Revenue from Cancellations: ${cancelled_revenue:,.2f}\")\n",
    "    if total_revenue > 0:  # Avoid division by zero\n",
    "        print(f\"Percentage of Revenue Lost: {(cancelled_revenue/total_revenue)*100:.2f}%\")\n",
    "\n",
    "    # Analysis by role type\n",
    "    print(\"\\n=== Impact by Role Type ===\")\n",
    "    role_impact = cancellations_with_shifts.groupby('Agent Req').agg({\n",
    "        'Shift ID': 'count',\n",
    "        'Charge': ['mean', 'sum'],\n",
    "        'Time': 'sum'\n",
    "    }).round(2)\n",
    "    role_impact.columns = ['Cancellations', 'Avg Rate', 'Total Charge', 'Total Hours']\n",
    "    role_impact['Est. Revenue Loss'] = role_impact['Avg Rate'] * role_impact['Total Hours']\n",
    "    print(role_impact.sort_values('Est. Revenue Loss', ascending=False))\n",
    "\n",
    "    # Analysis by cancellation type\n",
    "    print(\"\\n=== Impact by Cancellation Type ===\")\n",
    "    type_impact = cancellations_with_shifts.groupby('cancellation_category').agg({\n",
    "        'Shift ID': 'count',\n",
    "        'Charge': ['mean', 'sum'],\n",
    "        'Time': 'sum'\n",
    "    }).round(2)\n",
    "    type_impact.columns = ['Cancellations', 'Avg Rate', 'Total Charge', 'Total Hours']\n",
    "    type_impact['Est. Revenue Loss'] = type_impact['Avg Rate'] * type_impact['Total Hours']\n",
    "    print(type_impact.sort_values('Est. Revenue Loss', ascending=False))\n",
    "\n",
    "    # Calculate impact by facility\n",
    "    print(\"\\n=== Top 5 Facilities by Revenue Loss ===\")\n",
    "    facility_impact = cancellations_with_shifts.groupby('Facility ID').agg({\n",
    "        'Shift ID': 'count',\n",
    "        'Charge': ['mean', 'sum'],\n",
    "        'Time': 'sum'\n",
    "    }).round(2)\n",
    "    facility_impact.columns = ['Cancellations', 'Avg Rate', 'Total Charge', 'Total Hours']\n",
    "    facility_impact['Est. Revenue Loss'] = facility_impact['Avg Rate'] * facility_impact['Total Hours']\n",
    "    print(facility_impact.nlargest(5, 'Est. Revenue Loss'))\n",
    "\n",
    "    # Store results\n",
    "    summary.add_summary('economic', 'overall_impact', {\n",
    "        'total_revenue': total_revenue,\n",
    "        'cancelled_revenue': cancelled_revenue,\n",
    "        'avg_hourly_rate': avg_hourly_revenue\n",
    "    })\n",
    "    summary.add_summary('economic', 'role_impact', role_impact.to_dict())\n",
    "    summary.add_summary('economic', 'type_impact', type_impact.to_dict())\n",
    "\n",
    "    return role_impact, type_impact, facility_impact\n",
    "\n",
    "# Run the analysis\n",
    "role_impact, type_impact, facility_impact = analyze_economic_impact(shifts_df, cancellations_with_shifts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
