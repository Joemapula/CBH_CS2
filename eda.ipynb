{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Environment checks\n",
    "print(f\"Python executable: {sys.executable}\")  \n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"\\nFiles in data directory:\", os.listdir('data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions for Data Loading and Cleaning\n",
    "def load_and_clean_shifts(df):\n",
    "    \"\"\"\n",
    "    Load and clean shifts dataset\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Raw shifts dataframe\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned shifts dataframe with proper datatypes\n",
    "        \n",
    "    Notes:\n",
    "        - Makes a copy to avoid modifying original data\n",
    "        - Converts datetime columns\n",
    "        - Handles potential errors in datetime conversion\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert datetime columns with error handling\n",
    "    datetime_cols = ['Start', 'End', 'Created At']\n",
    "    for col in datetime_cols:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col], format='mixed')\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {col} to datetime: {str(e)}\")\n",
    "                # Log problematic rows for investigation\n",
    "                problematic_rows = df[pd.to_datetime(df[col], format='mixed', errors='coerce').isna()]\n",
    "                if not problematic_rows.empty:\n",
    "                    print(f\"Problematic rows in {col}:\")\n",
    "                    print(problematic_rows[col].head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_and_clean_bookings(df):\n",
    "    \"\"\"\n",
    "    Load and clean booking logs dataset\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Raw bookings dataframe\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned bookings dataframe with proper datatypes\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Convert datetime columns\n",
    "    try:\n",
    "        df['Created At'] = pd.to_datetime(df['Created At'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting Created At: {str(e)}\")\n",
    "    return df\n",
    "\n",
    "def load_and_clean_cancellations(df):\n",
    "    \"\"\"\n",
    "    Load and clean cancellation logs dataset\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Raw cancellations dataframe\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned cancellations dataframe with proper datatypes\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Convert datetime columns with flexible parsing\n",
    "    try:\n",
    "        df['Created At'] = pd.to_datetime(df['Created At'], format='mixed')\n",
    "        df['Shift Start Logs'] = pd.to_datetime(df['Shift Start Logs'], format='mixed')\n",
    "    except Exception as e:\n",
    "        print(f\"Error in datetime conversion: {str(e)}\")\n",
    "        # Try to identify problematic rows\n",
    "        prob_rows = df[pd.to_datetime(df['Shift Start Logs'], format='mixed', errors='coerce').isna()]\n",
    "        if not prob_rows.empty:\n",
    "            print(\"\\nSample of problematic date formats:\")\n",
    "            print(prob_rows['Shift Start Logs'].head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def categorize_lead_time(hours):\n",
    "    \"\"\"\n",
    "    Categorize lead times based on business rules.\n",
    "    \n",
    "    Parameters:\n",
    "        hours (float): Lead time in hours\n",
    "        \n",
    "    Returns:\n",
    "        str: Category of lead time\n",
    "    \"\"\"\n",
    "    if hours < 0:\n",
    "        return 'No-Show'  # Cancelled after shift start\n",
    "    elif hours < 4:\n",
    "        return 'Late (<4hrs)'\n",
    "    elif hours < 24:\n",
    "        return 'Same Day'\n",
    "    elif hours < 72:\n",
    "        return 'Advance (<3 days)'\n",
    "    return 'Early (3+ days)'\n",
    "\n",
    "def clean_lead_times(cancellations_df):\n",
    "    \"\"\"\n",
    "    Clean and categorize lead times in cancellation data\n",
    "    \n",
    "    Parameters:\n",
    "        cancellations_df (pd.DataFrame): Raw cancellations dataframe\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned cancellations data with categorized lead times\n",
    "        pd.Series: Statistics about removed records for quality control\n",
    "    \"\"\"\n",
    "    df = cancellations_df.copy()\n",
    "    \n",
    "    # Track data quality issues\n",
    "    quality_stats = {\n",
    "        'original_rows': len(df),\n",
    "        'null_lead_times': df['Lead Time'].isnull().sum(),\n",
    "        'infinite_values': (~np.isfinite(df['Lead Time'])).sum()\n",
    "    }\n",
    "    \n",
    "    # Only remove truly invalid data\n",
    "    mask = df['Lead Time'].notnull() & np.isfinite(df['Lead Time'])\n",
    "    df = df[mask]\n",
    "    \n",
    "    # Add cleaned lead time without filtering extremes\n",
    "    df['clean_lead_time'] = df['Lead Time']\n",
    "    \n",
    "    # Categorize all lead times\n",
    "    df['cancellation_category'] = df['clean_lead_time'].apply(categorize_lead_time)\n",
    "    \n",
    "    # Add flags for extreme values for analysis\n",
    "    df['is_extreme_negative'] = df['Lead Time'] < -72  # Flag cancellations >3 days after\n",
    "    df['is_extreme_positive'] = df['Lead Time'] > 1000 # Flag cancellations >41 days before\n",
    "    \n",
    "    quality_stats['final_rows'] = len(df)\n",
    "    quality_stats['removed_rows'] = quality_stats['original_rows'] - quality_stats['final_rows']\n",
    "    \n",
    "    return df, pd.Series(quality_stats)\n",
    "\n",
    "# Data Summary Storage Class\n",
    "class DataSummary:\n",
    "    \"\"\"Class to store and manage analysis results\"\"\"\n",
    "    def __init__(self):\n",
    "        self.summaries = {}\n",
    "    \n",
    "    def add_summary(self, dataset_name, summary_type, data):\n",
    "        \"\"\"Add summary statistics to storage\"\"\"\n",
    "        if dataset_name not in self.summaries:\n",
    "            self.summaries[dataset_name] = {}\n",
    "        self.summaries[dataset_name][summary_type] = data\n",
    "    \n",
    "    def get_summary(self, dataset_name, summary_type=None):\n",
    "        \"\"\"Retrieve stored summary statistics\"\"\"\n",
    "        if summary_type:\n",
    "            return self.summaries.get(dataset_name, {}).get(summary_type)\n",
    "        return self.summaries.get(dataset_name)\n",
    "    \n",
    "    def print_summary(self, dataset_name):\n",
    "        \"\"\"Print stored summaries for a dataset\"\"\"\n",
    "        if dataset_name in self.summaries:\n",
    "            print(f\"\\nSummary for {dataset_name}:\")\n",
    "            for summary_type, data in self.summaries[dataset_name].items():\n",
    "                print(f\"\\n{summary_type}:\")\n",
    "                print(data)\n",
    "\n",
    "# Initialize summary storage\n",
    "summary = DataSummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Loading and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do note this only previews shifts data not cancels and bookings \n",
    "# we should add other datasets to summary \n",
    "# check how unique ids is defined \n",
    "# note the date ranges for each dataset are different \n",
    "# \n",
    "# 41k unique shifts, 127k unique bookings, 78k unique cancels -> 41 + 78 ~ 120 seems reasonable but lots of \n",
    "# this means each shift is booked 3 times and canceled twice? \n",
    "\n",
    "# === Load and Prepare All Datasets ===\n",
    "print(\"Loading and preparing all datasets...\")\n",
    "\n",
    "# Load all datasets\n",
    "shifts_df = pd.read_csv('data/cleveland_shifts_large.csv')\n",
    "bookings_df = pd.read_csv('data/booking_logs_large.csv')\n",
    "cancellations_df = pd.read_csv('data/cancel_logs_large.csv')\n",
    "\n",
    "# Clean and prepare the data\n",
    "shifts_df = load_and_clean_shifts(shifts_df)\n",
    "bookings_df = load_and_clean_bookings(bookings_df)\n",
    "cancellations_df = load_and_clean_cancellations(cancellations_df)\n",
    "\n",
    "# Initial shifts data exploration\n",
    "print(\"\\n=== Shifts Data Overview ===\")\n",
    "print(\"Dataset Shape:\", shifts_df.shape)\n",
    "print(\"\\nColumns:\", shifts_df.columns.tolist())\n",
    "print(\"\\nData Types:\\n\", shifts_df.dtypes)\n",
    "\n",
    "# Missing value analysis\n",
    "missing_values = shifts_df.isnull().sum()\n",
    "print(\"\\nMissing Values:\\n\", missing_values)\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(shifts_df.head())\n",
    "\n",
    "# Store initial findings\n",
    "summary.add_summary('shifts', 'shape', shifts_df.shape)\n",
    "summary.add_summary('shifts', 'dtypes', shifts_df.dtypes)\n",
    "summary.add_summary('shifts', 'missing_values', missing_values)\n",
    "\n",
    "# Cross-dataset validation\n",
    "print(\"\\n=== Dataset Cross-Validation ===\")\n",
    "for name, df in [('Shifts', shifts_df), ('Bookings', bookings_df), ('Cancellations', cancellations_df)]:\n",
    "    print(f\"\\n=== {name} Dataset ===\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Date Range: {pd.to_datetime(df['Created At']).min()} to {pd.to_datetime(df['Created At']).max()}\")\n",
    "    print(\"\\nSample of unique IDs:\")\n",
    "    print(df['ID'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Dive (formerly Data Quality Analysis) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Check for duplicate IDs in a DataFrame\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame to check for duplicates\n",
    "    dataset_name : str\n",
    "        Name of the dataset for reporting\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : A dictionary with duplicate analysis results\n",
    "    \"\"\"\n",
    "    # Identify the ID column (adjust if different in your specific datasets)\n",
    "    id_column = 'ID'\n",
    "    \n",
    "    # Count total rows\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # Count unique IDs\n",
    "    unique_ids = df[id_column].nunique()\n",
    "    \n",
    "    # Find duplicate IDs\n",
    "    duplicate_ids = df[df.duplicated(subset=[id_column], keep=False)]\n",
    "    num_duplicate_ids = len(duplicate_ids)\n",
    "    \n",
    "    # Group duplicates to see how many times each duplicate appears\n",
    "    duplicate_counts = duplicate_ids[id_column].value_counts()\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(f\"\\n=== Duplicate Analysis for {dataset_name} ===\")\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(f\"Unique IDs: {unique_ids}\")\n",
    "    print(f\"Number of duplicate IDs: {num_duplicate_ids}\")\n",
    "    \n",
    "    # If there are duplicates, show more details\n",
    "    if num_duplicate_ids > 0:\n",
    "        print(\"\\nDuplicate ID Frequency:\")\n",
    "        print(duplicate_counts.head())  # Show top duplicates\n",
    "        \n",
    "        print(\"\\nSample of rows with duplicate IDs:\")\n",
    "        print(duplicate_ids.groupby(id_column).first())\n",
    "    \n",
    "    return {\n",
    "        'total_rows': total_rows,\n",
    "        'unique_ids': unique_ids,\n",
    "        'num_duplicate_ids': num_duplicate_ids\n",
    "    }\n",
    "\n",
    "# Apply the function to each dataset\n",
    "shifts_duplicate_analysis = check_duplicates(shifts_df, 'Shifts Dataset')\n",
    "bookings_duplicate_analysis = check_duplicates(bookings_df, 'Bookings Dataset')\n",
    "cancellations_duplicate_analysis = check_duplicates(cancellations_df, 'Cancellations Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Lead Time Analysis ===\n",
    "print(\"Analyzing lead times and cancellation patterns...\")\n",
    "\n",
    "# Clean lead times and get quality stats\n",
    "clean_cancellations, quality_stats = clean_lead_times(cancellations_df)\n",
    "\n",
    "# Basic cancellation metrics \n",
    "# to do - this may be incorrect due to date overlap \n",
    "shifts_with_cancellations = len(set(shifts_df['ID']) & set(cancellations_df['Shift ID']))\n",
    "print(f\"Shifts with cancellations: {shifts_with_cancellations}\")\n",
    "print(f\"Percentage of shifts cancelled: {(shifts_with_cancellations/len(shifts_df))*100:.2f}%\")\n",
    "\n",
    "print(\"\\n=== Data Quality Statistics ===\")\n",
    "print(quality_stats)\n",
    "\n",
    "print(\"\\n=== Lead Time Distribution ===\")\n",
    "print(\"Overall Lead Time Statistics:\")\n",
    "print(cancellations_df['Lead Time'].describe().round(2))\n",
    "\n",
    "print(\"\\n=== Cancellation Categories ===\")\n",
    "print(clean_cancellations['cancellation_category'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n=== Extreme Values Analysis ===\")\n",
    "print(f\"Very Late Cancellations (>3 days after): {clean_cancellations['is_extreme_negative'].sum()}\")\n",
    "print(f\"Very Early Cancellations (>41 days before): {clean_cancellations['is_extreme_positive'].sum()}\")\n",
    "\n",
    "# Distribution of lead times for extreme cases\n",
    "if clean_cancellations['is_extreme_negative'].any():\n",
    "    print(\"\\nVery Late Cancellation Stats:\")\n",
    "    print(clean_cancellations[clean_cancellations['is_extreme_negative']]['Lead Time'].describe())\n",
    "\n",
    "if clean_cancellations['is_extreme_positive'].any():\n",
    "    print(\"\\nVery Early Cancellation Stats:\")\n",
    "    print(clean_cancellations[clean_cancellations['is_extreme_positive']]['Lead Time'].describe())\n",
    "\n",
    "# Store results in summary\n",
    "summary.add_summary('cancellations', 'quality_stats', quality_stats.to_dict())\n",
    "summary.add_summary('cancellations', 'extreme_values', {\n",
    "    'very_late': clean_cancellations['is_extreme_negative'].sum(),\n",
    "    'very_early': clean_cancellations['is_extreme_positive'].sum()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Comprehensive Cancellation Analysis ===\n",
    "print(\"=== Cancellation Type Analysis ===\")\n",
    "# Basic cancellation types\n",
    "cancellation_types = cancellations_df['Action'].value_counts()\n",
    "print(\"\\nCancellation Action Types:\")\n",
    "print(cancellation_types)\n",
    "\n",
    "# Detailed Pattern Analysis\n",
    "def analyze_cancellation_patterns(clean_cancellations, shifts_df):\n",
    "    \"\"\"\n",
    "    Analyze patterns in cancellations, including:\n",
    "    - Types of cancellations (NCNS vs Regular)\n",
    "    - Role-based patterns\n",
    "    - Shift type patterns\n",
    "    \"\"\"\n",
    "    # Merge with shifts to analyze by role\n",
    "    cancellations_with_shifts = pd.merge(\n",
    "        clean_cancellations,\n",
    "        shifts_df[['ID', 'Agent Req', 'Shift Type', 'Charge']],\n",
    "        left_on='Shift ID',\n",
    "        right_on='ID',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Cancellations by Role ===\")\n",
    "    role_cancels = pd.crosstab(\n",
    "        cancellations_with_shifts['Agent Req'],\n",
    "        cancellations_with_shifts['cancellation_category'],\n",
    "        normalize='index'\n",
    "    ).round(3) * 100\n",
    "    print(role_cancels)\n",
    "    \n",
    "    print(\"\\n=== Cancellations by Shift Type ===\")\n",
    "    shift_cancels = pd.crosstab(\n",
    "        cancellations_with_shifts['Shift Type'],\n",
    "        cancellations_with_shifts['cancellation_category'],\n",
    "        normalize='index'\n",
    "    ).round(3) * 100\n",
    "    print(shift_cancels)\n",
    "    \n",
    "    # Role-based impact analysis\n",
    "    role_impact = cancellations_with_shifts.groupby('Agent Req').agg({\n",
    "        'Shift ID': 'count',\n",
    "        'Lead Time': ['mean', 'std'],\n",
    "        'Action': lambda x: (x == 'NO_CALL_NO_SHOW').mean() * 100\n",
    "    }).round(2)\n",
    "    role_impact.columns = ['Total Cancellations', 'Avg Lead Time', 'Lead Time Std', 'NCNS Rate']\n",
    "    print(\"\\n=== Role-Based Impact ===\")\n",
    "    print(role_impact)\n",
    "    \n",
    "    # Store results\n",
    "    summary.add_summary('cancellations', 'action_types', cancellation_types.to_dict())\n",
    "    summary.add_summary('cancellations', 'role_patterns', role_cancels.to_dict())\n",
    "    summary.add_summary('cancellations', 'shift_patterns', shift_cancels.to_dict())\n",
    "    summary.add_summary('cancellations', 'role_impact', role_impact.to_dict())\n",
    "    \n",
    "    return cancellations_with_shifts\n",
    "\n",
    "# Run the analysis\n",
    "cancellations_with_shifts = analyze_cancellation_patterns(clean_cancellations, shifts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Numerical Analysis ===\n",
    "print(\"\\n=== Numerical Analysis ===\")\n",
    "# Basic statistics for numerical columns\n",
    "numeric_stats = shifts_df[['Charge', 'Time']].describe()\n",
    "print(\"\\nNumerical Statistics:\")\n",
    "print(numeric_stats)\n",
    "\n",
    "# Additional numeric insights\n",
    "print(\"\\nCharge Rate Analysis:\")\n",
    "print(f\"Shifts with zero charge: {(shifts_df['Charge'] == 0).sum()}\")\n",
    "print(f\"Average charge by agent type:\")\n",
    "print(shifts_df.groupby('Agent Req')['Charge'].mean().round(2))\n",
    "\n",
    "# === Categorical Analysis ===\n",
    "print(\"\\n=== Categorical Analysis ===\")\n",
    "# Shift type distribution\n",
    "print(\"\\nShift Type Distribution:\")\n",
    "shift_type_dist = shifts_df['Shift Type'].value_counts(dropna=True)\n",
    "print(shift_type_dist)\n",
    "\n",
    "# Agent requirements\n",
    "print(\"\\nAgent Requirement Distribution:\")\n",
    "agent_req_dist = shifts_df['Agent Req'].value_counts(dropna=True)\n",
    "print(agent_req_dist)\n",
    "\n",
    "# Cross-tabulation of shift types and agent requirements\n",
    "print(\"\\nShift Types by Agent Requirements:\")\n",
    "print(pd.crosstab(shifts_df['Shift Type'], shifts_df['Agent Req']))\n",
    "\n",
    "# === Data Completeness Analysis ===\n",
    "print(\"\\n=== Data Completeness Analysis ===\")\n",
    "complete_rows = shifts_df.dropna().shape[0]\n",
    "print(f\"Complete rows: {complete_rows} out of {shifts_df.shape[0]}\")\n",
    "print(f\"Completion rate: {(complete_rows/shifts_df.shape[0]*100):.2f}%\")\n",
    "\n",
    "# === Time-Based Analysis ===\n",
    "print(\"\\n=== Time-Based Analysis ===\")\n",
    "# Extract time components\n",
    "shifts_df['Hour'] = shifts_df['Start'].dt.hour\n",
    "shifts_df['Day'] = shifts_df['Start'].dt.day_name()\n",
    "shifts_df['Month'] = shifts_df['Start'].dt.month\n",
    "shifts_df['Shift_Length'] = (shifts_df['End'] - shifts_df['Start']).dt.total_seconds() / 3600\n",
    "# Time patterns\n",
    "print(\"\\nShifts by Hour:\")\n",
    "hour_dist = shifts_df['Hour'].value_counts().sort_index()\n",
    "print(hour_dist)\n",
    "\n",
    "print(\"\\nShifts by Day of Week:\")\n",
    "day_dist = shifts_df['Day'].value_counts()\n",
    "print(day_dist)\n",
    "\n",
    "print(\"\\nShift Length Distribution:\")\n",
    "print(shifts_df['Shift_Length'].describe().round(2))\n",
    "\n",
    "# === Facility Analysis ===\n",
    "print(\"\\n=== Facility Analysis ===\")\n",
    "facility_stats = shifts_df.groupby('Facility ID').agg({\n",
    "    'ID': 'count',\n",
    "    'Charge': 'mean',\n",
    "    'Time': 'mean'\n",
    "}).rename(columns={\n",
    "    'ID': 'Number of Shifts',\n",
    "    'Charge': 'Average Charge',\n",
    "    'Time': 'Average Shift Length'\n",
    "})\n",
    "print(\"\\nFacility Statistics:\")\n",
    "print(facility_stats.head())\n",
    "print(f\"\\nTotal unique facilities: {shifts_df['Facility ID'].nunique()}\")\n",
    "\n",
    "# Store all results\n",
    "summary.add_summary('shifts', 'numeric_stats', numeric_stats)\n",
    "summary.add_summary('shifts', 'shift_types', shift_type_dist.to_dict())\n",
    "summary.add_summary('shifts', 'agent_types', agent_req_dist.to_dict())\n",
    "summary.add_summary('shifts', 'hour_distribution', hour_dist.to_dict())\n",
    "summary.add_summary('shifts', 'day_distribution', day_dist.to_dict())\n",
    "summary.add_summary('shifts', 'facility_stats', facility_stats.to_dict())\n",
    "\n",
    "# Optional: Create visualizations\n",
    "# We can add matplotlib/seaborn plots here if you'd like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship analysis\n",
    "# First, let's see how many shifts had cancellations\n",
    "shifts_with_cancellations = len(set(shifts_df['ID']) & set(cancellations_df['Shift ID']))\n",
    "print(f\"Shifts with cancellations: {shifts_with_cancellations}\")\n",
    "print(f\"Percentage of shifts cancelled: {(shifts_with_cancellations/len(shifts_df))*100:.2f}%\")\n",
    "\n",
    "# Analyze cancellation lead times\n",
    "cancellations_df['Lead Time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Booking Pattern Analysis ===\n",
    "def analyze_booking_patterns(bookings_df, shifts_df, clean_cancellations):\n",
    "    \"\"\"\n",
    "    Analyze patterns in shift bookings, including:\n",
    "    - Time from posting to booking\n",
    "    - Successful vs cancelled bookings\n",
    "    - Rebooking patterns after cancellations\n",
    "    \n",
    "    Parameters:\n",
    "        bookings_df (pd.DataFrame): Booking logs data\n",
    "        shifts_df (pd.DataFrame): Shifts data\n",
    "        clean_cancellations (pd.DataFrame): Cleaned cancellations data\n",
    "    \"\"\"\n",
    "    print(\"=== Booking Success Analysis ===\")\n",
    "\n",
    "    # Calculate time to fill (from shift creation to booking)\n",
    "    bookings_with_shifts = pd.merge(\n",
    "        bookings_df,\n",
    "        shifts_df[['ID', 'Created At', 'Agent Req', 'Shift Type', 'Charge']],\n",
    "        left_on='Shift ID',\n",
    "        right_on='ID',\n",
    "        how='left',\n",
    "        suffixes=('_booking', '_shift')\n",
    "    )\n",
    "    \n",
    "    bookings_with_shifts['time_to_fill'] = (\n",
    "        pd.to_datetime(bookings_with_shifts['Created At_booking']) - \n",
    "        pd.to_datetime(bookings_with_shifts['Created At_shift'])\n",
    "    ).dt.total_seconds() / 3600  # Convert to hours\n",
    "    \n",
    "    print(\"\\nTime to Fill Statistics (hours):\")\n",
    "    print(bookings_with_shifts['time_to_fill'].describe().round(2))\n",
    "    \n",
    "    # Analyze bookings by role and shift type\n",
    "    print(\"\\n=== Bookings by Role ===\")\n",
    "    role_bookings = bookings_with_shifts.groupby('Agent Req').agg({\n",
    "        'Shift ID': 'count',  # Changed from 'ID' to 'Shift ID'\n",
    "        'time_to_fill': 'mean',\n",
    "        'Charge': 'mean'\n",
    "    }).round(2)\n",
    "    role_bookings.columns = ['Number of Bookings', 'Avg Time to Fill', 'Avg Charge']\n",
    "    print(role_bookings)\n",
    "    \n",
    "    # Look at shifts that got cancelled and rebooked\n",
    "    rebooked_cancellations = clean_cancellations['Shift ID'].value_counts()\n",
    "    \n",
    "    print(\"\\n=== Rebooking Analysis ===\")\n",
    "    print(f\"Shifts cancelled multiple times: {(rebooked_cancellations > 1).sum()}\")\n",
    "    print(f\"Maximum cancellations for a single shift: {rebooked_cancellations.max()}\")\n",
    "    \n",
    "    # Additional timing analysis\n",
    "    print(\"\\n=== Booking Time Patterns ===\")\n",
    "    bookings_with_shifts['booking_hour'] = pd.to_datetime(bookings_with_shifts['Created At_booking']).dt.hour\n",
    "    bookings_with_shifts['booking_day'] = pd.to_datetime(bookings_with_shifts['Created At_booking']).dt.day_name()\n",
    "    \n",
    "    print(\"\\nBookings by Hour of Day:\")\n",
    "    print(bookings_with_shifts['booking_hour'].value_counts().sort_index())\n",
    "    \n",
    "    print(\"\\nBookings by Day of Week:\")\n",
    "    print(bookings_with_shifts['booking_day'].value_counts())\n",
    "    \n",
    "    # Store results\n",
    "    summary.add_summary('bookings', 'time_to_fill', \n",
    "                       bookings_with_shifts['time_to_fill'].describe().to_dict())\n",
    "    summary.add_summary('bookings', 'role_patterns', role_bookings.to_dict())\n",
    "    summary.add_summary('bookings', 'rebooking_stats', {\n",
    "        'multiple_cancellations': (rebooked_cancellations > 1).sum(),\n",
    "        'max_cancellations': rebooked_cancellations.max()\n",
    "    })\n",
    "    \n",
    "    return bookings_with_shifts\n",
    "\n",
    "# Run the analysis\n",
    "bookings_with_shifts = analyze_booking_patterns(bookings_df, shifts_df, clean_cancellations)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Economic Impact Analysis ===\n",
    "def analyze_economic_impact(shifts_df, cancellations_with_shifts):\n",
    "    \"\"\"\n",
    "    Analyze economic impact of cancellations, including:\n",
    "    - Revenue loss from cancellations\n",
    "    - Impact by facility and role type\n",
    "    - Patterns in high-cost cancellations\n",
    "    \"\"\"\n",
    "    print(\"=== Economic Impact Analysis ===\")\n",
    "    \n",
    "    # First, ensure we have all needed columns by merging with shifts data if needed\n",
    "    if 'Time' not in cancellations_with_shifts.columns:\n",
    "        cancellations_with_shifts = pd.merge(\n",
    "            cancellations_with_shifts,\n",
    "            shifts_df[['ID', 'Time', 'Charge']],\n",
    "            left_on='Shift ID',\n",
    "            right_on='ID',\n",
    "            how='left',\n",
    "            suffixes=('', '_shift')\n",
    "        )\n",
    "\n",
    "    # Calculate baseline metrics\n",
    "    total_revenue = (shifts_df['Charge'] * shifts_df['Time']).sum()\n",
    "    avg_hourly_revenue = shifts_df['Charge'].mean()\n",
    "    \n",
    "    # Analyze cancelled shifts\n",
    "    cancelled_revenue = (cancellations_with_shifts['Charge'] * \n",
    "                        cancellations_with_shifts['Time']).sum()\n",
    "    \n",
    "    print(\"\\nBaseline Metrics:\")\n",
    "    print(f\"Total Potential Revenue: ${total_revenue:,.2f}\")\n",
    "    print(f\"Average Hourly Rate: ${avg_hourly_revenue:.2f}\")\n",
    "    print(f\"Lost Revenue from Cancellations: ${cancelled_revenue:,.2f}\")\n",
    "    if total_revenue > 0:  # Avoid division by zero\n",
    "        print(f\"Percentage of Revenue Lost: {(cancelled_revenue/total_revenue)*100:.2f}%\")\n",
    "\n",
    "    # Analysis by role type\n",
    "    print(\"\\n=== Impact by Role Type ===\")\n",
    "    role_impact = cancellations_with_shifts.groupby('Agent Req').agg({\n",
    "        'Shift ID': 'count',\n",
    "        'Charge': ['mean', 'sum'],\n",
    "        'Time': 'sum'\n",
    "    }).round(2)\n",
    "    role_impact.columns = ['Cancellations', 'Avg Rate', 'Total Charge', 'Total Hours']\n",
    "    role_impact['Est. Revenue Loss'] = role_impact['Avg Rate'] * role_impact['Total Hours']\n",
    "    print(role_impact.sort_values('Est. Revenue Loss', ascending=False))\n",
    "\n",
    "    # Analysis by cancellation type\n",
    "    print(\"\\n=== Impact by Cancellation Type ===\")\n",
    "    type_impact = cancellations_with_shifts.groupby('cancellation_category').agg({\n",
    "        'Shift ID': 'count',\n",
    "        'Charge': ['mean', 'sum'],\n",
    "        'Time': 'sum'\n",
    "    }).round(2)\n",
    "    type_impact.columns = ['Cancellations', 'Avg Rate', 'Total Charge', 'Total Hours']\n",
    "    type_impact['Est. Revenue Loss'] = type_impact['Avg Rate'] * type_impact['Total Hours']\n",
    "    print(type_impact.sort_values('Est. Revenue Loss', ascending=False))\n",
    "\n",
    "    # Calculate impact by facility\n",
    "    print(\"\\n=== Top 5 Facilities by Revenue Loss ===\")\n",
    "    facility_impact = cancellations_with_shifts.groupby('Facility ID').agg({\n",
    "        'Shift ID': 'count',\n",
    "        'Charge': ['mean', 'sum'],\n",
    "        'Time': 'sum'\n",
    "    }).round(2)\n",
    "    facility_impact.columns = ['Cancellations', 'Avg Rate', 'Total Charge', 'Total Hours']\n",
    "    facility_impact['Est. Revenue Loss'] = facility_impact['Avg Rate'] * facility_impact['Total Hours']\n",
    "    print(facility_impact.nlargest(5, 'Est. Revenue Loss'))\n",
    "\n",
    "    # Store results\n",
    "    summary.add_summary('economic', 'overall_impact', {\n",
    "        'total_revenue': total_revenue,\n",
    "        'cancelled_revenue': cancelled_revenue,\n",
    "        'avg_hourly_rate': avg_hourly_revenue\n",
    "    })\n",
    "    summary.add_summary('economic', 'role_impact', role_impact.to_dict())\n",
    "    summary.add_summary('economic', 'type_impact', type_impact.to_dict())\n",
    "\n",
    "    return role_impact, type_impact, facility_impact\n",
    "\n",
    "# Run the analysis\n",
    "role_impact, type_impact, facility_impact = analyze_economic_impact(shifts_df, cancellations_with_shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audit_data_quality(shifts_df, cancellations_df, bookings_df):\n",
    "    \"\"\"\n",
    "    Comprehensive data quality audit focusing on business-critical issues\n",
    "    \n",
    "    Parameters:\n",
    "    - shifts_df: DataFrame containing shift data\n",
    "    - cancellations_df: DataFrame containing cancellation logs\n",
    "    - bookings_df: DataFrame containing booking logs\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing quality issues by dataset\n",
    "    \"\"\"\n",
    "    quality_issues = {\n",
    "        'shifts': {},\n",
    "        'cancellations': {},\n",
    "        'bookings': {}\n",
    "    }\n",
    "    \n",
    "    # Shifts Analysis\n",
    "    shifts_issues = {\n",
    "        # Financial data issues\n",
    "        'zero_charge': (shifts_df['Charge'] == 0).sum(),\n",
    "        'negative_charge': (shifts_df['Charge'] < 0).sum(),\n",
    "        \n",
    "        # Time-related issues\n",
    "        'zero_time': (shifts_df['Time'] == 0).sum(),\n",
    "        'negative_time': (shifts_df['Time'] < 0).sum(),\n",
    "        'end_before_start': (shifts_df['End'] < shifts_df['Start']).sum(),\n",
    "        \n",
    "        # Missing data\n",
    "        'missing_agent': shifts_df['Agent ID'].isnull().sum(),\n",
    "        'missing_facility': shifts_df['Facility ID'].isnull().sum(),\n",
    "        'missing_shift_type': shifts_df['Shift Type'].isnull().sum(),\n",
    "        \n",
    "        # Verification issues\n",
    "        'unverified_completed': ((shifts_df['End'] < pd.Timestamp.now()) & \n",
    "                                (shifts_df['Verified'].isnull())).sum(),\n",
    "        \n",
    "        # Invalid shift types\n",
    "        'invalid_shift_types': shifts_df[~shifts_df['Shift Type'].isin(['am', 'pm', 'noc', 'custom'])].shape[0]\n",
    "    }\n",
    "    \n",
    "    # Cancellations Analysis\n",
    "    cancel_issues = {\n",
    "        # Lead time issues\n",
    "        'invalid_lead_time': (cancellations_df['Lead Time'].isnull() | \n",
    "                            ~np.isfinite(cancellations_df['Lead Time'])).sum(),\n",
    "        'extreme_negative_lead': (cancellations_df['Lead Time'] < -72).sum(),  # More than 3 days after start\n",
    "        'extreme_positive_lead': (cancellations_df['Lead Time'] > 720).sum(),  # More than 30 days before\n",
    "        \n",
    "        # Missing data\n",
    "        'missing_worker': cancellations_df['Worker ID'].isnull().sum(),\n",
    "        'missing_facility': cancellations_df['Facility ID'].isnull().sum(),\n",
    "        \n",
    "        # Duplicate issues\n",
    "        'duplicate_cancels': cancellations_df.groupby('Shift ID').size().gt(1).sum(),\n",
    "        \n",
    "        # Action type validation\n",
    "        'invalid_actions': cancellations_df[~cancellations_df['Action'].isin(\n",
    "            ['WORKER_CANCEL', 'NO_CALL_NO_SHOW'])].shape[0]\n",
    "    }\n",
    "    \n",
    "    # Bookings Analysis\n",
    "    bookings_issues = {\n",
    "        # Missing data\n",
    "        'missing_worker': bookings_df['Worker ID'].isnull().sum(),\n",
    "        'missing_facility': bookings_df['Facility ID'].isnull().sum(),\n",
    "        \n",
    "        # Lead time issues (time between booking and shift start)\n",
    "        'invalid_lead_time': (bookings_df['Lead Time'].isnull() | \n",
    "                            ~np.isfinite(bookings_df['Lead Time'])).sum(),\n",
    "        \n",
    "        # Action validation\n",
    "        'invalid_actions': bookings_df[bookings_df['Action'] != 'SHIFT_CLAIM'].shape[0]\n",
    "    }\n",
    "    \n",
    "    # Cross-dataset validation\n",
    "    cross_validation = {\n",
    "        'orphaned_cancels': cancellations_df[~cancellations_df['Shift ID'].isin(shifts_df['ID'])].shape[0],\n",
    "        'orphaned_bookings': bookings_df[~bookings_df['Shift ID'].isin(shifts_df['ID'])].shape[0],\n",
    "        'multiple_workers': shifts_df.groupby('ID')['Agent ID'].nunique().gt(1).sum(),\n",
    "        'booking_cancel_mismatch': len(\n",
    "            set(cancellations_df[cancellations_df['Action'] == 'WORKER_CANCEL']['Shift ID']) - \n",
    "            set(bookings_df['Shift ID'])\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    quality_issues['shifts'] = shifts_issues\n",
    "    quality_issues['cancellations'] = cancel_issues\n",
    "    quality_issues['bookings'] = bookings_issues\n",
    "    quality_issues['cross_validation'] = cross_validation\n",
    "    \n",
    "    return quality_issues\n",
    "\n",
    "# Function to display audit results in a readable format\n",
    "def display_audit_results(audit_results):\n",
    "    \"\"\"\n",
    "    Display audit results in a clear, organized format\n",
    "    \"\"\"\n",
    "    for dataset, issues in audit_results.items():\n",
    "        print(f\"\\n=== {dataset.upper()} QUALITY ISSUES ===\")\n",
    "        for issue, count in issues.items():\n",
    "            print(f\"{issue}: {count:,}\")\n",
    "\n",
    "\n",
    "# === In Initial Data Loading and Validation Section ===\n",
    "\n",
    "print(\"Performing data quality audit...\")\n",
    "# Run the audit\n",
    "audit_results = audit_data_quality(shifts_df, cancellations_df, bookings_df)\n",
    "\n",
    "# Display results\n",
    "display_audit_results(audit_results)\n",
    "\n",
    "# Store results in summary\n",
    "summary.add_summary('data_quality', 'audit_results', audit_results)\n",
    "\n",
    "# Optional: Display specific issues that need attention\n",
    "significant_issues = {\n",
    "    dataset: {issue: count for issue, count in issues.items() if count > 0}\n",
    "    for dataset, issues in audit_results.items()\n",
    "}\n",
    "\n",
    "print(\"\\nSignificant issues requiring attention:\")\n",
    "for dataset, issues in significant_issues.items():\n",
    "    if issues:  # Only show datasets with issues\n",
    "        print(f\"\\n{dataset}:\")\n",
    "        for issue, count in issues.items():\n",
    "            print(f\"- {issue}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This suggests that:\n",
    "\n",
    "Many cancellations and bookings don't link to shifts in our dataset\n",
    "Could be due to date range mismatches or data completeness issues\n",
    "Critical for understanding true cancellation rates\n",
    "\n",
    "\n",
    "Worker/Agent Data Gaps\n",
    "\n",
    "Copymissing_agent: 20,035 (shifts)\n",
    "missing_worker: 191 (cancellations)\n",
    "missing_worker: 140 (bookings)\n",
    "This matches what we saw earlier but gives us a more complete picture. Particularly important because:\n",
    "\n",
    "About half of shifts are missing agent IDs\n",
    "Affects our ability to analyze worker patterns\n",
    "Could impact our ability to track repeat cancellations\n",
    "\n",
    "\n",
    "Lead Time Issues\n",
    "\n",
    "Copyextreme_negative_lead: 4,960\n",
    "extreme_positive_lead: 741\n",
    "This provides more granular insight than our earlier analysis. Important because:\n",
    "\n",
    "Shows significant number of very late cancellations (>3 days after start)\n",
    "Identifies early cancellations that might need different handling\n",
    "Relevant to the attendance policy analysis\n",
    "\n",
    "\n",
    "Financial Data Quality\n",
    "\n",
    "Copyzero_charge: 5,019\n",
    "zero_time: 79\n",
    "negative_time: 22\n",
    "Matches our earlier findings but gives more context about potential revenue impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point realized the data doesn't overlap in dates \n",
    "def analyze_data_coverage():\n",
    "    \"\"\"\n",
    "    Analyze the time coverage and relationships between datasets\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing date ranges and overlap analysis for each dataset\n",
    "    \"\"\"\n",
    "    # Get date ranges for each dataset\n",
    "    print(\"Analyzing dataset date coverage...\")\n",
    "    \n",
    "    shift_dates = shifts_df['Start'].dt.date.value_counts().sort_index()\n",
    "    cancel_dates = cancellations_df['Created At'].dt.date.value_counts().sort_index()\n",
    "    booking_dates = bookings_df['Created At'].dt.date.value_counts().sort_index()\n",
    "    \n",
    "    # Analyze overlap periods\n",
    "    date_ranges = {\n",
    "        'shifts': {\n",
    "            'start': shift_dates.index.min(),\n",
    "            'end': shift_dates.index.max(),\n",
    "            'total_days': len(shift_dates),\n",
    "            'avg_shifts_per_day': shift_dates.mean()\n",
    "        },\n",
    "        'cancellations': {\n",
    "            'start': cancel_dates.index.min(),\n",
    "            'end': cancel_dates.index.max(),\n",
    "            'total_days': len(cancel_dates),\n",
    "            'avg_cancels_per_day': cancel_dates.mean()\n",
    "        },\n",
    "        'bookings': {\n",
    "            'start': booking_dates.index.min(),\n",
    "            'end': booking_dates.index.max(),\n",
    "            'total_days': len(booking_dates),\n",
    "            'avg_bookings_per_day': booking_dates.mean()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return date_ranges\n",
    "\n",
    "def analyze_missing_data_impact():\n",
    "    \"\"\"\n",
    "    Assess how missing data affects our key metrics\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing comparative analysis of shifts with/without missing data\n",
    "    \"\"\"\n",
    "    print(\"\\nAnalyzing impact of missing data...\")\n",
    "    \n",
    "    # Analyze shifts with/without missing agent IDs\n",
    "    missing_agent_shifts = shifts_df[shifts_df['Agent ID'].isnull()]\n",
    "    complete_shifts = shifts_df[shifts_df['Agent ID'].notnull()]\n",
    "    \n",
    "    # Get cancellation rates\n",
    "    missing_cancels = len(set(missing_agent_shifts['ID']) & set(cancellations_df['Shift ID']))\n",
    "    complete_cancels = len(set(complete_shifts['ID']) & set(cancellations_df['Shift ID']))\n",
    "    \n",
    "    comparison = {\n",
    "        'missing_agent': {\n",
    "            'count': len(missing_agent_shifts),\n",
    "            'avg_charge': missing_agent_shifts['Charge'].mean(),\n",
    "            'avg_duration': missing_agent_shifts['Time'].mean(),\n",
    "            'cancellation_count': missing_cancels,\n",
    "            'cancellation_rate': missing_cancels / len(missing_agent_shifts) if len(missing_agent_shifts) > 0 else 0,\n",
    "            'verified_rate': missing_agent_shifts['Verified'].mean()\n",
    "        },\n",
    "        'complete_data': {\n",
    "            'count': len(complete_shifts),\n",
    "            'avg_charge': complete_shifts['Charge'].mean(),\n",
    "            'avg_duration': complete_shifts['Time'].mean(),\n",
    "            'cancellation_count': complete_cancels,\n",
    "            'cancellation_rate': complete_cancels / len(complete_shifts) if len(complete_shifts) > 0 else 0,\n",
    "            'verified_rate': complete_shifts['Verified'].mean()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "# Run both analyses\n",
    "print(\"Running additional data quality analyses...\\n\")\n",
    "\n",
    "# Analyze data coverage\n",
    "coverage_results = analyze_data_coverage()\n",
    "print(\"\\n=== Dataset Coverage Analysis ===\")\n",
    "for dataset, info in coverage_results.items():\n",
    "    print(f\"\\n{dataset.upper()} Coverage:\")\n",
    "    for metric, value in info.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "# Analyze missing data impact\n",
    "impact_results = analyze_missing_data_impact()\n",
    "print(\"\\n=== Missing Data Impact Analysis ===\")\n",
    "for category, metrics in impact_results.items():\n",
    "    print(f\"\\n{category.replace('_', ' ').title()}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        if 'rate' in metric:\n",
    "            print(f\"{metric}: {value:.2%}\")\n",
    "        else:\n",
    "            print(f\"{metric}: {value:,.2f}\")\n",
    "\n",
    "# Store results in summary\n",
    "summary.add_summary('data_quality', 'coverage_analysis', coverage_results)\n",
    "summary.add_summary('data_quality', 'missing_data_impact', impact_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data_completeness(shifts_df, cancellations_df, bookings_df, verbose=True):\n",
    "    \"\"\"\n",
    "    Analyze dataset completeness and coverage periods.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    shifts_df : pandas.DataFrame\n",
    "        Shift data containing columns: 'Start', 'Agent ID', etc.\n",
    "    cancellations_df : pandas.DataFrame\n",
    "        Cancellation data containing columns: 'Created At', etc.\n",
    "    bookings_df : pandas.DataFrame\n",
    "        Booking data containing columns: 'Created At', etc.\n",
    "    verbose : bool, default=True\n",
    "        If True, prints detailed analysis results\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing coverage analysis and completeness metrics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Dataset Coverage Analysis\n",
    "    coverage = {\n",
    "        'shifts': {\n",
    "            'date_range': (shifts_df['Start'].min(), shifts_df['End'].max()),\n",
    "            'total_records': len(shifts_df),\n",
    "            'daily_average': len(shifts_df) / shifts_df['Start'].dt.date.nunique()\n",
    "        },\n",
    "        'cancellations': {\n",
    "            'date_range': (cancellations_df['Created At'].min(), \n",
    "                         cancellations_df['Created At'].max()),\n",
    "            'total_records': len(cancellations_df),\n",
    "            'daily_average': len(cancellations_df) / cancellations_df['Created At'].dt.date.nunique()\n",
    "        },\n",
    "        'bookings': {\n",
    "            'date_range': (bookings_df['Created At'].min(), \n",
    "                         bookings_df['Created At'].max()),\n",
    "            'total_records': len(bookings_df),\n",
    "            'daily_average': len(bookings_df) / bookings_df['Created At'].dt.date.nunique()\n",
    "        }\n",
    "    }\n",
    "    results['coverage'] = coverage\n",
    "    \n",
    "    # Data Completeness Analysis\n",
    "    completeness = {\n",
    "        'shifts': {\n",
    "            'missing_agent_id': {\n",
    "                'count': shifts_df['Agent ID'].isnull().sum(),\n",
    "                'percentage': (shifts_df['Agent ID'].isnull().sum() / len(shifts_df)) * 100\n",
    "            },\n",
    "            'verified_shifts': {\n",
    "                'count': shifts_df['Verified'].sum(),\n",
    "                'percentage': (shifts_df['Verified'].sum() / len(shifts_df)) * 100\n",
    "            }\n",
    "        },\n",
    "        'cancellations': {\n",
    "            'missing_worker_id': {\n",
    "                'count': cancellations_df['Worker ID'].isnull().sum(),\n",
    "                'percentage': (cancellations_df['Worker ID'].isnull().sum() / len(cancellations_df)) * 100\n",
    "            }\n",
    "        },\n",
    "        'bookings': {\n",
    "            'missing_worker_id': {\n",
    "                'count': bookings_df['Worker ID'].isnull().sum(),\n",
    "                'percentage': (bookings_df['Worker ID'].isnull().sum() / len(bookings_df)) * 100\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    results['completeness'] = completeness\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=== Dataset Coverage Analysis ===\")\n",
    "        print(\"\\nTime Periods:\")\n",
    "        for dataset, info in coverage.items():\n",
    "            print(f\"\\n{dataset.upper()}:\")\n",
    "            print(f\"Date Range: {info['date_range'][0].date()} to {info['date_range'][1].date()}\")\n",
    "            print(f\"Total Records: {info['total_records']:,}\")\n",
    "            print(f\"Daily Average: {info['daily_average']:.2f}\")\n",
    "        \n",
    "        print(\"\\n=== Data Completeness Analysis ===\")\n",
    "        for dataset, metrics in completeness.items():\n",
    "            print(f\"\\n{dataset.upper()} Completeness:\")\n",
    "            for field, values in metrics.items():\n",
    "                print(f\"{field}:\")\n",
    "                print(f\"  Count: {values['count']:,}\")\n",
    "                print(f\"  Percentage: {values['percentage']:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the analysis\n",
    "completeness_results = analyze_data_completeness(shifts_df, cancellations_df, bookings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Critical Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missing_agent_patterns(shifts_df):\n",
    "    \"\"\"\n",
    "    Analyze patterns in shifts with missing Agent IDs\n",
    "    \n",
    "    Parameters:\n",
    "    shifts_df: DataFrame containing shift data\n",
    "    \n",
    "    Returns:\n",
    "    Dictionary containing analysis results\n",
    "    \"\"\"\n",
    "    # Separate shifts with/without Agent IDs\n",
    "    missing_agent = shifts_df[shifts_df['Agent ID'].isnull()]\n",
    "    has_agent = shifts_df[shifts_df['Agent ID'].notnull()]\n",
    "    \n",
    "    analysis = {\n",
    "        'temporal_patterns': {\n",
    "            'missing_by_month': missing_agent['Start'].dt.to_period('M').value_counts().sort_index(),\n",
    "            'missing_by_dow': missing_agent['Start'].dt.day_name().value_counts(),\n",
    "            'missing_by_shift_type': missing_agent['Shift Type'].value_counts()\n",
    "        },\n",
    "        \n",
    "        'verification_status': {\n",
    "            'missing_verified': missing_agent['Verified'].value_counts(),\n",
    "            'has_agent_verified': has_agent['Verified'].value_counts()\n",
    "        },\n",
    "        \n",
    "        'facility_patterns': {\n",
    "            'facilities_missing': missing_agent['Facility ID'].value_counts(),\n",
    "            'missing_rate_by_facility': (\n",
    "                missing_agent.groupby('Facility ID').size() / \n",
    "                shifts_df.groupby('Facility ID').size()\n",
    "            ).sort_values(ascending=False)\n",
    "        },\n",
    "        \n",
    "        'charge_comparison': {\n",
    "            'missing_charges': missing_agent['Charge'].describe(),\n",
    "            'has_agent_charges': has_agent['Charge'].describe()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"=== Analysis of Shifts with Missing Agent IDs ===\\n\")\n",
    "    print(f\"Total Shifts: {len(shifts_df):,}\")\n",
    "    print(f\"Shifts Missing Agent ID: {len(missing_agent):,} ({len(missing_agent)/len(shifts_df):.1%})\")\n",
    "    print(f\"Shifts with Agent ID: {len(has_agent):,} ({len(has_agent)/len(shifts_df):.1%})\")\n",
    "    \n",
    "    print(\"\\n=== Verification Status ===\")\n",
    "    print(\"\\nShifts Missing Agent ID:\")\n",
    "    print(analysis['verification_status']['missing_verified'])\n",
    "    print(\"\\nShifts with Agent ID:\")\n",
    "    print(analysis['verification_status']['has_agent_verified'])\n",
    "    \n",
    "    print(\"\\n=== Shift Type Distribution (Missing Agent ID) ===\")\n",
    "    print(analysis['temporal_patterns']['missing_by_shift_type'])\n",
    "    \n",
    "    print(\"\\n=== Top 5 Facilities with Missing Agent IDs ===\")\n",
    "    print(\"Count:\")\n",
    "    print(analysis['facility_patterns']['facilities_missing'].head())\n",
    "    print(\"\\nRate:\")\n",
    "    print(analysis['facility_patterns']['missing_rate_by_facility'].head())\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "\"\"\"This analysis should help us:\n",
    "\n",
    "Identify patterns in missing Agent IDs\n",
    "See if certain facilities have more missing IDs\n",
    "Compare verification rates\n",
    "Understand if missing IDs are random or systematic \"\"\"\n",
    "# Run the analysis\n",
    "missing_agent_analysis = analyze_missing_agent_patterns(shifts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's verify the data structure\n",
    "overlap_start = shifts_df['Start'].dt.date.min()\n",
    "overlap_end = shifts_df['Start'].dt.date.max()\n",
    "\n",
    "# Print shapes and types for debugging\n",
    "print(\"Overlap period:\", overlap_start, \"to\", overlap_end)\n",
    "print(\"\\nDataset shapes:\")\n",
    "print(f\"Shifts: {overlap_shifts.shape}\")\n",
    "print(f\"Cancellations: {overlap_cancels.shape}\")\n",
    "print(f\"Bookings: {overlap_bookings.shape}\")\n",
    "\n",
    "# Let's check a simpler version of the worker calculation first\n",
    "def analyze_worker_basic(shifts, cancels):\n",
    "    \"\"\"Simplified version to debug the core calculation\"\"\"\n",
    "    # Get workers with shifts\n",
    "    workers = shifts[shifts['Agent ID'].notnull()]['Agent ID'].unique()\n",
    "    \n",
    "    # Create base metrics\n",
    "    metrics = pd.DataFrame(index=workers)\n",
    "    \n",
    "    # Calculate basic stats\n",
    "    shift_counts = shifts[shifts['Agent ID'].notnull()].groupby('Agent ID')['ID'].count()\n",
    "    cancel_counts = cancels.groupby('Worker ID').size()\n",
    "    \n",
    "    # Ensure matching indices\n",
    "    metrics['total_shifts'] = shift_counts\n",
    "    metrics['cancellations'] = cancel_counts.reindex(workers).fillna(0)\n",
    "    metrics['reliability'] = 1 - (metrics['cancellations'] / metrics['total_shifts'])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Try the simplified version\n",
    "test_metrics = analyze_worker_basic(overlap_shifts, overlap_cancels)\n",
    "print(\"\\nTest metrics head:\")\n",
    "print(test_metrics.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_business_patterns(start_date='2021-10-01', end_date='2022-01-31'):\n",
    "    \"\"\"\n",
    "    Analyze business patterns within the core overlapping period\n",
    "    \n",
    "    Parameters:\n",
    "    - start_date: Beginning of analysis period\n",
    "    - end_date: End of analysis period\n",
    "    \"\"\"\n",
    "    # Filter to overlapping period\n",
    "    mask_period = lambda df: (\n",
    "        df['Created At'].dt.date >= pd.to_datetime(start_date).date() &\n",
    "        df['Created At'].dt.date <= pd.to_datetime(end_date).date()\n",
    "    )\n",
    "    \n",
    "    shifts_filtered = shifts_df[shifts_df['Start'].dt.date.between(start_date, end_date)]\n",
    "    cancels_filtered = cancellations_df[mask_period(cancellations_df)]\n",
    "    bookings_filtered = bookings_df[mask_period(bookings_df)]\n",
    "    \n",
    "    # Time-based patterns\n",
    "    time_patterns = {\n",
    "        'hourly_patterns': pd.DataFrame({\n",
    "            'cancellations': cancels_filtered['Created At'].dt.hour.value_counts().sort_index(),\n",
    "            'bookings': bookings_filtered['Created At'].dt.hour.value_counts().sort_index()\n",
    "        }).fillna(0),\n",
    "        \n",
    "        'daily_patterns': pd.DataFrame({\n",
    "            'shifts': shifts_filtered['Start'].dt.day_name().value_counts(),\n",
    "            'cancellations': cancels_filtered['Created At'].dt.day_name().value_counts(),\n",
    "            'bookings': bookings_filtered['Created At'].dt.day_name().value_counts()\n",
    "        }).fillna(0),\n",
    "        \n",
    "        'lead_time_success': bookings_filtered[\n",
    "            ~bookings_filtered['Shift ID'].isin(cancels_filtered['Shift ID'])\n",
    "        ]['Lead Time'].describe()\n",
    "    }\n",
    "    \n",
    "    # Worker reliability - separating by data completeness\n",
    "    worker_patterns = {\n",
    "        'complete_data': analyze_worker_patterns(\n",
    "            shifts_filtered[shifts_filtered['Agent ID'].notnull()],\n",
    "            cancels_filtered,\n",
    "            bookings_filtered\n",
    "        ),\n",
    "        'missing_data': analyze_worker_patterns(\n",
    "            shifts_filtered[shifts_filtered['Agent ID'].isnull()],\n",
    "            cancels_filtered,\n",
    "            bookings_filtered\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Facility analysis\n",
    "    facility_patterns = {\n",
    "        'cancel_rates': calculate_facility_metrics(\n",
    "            shifts_filtered, cancels_filtered, bookings_filtered\n",
    "        ),\n",
    "        'ncns_impact': analyze_ncns_impact(\n",
    "            shifts_filtered, cancels_filtered\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'time_patterns': time_patterns,\n",
    "        'worker_patterns': worker_patterns,\n",
    "        'facility_patterns': facility_patterns\n",
    "    }\n",
    "\n",
    "def analyze_worker_patterns(shifts, cancels, bookings):\n",
    "    \"\"\"\n",
    "    Analyze comprehensive booking and cancellation patterns at the worker level\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    shifts : pd.DataFrame\n",
    "        Shift data including verified status and worker information\n",
    "    cancels : pd.DataFrame\n",
    "        Cancellation data with timing and reason information\n",
    "    bookings : pd.DataFrame\n",
    "        Booking data with lead times and worker details\n",
    "        \n",
    "    Returns:\n",
    "    -----------\n",
    "    dict\n",
    "        Dictionary containing detailed worker behavior analysis\n",
    "    \"\"\"\n",
    "    # Start with valid workers only\n",
    "    active_workers = shifts[shifts['Agent ID'].notnull()]['Agent ID'].unique()\n",
    "    \n",
    "    # Initialize success metrics DataFrame first\n",
    "    success_metrics = pd.DataFrame(index=active_workers)\n",
    "    \n",
    "    # Basic counts\n",
    "    shift_counts = shifts[shifts['Agent ID'].notnull()].groupby('Agent ID')['ID'].count()\n",
    "    cancel_counts = cancels.groupby('Worker ID').size()\n",
    "    ncns_counts = cancels[cancels['Action'] == 'NO_CALL_NO_SHOW'].groupby('Worker ID').size()\n",
    "    \n",
    "    # Add basic metrics with proper index alignment\n",
    "    success_metrics['total_shifts'] = shift_counts\n",
    "    success_metrics['cancellations'] = cancel_counts.reindex(active_workers).fillna(0)\n",
    "    success_metrics['ncns'] = ncns_counts.reindex(active_workers).fillna(0)\n",
    "    \n",
    "    # Calculate derived metrics\n",
    "    success_metrics['reliability'] = 1 - (success_metrics['cancellations'] / success_metrics['total_shifts'])\n",
    "    success_metrics['ncns_rate'] = success_metrics['ncns'] / success_metrics['cancellations'].replace(0, 1)\n",
    "    \n",
    "    # Add verification rate\n",
    "    verify_rate = shifts[shifts['Agent ID'].notnull()].groupby('Agent ID')['Verified'].mean()\n",
    "    success_metrics['completion_rate'] = verify_rate\n",
    "    \n",
    "    # Add charge and time metrics\n",
    "    charge_stats = shifts[shifts['Agent ID'].notnull()].groupby('Agent ID')['Charge'].agg(['mean', 'std'])\n",
    "    time_stats = shifts[shifts['Agent ID'].notnull()].groupby('Agent ID')['Time'].agg(['mean', 'std'])\n",
    "    \n",
    "    success_metrics['avg_charge'] = charge_stats['mean']\n",
    "    success_metrics['consistency'] = 1 - (time_stats['std'] / time_stats['mean'].replace(0, np.inf))\n",
    "    \n",
    "    # Pre-calculate datetime features for patterns\n",
    "    bookings = bookings.copy()\n",
    "    bookings['booking_hour'] = bookings['Created At'].dt.hour\n",
    "    bookings['booking_day'] = bookings['Created At'].dt.day_name()\n",
    "    \n",
    "    # Analyze booking patterns\n",
    "    booking_patterns = {\n",
    "        'time_preferences': {\n",
    "            'booking_hours': bookings.groupby('Worker ID')['booking_hour'].value_counts(),\n",
    "            'booking_days': bookings.groupby('Worker ID')['booking_day'].value_counts(),\n",
    "            'lead_time_stats': bookings.groupby('Worker ID')['Lead Time'].describe()\n",
    "        },\n",
    "        'shift_preferences': {\n",
    "            'shift_types': shifts[shifts['Agent ID'].isin(active_workers)].groupby(\n",
    "                ['Agent ID', 'Shift Type']).size().unstack(fill_value=0),\n",
    "            'facility_choices': shifts[shifts['Agent ID'].isin(active_workers)].groupby(\n",
    "                ['Agent ID', 'Facility ID']).size().unstack(fill_value=0)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Calculate overall score\n",
    "    weights = {\n",
    "        'completion_rate': 0.4,\n",
    "        'reliability': 0.3,\n",
    "        'consistency': 0.2,\n",
    "        'avg_charge': 0.1\n",
    "    }\n",
    "    \n",
    "    # Normalize any missing columns\n",
    "    available_metrics = [m for m in weights.keys() if m in success_metrics.columns]\n",
    "    weight_sum = sum(weights[m] for m in available_metrics)\n",
    "    \n",
    "    success_metrics['overall_score'] = sum(\n",
    "        success_metrics[metric] * (weights[metric] / weight_sum)\n",
    "        for metric in available_metrics\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'success_metrics': success_metrics,\n",
    "        'booking_patterns': booking_patterns\n",
    "    }\n",
    "\"\"\"Worker Profiles:\n",
    "\n",
    "Creates a baseline profile for each worker using shift data\n",
    "Captures basic metrics like total shifts, verification rates, and pricing patterns\n",
    "\n",
    "\n",
    "Booking Patterns:\n",
    "\n",
    "Analyzes when workers prefer to book shifts (time of day, day of week)\n",
    "Examines lead time patterns\n",
    "Identifies preferences for shift types and facilities\n",
    "\n",
    "\n",
    "Cancellation Patterns:\n",
    "\n",
    "Studies when cancellations typically occur\n",
    "Calculates cancellation rates and no-show rates\n",
    "Analyzes lead times for cancellations\n",
    "\n",
    "\n",
    "Success Metrics:\n",
    "\n",
    "Combines multiple factors into an overall worker score\n",
    "Uses weighted metrics for completion, reliability, consistency, and earnings\n",
    "Allows for customization of weights based on business priorities\"\"\"\n",
    "\n",
    "def calculate_facility_metrics(shifts, cancels, bookings):\n",
    "    \"\"\"Calculate key facility metrics\"\"\"\n",
    "    return {\n",
    "        'cancel_rates': (cancels.groupby('Facility ID').size() / \n",
    "                        shifts.groupby('Facility ID').size()),\n",
    "        'rebooking_success': calculate_rebooking_rates(shifts, cancels, bookings),\n",
    "        'shift_fulfillment': calculate_fulfillment_rates(shifts, bookings)\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_worker_reliability(shifts, cancels):\n",
    "    \"\"\"\n",
    "    Calculate reliability scores for workers based on their history\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    shifts : pd.DataFrame\n",
    "        Shift data with worker information\n",
    "    cancels : pd.DataFrame\n",
    "        Cancellation data\n",
    "        \n",
    "    Returns:\n",
    "    -----------\n",
    "    pd.DataFrame\n",
    "        Worker reliability metrics\n",
    "    \"\"\"\n",
    "    worker_metrics = pd.DataFrame()\n",
    "    \n",
    "    # Only analyze workers with valid IDs\n",
    "    valid_workers = shifts[shifts['Agent ID'].notnull()]\n",
    "    \n",
    "    # Calculate basic metrics per worker\n",
    "    worker_metrics = valid_workers.groupby('Agent ID').agg({\n",
    "        'ID': 'count',  # Total shifts\n",
    "        'Verified': 'mean',  # Verification rate\n",
    "        'Charge': 'mean'  # Average charge rate\n",
    "    }).rename(columns={\n",
    "        'ID': 'total_shifts',\n",
    "        'Verified': 'verification_rate',\n",
    "        'Charge': 'avg_charge'\n",
    "    })\n",
    "    \n",
    "    # Add cancellation metrics\n",
    "    cancellation_rates = (\n",
    "        cancels.groupby('Worker ID')\n",
    "        .agg({\n",
    "            'Shift ID': 'count',\n",
    "            'Action': lambda x: (x == 'NO_CALL_NO_SHOW').mean()\n",
    "        })\n",
    "        .rename(columns={\n",
    "            'Shift ID': 'cancellations',\n",
    "            'Action': 'ncns_rate'\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    worker_metrics = worker_metrics.join(\n",
    "        cancellation_rates, \n",
    "        how='left'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Calculate reliability score (you can adjust the formula)\n",
    "    worker_metrics['reliability_score'] = (\n",
    "        worker_metrics['verification_rate'] * 0.4 +\n",
    "        (1 - worker_metrics['ncns_rate']) * 0.4 +\n",
    "        (1 - worker_metrics['cancellations']/worker_metrics['total_shifts']) * 0.2\n",
    "    )\n",
    "    \n",
    "    return worker_metrics.sort_values('reliability_score', ascending=False)\n",
    "\n",
    "def analyze_cancel_timing(cancels):\n",
    "    \"\"\"\n",
    "    Analyze cancellation timing patterns including day/hour distribution,\n",
    "    lead times, and seasonal patterns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cancels : pd.DataFrame\n",
    "        Cancellation data with datetime columns and lead times\n",
    "        \n",
    "    Returns:\n",
    "    -----------\n",
    "    dict : Dictionary containing timing analysis results\n",
    "    \"\"\"\n",
    "    timing_analysis = {\n",
    "        # Time of day patterns\n",
    "        'hourly_distribution': cancels['Created At'].dt.hour.value_counts().sort_index(),\n",
    "        'daily_distribution': cancels['Created At'].dt.day_name().value_counts(),\n",
    "        \n",
    "        # Lead time analysis\n",
    "        'lead_time_stats': cancels['Lead Time'].describe(),\n",
    "        'lead_time_buckets': pd.cut(\n",
    "            cancels['Lead Time'],\n",
    "            bins=[-float('inf'), 0, 4, 24, 72, float('inf')],\n",
    "            labels=['After Start', 'Under 4hrs', '4-24hrs', '1-3 days', 'Over 3 days']\n",
    "        ).value_counts().sort_index(),\n",
    "        \n",
    "        # Action type by timing\n",
    "        'timing_by_action': pd.crosstab(\n",
    "            pd.cut(cancels['Lead Time'], \n",
    "                  bins=[-float('inf'), 0, 4, 24, 72, float('inf')],\n",
    "                  labels=['After Start', 'Under 4hrs', '4-24hrs', '1-3 days', 'Over 3 days']),\n",
    "            cancels['Action']\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return timing_analysis\n",
    "\n",
    "def calculate_rebooking_rates(shifts, cancels, bookings):\n",
    "    \"\"\"\n",
    "    Calculate how successfully cancelled shifts get rebooked\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    shifts : pd.DataFrame\n",
    "        Shift data\n",
    "    cancels : pd.DataFrame\n",
    "        Cancellation data\n",
    "    bookings : pd.DataFrame\n",
    "        Booking data\n",
    "        \n",
    "    Returns:\n",
    "    -----------\n",
    "    dict : Dictionary containing rebooking analysis\n",
    "    \"\"\"\n",
    "    # Get cancelled shifts\n",
    "    cancelled_shifts = cancels['Shift ID'].unique()\n",
    "    \n",
    "    # Look at subsequent bookings for cancelled shifts\n",
    "    rebooking_analysis = {\n",
    "        'total_cancellations': len(cancelled_shifts),\n",
    "        'rebooked_count': sum(\n",
    "            bookings['Shift ID'].isin(cancelled_shifts) &\n",
    "            (bookings['Created At'] > cancels.groupby('Shift ID')['Created At'].first())\n",
    "        ),\n",
    "        'rebooking_lead_times': bookings[\n",
    "            bookings['Shift ID'].isin(cancelled_shifts)\n",
    "        ]['Lead Time'].describe(),\n",
    "        \n",
    "        # Facility level analysis\n",
    "        'facility_rebooking_rates': pd.DataFrame({\n",
    "            'cancellations': cancels.groupby('Facility ID').size(),\n",
    "            'rebookings': bookings[\n",
    "                bookings['Shift ID'].isin(cancelled_shifts)\n",
    "            ].groupby('Facility ID').size()\n",
    "        }).fillna(0)\n",
    "    }\n",
    "    \n",
    "    # Calculate success rate\n",
    "    rebooking_analysis['overall_rebooking_rate'] = (\n",
    "        rebooking_analysis['rebooked_count'] / \n",
    "        rebooking_analysis['total_cancellations']\n",
    "    )\n",
    "    \n",
    "    return rebooking_analysis\n",
    "\n",
    "def calculate_fulfillment_rates(shifts, bookings):\n",
    "    \"\"\"\n",
    "    Calculate shift fulfillment rates and patterns\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    shifts : pd.DataFrame\n",
    "        Shift data including verification status\n",
    "    bookings : pd.DataFrame\n",
    "        Booking data\n",
    "        \n",
    "    Returns:\n",
    "    -----------\n",
    "    dict : Dictionary containing fulfillment analysis\n",
    "    \"\"\"\n",
    "    fulfillment_analysis = {\n",
    "        # Overall fulfillment\n",
    "        'total_shifts': len(shifts),\n",
    "        'booked_shifts': len(shifts[shifts['Agent ID'].notnull()]),\n",
    "        'verified_shifts': shifts['Verified'].sum(),\n",
    "        \n",
    "        # Fulfillment by type\n",
    "        'fulfillment_by_type': pd.DataFrame({\n",
    "            'total': shifts.groupby('Shift Type').size(),\n",
    "            'booked': shifts[shifts['Agent ID'].notnull()].groupby('Shift Type').size(),\n",
    "            'verified': shifts[shifts['Verified']].groupby('Shift Type').size()\n",
    "        }).fillna(0),\n",
    "        \n",
    "        # Fulfillment by role\n",
    "        'fulfillment_by_role': pd.DataFrame({\n",
    "            'total': shifts.groupby('Agent Req').size(),\n",
    "            'booked': shifts[shifts['Agent ID'].notnull()].groupby('Agent Req').size(),\n",
    "            'verified': shifts[shifts['Verified']].groupby('Agent Req').size()\n",
    "        }).fillna(0)\n",
    "    }\n",
    "    \n",
    "    # Calculate rates\n",
    "    fulfillment_analysis['overall_booking_rate'] = (\n",
    "        fulfillment_analysis['booked_shifts'] / \n",
    "        fulfillment_analysis['total_shifts']\n",
    "    )\n",
    "    fulfillment_analysis['overall_verification_rate'] = (\n",
    "        fulfillment_analysis['verified_shifts'] / \n",
    "        fulfillment_analysis['total_shifts']\n",
    "    )\n",
    "    \n",
    "    return fulfillment_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis\n",
    "worker_analysis = analyze_worker_patterns(\n",
    "    overlap_shifts,\n",
    "    overlap_cancels,\n",
    "    overlap_bookings\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n=== Worker Analysis ===\")\n",
    "print(\"\\nMetrics Summary:\")\n",
    "print(worker_analysis['success_metrics'].describe())\n",
    "\n",
    "print(\"\\nTop 5 Most Reliable Workers:\")\n",
    "print(worker_analysis['success_metrics'].nlargest(5, 'reliability')[\n",
    "    ['total_shifts', 'cancellations', 'reliability', 'completion_rate']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "overlap_start = shifts_df['Start'].dt.date.min()\n",
    "overlap_end = shifts_df['Start'].dt.date.max()\n",
    "\n",
    "# Filter data to overlapping period\n",
    "overlap_shifts = shifts_df[shifts_df['Start'].dt.date.between(overlap_start, overlap_end)]\n",
    "overlap_cancels = cancellations_df[\n",
    "    cancellations_df['Created At'].dt.date.between(overlap_start, overlap_end)\n",
    "]\n",
    "overlap_bookings = bookings_df[\n",
    "    bookings_df['Created At'].dt.date.between(overlap_start, overlap_end)\n",
    "]\n",
    "\n",
    "# Run the analysis\n",
    "worker_analysis = analyze_worker_patterns(\n",
    "    overlap_shifts,\n",
    "    overlap_cancels,\n",
    "    overlap_bookings\n",
    ")\n",
    "\n",
    "# Display summary results\n",
    "print(\"\\n=== Worker Pattern Analysis ===\")\n",
    "print(\"\\nTop Performing Workers:\")\n",
    "print(worker_analysis['success_metrics'].nlargest(5, 'overall_score'))\n",
    "\n",
    "print(\"\\nBooking Time Preferences (Top 3 Hours):\")\n",
    "print(worker_analysis['booking_patterns']['time_preferences']['booking_hours']\n",
    "      .groupby(level=0).nlargest(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think critically about this:\n",
    "\n",
    "Understanding the Data Context\n",
    "From the proposal:\n",
    "\n",
    "\n",
    "They explicitly say booking data is a subset\n",
    "They state it's \"OK\" because it's meant to show booking behavior patterns\n",
    "However, they don't mention if cancellation data is complete or a subset\n",
    "The shifts data appears to be the \"source of truth\" (Oct 2021 - Jan 2022)\n",
    "\n",
    "\n",
    "Analysis Implications\n",
    "We should split our analysis into two categories:\n",
    "\n",
    "A. Full Period Analysis (Using Shifts Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Lets think critically about this:\n",
    "\n",
    "Understanding the Data Context\n",
    "From the proposal:\n",
    "\n",
    "\n",
    "They explicitly say booking data is a subset\n",
    "They state it's \"OK\" because it's meant to show booking behavior patterns\n",
    "However, they don't mention if cancellation data is complete or a subset\n",
    "The shifts data appears to be the \"source of truth\" (Oct 2021 - Jan 2022)\n",
    "\n",
    "\n",
    "Analysis Implications\n",
    "We should split our analysis into two categories: \"\"\"\n",
    "# A. Full Period Analysis (Using Shifts Data)\n",
    "#  B: Behavioral Analysis (Using Overlap Period)\n",
    "\n",
    "\"\"\"The key insight is that we should:\n",
    "\n",
    "Use shifts data for absolute metrics\n",
    "Use overlap periods for behavioral analysis\n",
    "Be clear about limitations in our findings\n",
    "Focus on patterns rather than absolute numbers for booking/cancellation behavior\n",
    "\n",
    "This matches their intent while making the best use of available data.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# A. Full Period Analysis (Using Shifts Data)\n",
    "def analyze_shifts_complete():\n",
    "    \"\"\"\n",
    "    Analyze the complete shifts dataset for overall marketplace health\n",
    "    \n",
    "    Note: This analysis uses only the shifts dataset which appears to be \n",
    "    complete for Oct 2021 - Jan 2022.\n",
    "    \"\"\"\n",
    "    shifts_analysis = {\n",
    "        # Basic marketplace metrics\n",
    "        'total_shifts': len(shifts_df),\n",
    "        'shifts_by_type': shifts_df['Shift Type'].value_counts(),\n",
    "        'verification_rate': shifts_df['Verified'].mean(),\n",
    "        \n",
    "        # Financial metrics\n",
    "        'charge_patterns': shifts_df.groupby('Agent Req')['Charge'].describe(),\n",
    "        \n",
    "        # Time patterns\n",
    "        'shift_distribution': {\n",
    "            'by_day': shifts_df['Start'].dt.day_name().value_counts(),\n",
    "            'by_hour': shifts_df['Start'].dt.hour.value_counts().sort_index()\n",
    "        },\n",
    "        \n",
    "        # Facility metrics\n",
    "        'facility_patterns': shifts_df.groupby('Facility ID').agg({\n",
    "            'ID': 'count',\n",
    "            'Verified': 'mean',\n",
    "            'Charge': 'mean'\n",
    "        }).rename(columns={'ID': 'total_shifts'})\n",
    "    }\n",
    "    return shifts_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  B: Behavioral Analysis (Using Overlap Period)\n",
    "def analyze_booking_behavior(start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Analyze HCP booking and cancellation behavior where we have all datasets\n",
    "    \n",
    "    Notes:\n",
    "    - This analysis uses the period where we have overlapping data\n",
    "    - Focus is on understanding behavioral patterns rather than absolute numbers\n",
    "    \"\"\"\n",
    "    # Filter to overlap period if dates provided\n",
    "    if start_date and end_date:\n",
    "        shifts_subset = shifts_df[shifts_df['Start'].dt.date.between(start_date, end_date)]\n",
    "        cancels_subset = cancellations_df[\n",
    "            cancellations_df['Created At'].dt.date.between(start_date, end_date)\n",
    "        ]\n",
    "        bookings_subset = bookings_df[\n",
    "            bookings_df['Created At'].dt.date.between(start_date, end_date)\n",
    "        ]\n",
    "    else:\n",
    "        shifts_subset = shifts_df\n",
    "        cancels_subset = cancellations_df\n",
    "        bookings_subset = bookings_df\n",
    "    \n",
    "    # Cross-reference data\n",
    "    shifts_with_outcomes = shifts_subset.copy()\n",
    "    shifts_with_outcomes['was_booked'] = shifts_subset['ID'].isin(bookings_subset['Shift ID'])\n",
    "    shifts_with_outcomes['was_cancelled'] = shifts_subset['ID'].isin(cancels_subset['Shift ID'])\n",
    "    \n",
    "    behavior_analysis = {\n",
    "        # Booking patterns\n",
    "        'booking_behavior': {\n",
    "            'lead_times': bookings_subset['Lead Time'].describe(),\n",
    "            'booking_times': bookings_subset['Created At'].dt.hour.value_counts().sort_index()\n",
    "        },\n",
    "        \n",
    "        # Cancellation patterns\n",
    "        'cancellation_behavior': {\n",
    "            'cancel_types': cancels_subset['Action'].value_counts(),\n",
    "            'lead_times': cancels_subset['Lead Time'].describe(),\n",
    "            'cancel_times': cancels_subset['Created At'].dt.hour.value_counts().sort_index()\n",
    "        },\n",
    "        \n",
    "        # Shift outcomes\n",
    "        'shift_outcomes': {\n",
    "            'total_shifts': len(shifts_subset),\n",
    "            'booked_count': shifts_with_outcomes['was_booked'].sum(),\n",
    "            'cancelled_count': shifts_with_outcomes['was_cancelled'].sum(),\n",
    "            'booking_rate': shifts_with_outcomes['was_booked'].mean(),\n",
    "            'cancellation_rate': shifts_with_outcomes['was_cancelled'].mean()\n",
    "        }\n",
    "    }\n",
    "    return behavior_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Missing Agent IDs Let's cross-check with both datasets:\n",
    "\n",
    "\n",
    "def analyze_missing_agents_behavior():\n",
    "    \"\"\"\n",
    "    Analyze what happens to shifts with missing Agent IDs\n",
    "    \"\"\"\n",
    "    # Get shifts with/without agents\n",
    "    missing_agent = shifts_df[shifts_df['Agent ID'].isnull()]\n",
    "    has_agent = shifts_df[shifts_df['Agent ID'].notnull()]\n",
    "    \n",
    "    # Cross reference with bookings and cancellations\n",
    "    missing_outcomes = {\n",
    "        'booked': missing_agent['ID'].isin(bookings_df['Shift ID']).mean(),\n",
    "        'cancelled': missing_agent['ID'].isin(cancellations_df['Shift ID']).mean(),\n",
    "        'verified': missing_agent['Verified'].mean()\n",
    "    }\n",
    "    \n",
    "    has_agent_outcomes = {\n",
    "        'booked': has_agent['ID'].isin(bookings_df['Shift ID']).mean(),\n",
    "        'cancelled': has_agent['ID'].isin(cancellations_df['Shift ID']).mean(),\n",
    "        'verified': has_agent['Verified'].mean()\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'missing_agent_outcomes': missing_outcomes,\n",
    "        'has_agent_outcomes': has_agent_outcomes\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all analyses\n",
    "print(\"Running comprehensive analyses...\")\n",
    "\n",
    "# 1. Full Shifts Analysis\n",
    "print(\"\\n=== COMPLETE SHIFTS ANALYSIS (Oct 2021 - Jan 2022) ===\")\n",
    "shifts_analysis = analyze_shifts_complete()\n",
    "print(\"\\nBasic Marketplace Metrics:\")\n",
    "print(f\"Total Shifts: {shifts_analysis['total_shifts']:,}\")\n",
    "print(\"\\nShift Types:\")\n",
    "print(shifts_analysis['shifts_by_type'])\n",
    "print(f\"\\nOverall Verification Rate: {shifts_analysis['verification_rate']:.2%}\")\n",
    "\n",
    "# 2. Behavioral Analysis \n",
    "# Using the overlap period (focusing on patterns rather than absolute numbers)\n",
    "print(\"\\n=== BEHAVIORAL ANALYSIS (Overlap Period) ===\")\n",
    "start_date = shifts_df['Start'].dt.date.min()  # Oct 1, 2021\n",
    "end_date = shifts_df['Start'].dt.date.max()    # Jan 31, 2022\n",
    "behavior_analysis = analyze_booking_behavior(start_date, end_date)\n",
    "\n",
    "print(\"\\nBooking Patterns:\")\n",
    "print(\"Lead Times (hours):\")\n",
    "print(behavior_analysis['booking_behavior']['lead_times'])\n",
    "\n",
    "print(\"\\nCancellation Types:\")\n",
    "print(behavior_analysis['cancellation_behavior']['cancel_types'])\n",
    "\n",
    "print(\"\\nShift Outcomes:\")\n",
    "for metric, value in behavior_analysis['shift_outcomes'].items():\n",
    "    if 'rate' in metric:\n",
    "        print(f\"{metric}: {value:.2%}\")\n",
    "    else:\n",
    "        print(f\"{metric}: {value:,}\")\n",
    "\n",
    "# 3. Missing Agent ID Analysis\n",
    "print(\"\\n=== MISSING AGENT ID ANALYSIS ===\")\n",
    "agent_behavior = analyze_missing_agents_behavior()\n",
    "\n",
    "print(\"\\nShifts with Missing Agent IDs:\")\n",
    "for metric, value in agent_behavior['missing_agent_outcomes'].items():\n",
    "    print(f\"{metric}: {value:.2%}\")\n",
    "\n",
    "print(\"\\nShifts with Agent IDs:\")\n",
    "for metric, value in agent_behavior['has_agent_outcomes'].items():\n",
    "    print(f\"{metric}: {value:.2%}\")\n",
    "\n",
    "# Store results in summary object for later use\n",
    "summary.add_summary('complete_analysis', 'shifts', shifts_analysis)\n",
    "summary.add_summary('complete_analysis', 'behavior', behavior_analysis)\n",
    "summary.add_summary('complete_analysis', 'missing_agents', agent_behavior)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This code:\n",
    "\n",
    "Analyzes the complete shifts dataset first\n",
    "Looks at booking/cancellation behavior in the overlap period\n",
    "Specifically examines shifts with/without Agent IDs\n",
    "Stores all results in our summary object\n",
    "\n",
    "The output will help us understand:\n",
    "\n",
    "Overall marketplace metrics from shifts data\n",
    "Behavioral patterns where we have complete data\n",
    "What missing Agent IDs might mean\n",
    "\n",
    "Each section is clearly labeled, and results are formatted for easy reading. We can use these results to:\n",
    "\n",
    "Identify key patterns\n",
    "Support our findings\n",
    "Guide additional analysis\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 1: SUCCESS PATH ANALYSIS\n",
    "#A. Define and Validate Success Metrics\n",
    "#Deep dive into successful shifts \n",
    "\n",
    "class ShiftSuccessAnalysis:\n",
    "    \"\"\"\n",
    "    Analyzes the complete lifecycle of shifts from posting to completion.\n",
    "    \n",
    "    Core metrics tracked:\n",
    "    - Booking success: Did the shift get booked?\n",
    "    - Retention success: Did the booking stick (no cancellation)?\n",
    "    - Completion success: Was the shift verified as worked?\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, shifts_df, bookings_df, cancellations_df):\n",
    "        \"\"\"Initialize with our three core datasets.\"\"\"\n",
    "        self.shifts_df = shifts_df.copy()\n",
    "        self.bookings_df = bookings_df.copy()\n",
    "        self.cancellations_df = cancellations_df.copy()\n",
    "        self.success_journey = None\n",
    "        \n",
    "        # Verify data compatibility\n",
    "        self._validate_data()\n",
    "        \n",
    "        # Create enhanced dataset\n",
    "        self._create_success_journey()\n",
    "    \n",
    "    def _validate_data(self):\n",
    "        \"\"\"\n",
    "        Ensure data quality and compatibility across datasets.\n",
    "        \"\"\"\n",
    "        # Check required columns\n",
    "        required_columns = {\n",
    "            'shifts': ['ID', 'Start', 'End', 'Verified', 'Agent ID', \n",
    "                      'Facility ID', 'Agent Req', 'Shift Type', 'Charge'],\n",
    "            'bookings': ['Shift ID', 'Created At', 'Worker ID'],\n",
    "            'cancellations': ['Shift ID', 'Created At', 'Action', 'Lead Time']\n",
    "        }\n",
    "        \n",
    "        for df_name, columns in required_columns.items():\n",
    "            df = getattr(self, f\"{df_name}_df\")\n",
    "            missing_cols = [col for col in columns if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing columns in {df_name}: {missing_cols}\")\n",
    "        \n",
    "        # Print coverage analysis\n",
    "        self._analyze_coverage()\n",
    "    \n",
    "    def _analyze_coverage(self):\n",
    "        \"\"\"Analyze data coverage and overlap.\"\"\"\n",
    "        shifts_ids = set(self.shifts_df['ID'])\n",
    "        booking_ids = set(self.bookings_df['Shift ID'])\n",
    "        cancel_ids = set(self.cancellations_df['Shift ID'])\n",
    "        \n",
    "        print(\"\\n=== Data Coverage Analysis ===\")\n",
    "        print(f\"\\nTotal Shifts: {len(shifts_ids):,}\")\n",
    "        print(f\"Shifts with Bookings: {len(shifts_ids & booking_ids):,} \"\n",
    "              f\"({len(shifts_ids & booking_ids)/len(shifts_ids):.1%})\")\n",
    "        print(f\"Shifts with Cancellations: {len(shifts_ids & cancel_ids):,} \"\n",
    "              f\"({len(shifts_ids & cancel_ids)/len(shifts_ids):.1%})\")\n",
    "        \n",
    "        # Analyze potential data quality issues\n",
    "        orphaned_bookings = len(booking_ids - shifts_ids)\n",
    "        orphaned_cancels = len(cancel_ids - shifts_ids)\n",
    "        \n",
    "        if orphaned_bookings or orphaned_cancels:\n",
    "            print(\"\\nPotential Data Quality Issues:\")\n",
    "            print(f\"Orphaned Bookings: {orphaned_bookings:,}\")\n",
    "            print(f\"Orphaned Cancellations: {orphaned_cancels:,}\")\n",
    "    \n",
    "    def _create_success_journey(self):\n",
    "        \"\"\"\n",
    "        Creates enhanced dataset tracking complete shift lifecycle.\n",
    "        \"\"\"\n",
    "        journey = self.shifts_df.copy()\n",
    "        \n",
    "        # Add booking information\n",
    "        booking_times = self.bookings_df.groupby('Shift ID').agg({\n",
    "            'Created At': ['first', 'count']\n",
    "        }).reset_index()\n",
    "        booking_times.columns = ['Shift ID', 'First Booking', 'Booking Count']\n",
    "        \n",
    "        journey = journey.merge(\n",
    "            booking_times, \n",
    "            left_on='ID', \n",
    "            right_on='Shift ID', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Add cancellation information\n",
    "        cancel_info = self.cancellations_df.groupby('Shift ID').agg({\n",
    "            'Created At': 'first',\n",
    "            'Action': 'first',\n",
    "            'Lead Time': 'first'\n",
    "        }).reset_index()\n",
    "        \n",
    "        journey = journey.merge(\n",
    "            cancel_info,\n",
    "            left_on='ID',\n",
    "            right_on='Shift ID',\n",
    "            how='left',\n",
    "            suffixes=('_booking', '_cancel')\n",
    "        )\n",
    "        \n",
    "        # Calculate success metrics\n",
    "        journey['was_booked'] = journey['First Booking'].notnull()\n",
    "        journey['was_cancelled'] = journey['Created At_cancel'].notnull()\n",
    "        journey['was_completed'] = journey['Verified']\n",
    "        \n",
    "        # Calculate time to shift start (from booking)\n",
    "        journey['lead_time'] = (\n",
    "            journey['Start'] - journey['First Booking']\n",
    "        ).dt.total_seconds() / 3600  # Convert to hours\n",
    "        \n",
    "        self.success_journey = journey\n",
    "        \n",
    "        # Print initial success metrics\n",
    "        self._print_success_metrics()\n",
    "    \n",
    "    def _print_success_metrics(self):\n",
    "        \"\"\"Print key success metrics from the journey data.\"\"\"\n",
    "        metrics = self.success_journey.agg({\n",
    "            'was_booked': 'mean',\n",
    "            'was_cancelled': 'mean',\n",
    "            'was_completed': 'mean'\n",
    "        })\n",
    "        \n",
    "        print(\"\\n=== Success Metrics ===\")\n",
    "        print(f\"Booking Rate: {metrics['was_booked']:.1%}\")\n",
    "        print(f\"Cancellation Rate: {metrics['was_cancelled']:.1%}\")\n",
    "        print(f\"Completion Rate: {metrics['was_completed']:.1%}\")\n",
    "    \n",
    "    def analyze_verification_discrepancy(self):\n",
    "        \"\"\"\n",
    "        Investigates why shifts might be verified without appearing in booking logs.\n",
    "        \"\"\"\n",
    "        verified_shifts = self.success_journey[self.success_journey['Verified']]\n",
    "        unbooked_verified = verified_shifts[~verified_shifts['was_booked']]\n",
    "        \n",
    "        results = {\n",
    "            'overview': {\n",
    "                'total_shifts': len(self.success_journey),\n",
    "                'verified_shifts': len(verified_shifts),\n",
    "                'unbooked_verified': len(unbooked_verified),\n",
    "                'verification_rate': len(verified_shifts) / len(self.success_journey),\n",
    "                'unbooked_verified_rate': len(unbooked_verified) / len(verified_shifts)\n",
    "            },\n",
    "            'unbooked_verified_patterns': {\n",
    "                'by_role': unbooked_verified['Agent Req'].value_counts(),\n",
    "                'by_shift_type': unbooked_verified['Shift Type'].value_counts(),\n",
    "                'by_facility': unbooked_verified['Facility ID'].value_counts().head()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        agent_patterns = self.success_journey.groupby(\n",
    "            self.success_journey['Agent ID'].isnull()\n",
    "        ).agg({\n",
    "            'was_booked': 'mean',\n",
    "            'Verified': 'mean',\n",
    "            'was_cancelled': 'mean'\n",
    "        }).round(3)\n",
    "        \n",
    "        results['agent_id_patterns'] = agent_patterns\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_success_patterns(self):\n",
    "        \"\"\"\n",
    "        Analyzes patterns in successfully completed shifts.\n",
    "        \"\"\"\n",
    "        successful = self.success_journey[\n",
    "            (self.success_journey['Verified']) & \n",
    "            (~self.success_journey['was_cancelled'])\n",
    "        ]\n",
    "        \n",
    "        patterns = {\n",
    "            'timing': {\n",
    "                'hour_distribution': successful['Start'].dt.hour.value_counts().sort_index(),\n",
    "                'day_distribution': successful['Start'].dt.day_name().value_counts(),\n",
    "                'lead_times': successful['lead_time'].describe()\n",
    "            },\n",
    "            'characteristics': {\n",
    "                'role_distribution': successful['Agent Req'].value_counts(),\n",
    "                'shift_types': successful['Shift Type'].value_counts(),\n",
    "                'charge_rates': successful.groupby('Agent Req')['Charge'].agg(['mean', 'std'])\n",
    "            },\n",
    "            'facility_patterns': {\n",
    "                'success_rates': (\n",
    "                    self.success_journey.groupby('Facility ID')['Verified'].agg(['mean', 'count'])\n",
    "                    .sort_values('mean', ascending=False)\n",
    "                    .query('count >= 10')  # Only facilities with sufficient data\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis\n",
    "analyzer = ShiftSuccessAnalysis(shifts_df, bookings_df, cancellations_df)\n",
    "\n",
    "# Analyze verification discrepancy\n",
    "discrepancy_results = analyzer.analyze_verification_discrepancy()\n",
    "\n",
    "print(\"\\n=== Verification Discrepancy Analysis ===\")\n",
    "print(\"\\nOverview:\")\n",
    "for metric, value in discrepancy_results['overview'].items():\n",
    "    if 'rate' in metric:\n",
    "        print(f\"{metric}: {value:.1%}\")\n",
    "    else:\n",
    "        print(f\"{metric}: {value:,}\")\n",
    "\n",
    "print(\"\\nUnbooked Verified Shifts by Role:\")\n",
    "print(discrepancy_results['unbooked_verified_patterns']['by_role'])\n",
    "\n",
    "print(\"\\nAgent ID Impact:\")\n",
    "print(discrepancy_results['agent_id_patterns'])\n",
    "\n",
    "# Get success patterns\n",
    "success_patterns = analyzer.get_success_patterns()\n",
    "\n",
    "print(\"\\n=== Success Patterns ===\")\n",
    "print(\"\\nMost Successful Shift Types:\")\n",
    "print(success_patterns['characteristics']['shift_types'])\n",
    "\n",
    "print(\"\\nAverage Charge Rates for Successful Shifts:\")\n",
    "print(success_patterns['characteristics']['charge_rates'])\n",
    "\n",
    "print(\"\\nTop 5 Facilities by Success Rate (min 10 shifts):\")\n",
    "print(success_patterns['facility_patterns']['success_rates'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing a path: backup pool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Backup Pool Estimation\n",
    "# The goal is to calculate the size of the backup pool needed to cover 75% of late cancellations.\n",
    "# Backup Pool Estimation\n",
    "\n",
    "# Assumptions\n",
    "BACKUP_COVERAGE_TARGET = 0.75  # Cover 75% of late cancellations\n",
    "\n",
    "def estimate_backup_pool(shifts_df, cancellations_df):\n",
    "    \"\"\"\n",
    "    Estimate the size of a backup pool needed to cover late cancellations.\n",
    "    \n",
    "    Parameters:\n",
    "    - shifts_df (pd.DataFrame): Shift data.\n",
    "    - cancellations_df (pd.DataFrame): Cancellations data with lead times.\n",
    "    \n",
    "    Returns:\n",
    "    - Estimated pool size needed for target coverage.\n",
    "    - Contextual insights into late cancellations.\n",
    "    \"\"\"\n",
    "    # Step 1: Focus on Late Cancellations (<4 hours)\n",
    "    late_cancellations = cancellations_df[\n",
    "        cancellations_df['Lead Time'] < 4\n",
    "    ]\n",
    "    total_late_cancels = len(late_cancellations)\n",
    "    \n",
    "    print(\"=== Backup Pool Estimation for Late Cancellations ===\")\n",
    "    print(f\"Total Late Cancellations (<4hrs): {total_late_cancels:,}\")\n",
    "    \n",
    "    # Step 2: Estimate coverage required\n",
    "    target_coverage = int(total_late_cancels * BACKUP_COVERAGE_TARGET)\n",
    "    print(f\"Target Coverage (75%): {target_coverage:,} shifts\")\n",
    "\n",
    "    # Step 3: Calculate HCP Availability and Estimate Pool Size\n",
    "    late_cancel_hcps = late_cancellations['Worker ID'].value_counts()\n",
    "    avg_shifts_per_hcp = late_cancel_hcps.mean()\n",
    "    \n",
    "    if avg_shifts_per_hcp > 0:\n",
    "        pool_size = int(np.ceil(target_coverage / avg_shifts_per_hcp))\n",
    "    else:\n",
    "        pool_size = 0\n",
    "    \n",
    "    print(f\"Average Late Cancellations per HCP: {avg_shifts_per_hcp:.2f}\")\n",
    "    print(f\"Estimated Backup Pool Size: {pool_size} HCPs\")\n",
    "    \n",
    "    print(\"\\nContext:\")\n",
    "    print(\"To meet 75% coverage of late cancellations, we estimate needing a pool of pre-vetted,\")\n",
    "    print(f\"reliable HCPs who can cover approximately {avg_shifts_per_hcp:.2f} late cancellations on average.\")\n",
    "    print(\"This estimate assumes that reliable HCPs are distributed evenly across cancellations.\")\n",
    "    \n",
    "    return pool_size\n",
    "\n",
    "def identify_reliable_hcps(bookings_df, cancellations_df, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Identify reliable HCPs with cancellation rates below a given threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - bookings_df (pd.DataFrame): Booking logs with Worker IDs.\n",
    "    - cancellations_df (pd.DataFrame): Cancellations data with Worker IDs.\n",
    "    - threshold (float): Maximum cancellation rate for reliability.\n",
    "    \n",
    "    Returns:\n",
    "    - Reliable HCPs as a DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Reliable HCP Identification ===\")\n",
    "    \n",
    "    # Step 1: Calculate Total Shifts and Cancellations per Worker\n",
    "    total_shifts = bookings_df['Worker ID'].value_counts()\n",
    "    total_cancellations = cancellations_df['Worker ID'].value_counts()\n",
    "    \n",
    "    # Step 2: Calculate Cancellation Rate\n",
    "    reliability_df = pd.DataFrame({\n",
    "        'Total Shifts': total_shifts,\n",
    "        'Cancellations': total_cancellations\n",
    "    }).fillna(0)\n",
    "    reliability_df['Cancellation Rate'] = reliability_df['Cancellations'] / reliability_df['Total Shifts']\n",
    "    \n",
    "    # Step 3: Identify Reliable Workers\n",
    "    reliable_hcps = reliability_df[reliability_df['Cancellation Rate'] <= threshold]\n",
    "    reliable_hcps_sorted = reliable_hcps.sort_values(by='Cancellation Rate')\n",
    "    \n",
    "    print(f\"Total Workers Analyzed: {len(reliability_df):,}\")\n",
    "    print(f\"Workers with Cancellation Rate  {threshold*100:.0f}%: {len(reliable_hcps):,}\")\n",
    "    print(\"\\nTop 5 Most Reliable Workers:\")\n",
    "    print(reliable_hcps_sorted.head())\n",
    "    \n",
    "    print(\"\\nContext:\")\n",
    "    print(\"Reliable HCPs are defined as those with a cancellation rate  10%.\")\n",
    "    print(\"This pool represents our most dependable workers, making them ideal candidates\")\n",
    "    print(\"for participation in the backup program. They are prioritized based on:\")\n",
    "    print(\"1. Total shifts worked.\")\n",
    "    print(\"2. Low cancellation counts.\")\n",
    "    \n",
    "    return reliable_hcps_sorted\n",
    "\n",
    "# Run the analyses\n",
    "backup_pool_size = estimate_backup_pool(shifts_df, cancellations_df)\n",
    "reliable_hcps = identify_reliable_hcps(bookings_df, cancellations_df)\n",
    "\n",
    "# Display final summary\n",
    "print(\"\\n=== Summary for WBD ===\")\n",
    "print(f\"Estimated Backup Pool Size (75% Late Cancel Coverage): {backup_pool_size} HCPs\")\n",
    "print(f\"Reliable HCPs Identified (Cancellation Rate  10%): {len(reliable_hcps)} workers\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CBH_CS2)",
   "language": "python",
   "name": "cbh_cs2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
