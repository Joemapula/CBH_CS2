{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Environment checks\n",
    "print(f\"Python executable: {sys.executable}\")  \n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"\\nFiles in data directory:\", os.listdir('data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions for Data Loading and Cleaning\n",
    "# update: now takes varied date ranges into account in loading\n",
    "def load_and_clean_shifts(df):\n",
    "    \"\"\"\n",
    "    Load and clean shifts dataset\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Raw shifts dataframe\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned shifts dataframe with proper datatypes\n",
    "        \n",
    "    Notes:\n",
    "        - Makes a copy to avoid modifying original data\n",
    "        - Converts datetime columns\n",
    "        - Handles potential errors in datetime conversion\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert datetime columns with error handling\n",
    "    datetime_cols = ['Start', 'End', 'Created At']\n",
    "    for col in datetime_cols:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col], format='mixed')\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {col} to datetime: {str(e)}\")\n",
    "                # Log problematic rows for investigation\n",
    "                problematic_rows = df[pd.to_datetime(df[col], format='mixed', errors='coerce').isna()]\n",
    "                if not problematic_rows.empty:\n",
    "                    print(f\"Problematic rows in {col}:\")\n",
    "                    print(problematic_rows[col].head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_and_clean_bookings(df):\n",
    "    \"\"\"\n",
    "    Load and clean booking logs dataset\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Raw bookings dataframe\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned bookings dataframe with proper datatypes\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Convert datetime columns\n",
    "    try:\n",
    "        df['Created At'] = pd.to_datetime(df['Created At'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting Created At: {str(e)}\")\n",
    "    return df\n",
    "\n",
    "def load_and_clean_cancellations(df):\n",
    "    \"\"\"\n",
    "    Load and clean cancellation logs dataset\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Raw cancellations dataframe\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned cancellations dataframe with proper datatypes\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Convert datetime columns with flexible parsing\n",
    "    try:\n",
    "        df['Created At'] = pd.to_datetime(df['Created At'], format='mixed')\n",
    "        df['Shift Start Logs'] = pd.to_datetime(df['Shift Start Logs'], format='mixed')\n",
    "    except Exception as e:\n",
    "        print(f\"Error in datetime conversion: {str(e)}\")\n",
    "        # Try to identify problematic rows\n",
    "        prob_rows = df[pd.to_datetime(df['Shift Start Logs'], format='mixed', errors='coerce').isna()]\n",
    "        if not prob_rows.empty:\n",
    "            print(\"\\nSample of problematic date formats:\")\n",
    "            print(prob_rows['Shift Start Logs'].head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def categorize_lead_time(hours):\n",
    "    \"\"\"\n",
    "    Categorize lead times based on business rules.\n",
    "    \n",
    "    Parameters:\n",
    "        hours (float): Lead time in hours\n",
    "        \n",
    "    Returns:\n",
    "        str: Category of lead time\n",
    "    \"\"\"\n",
    "    if hours < 0:\n",
    "        return 'No-Show'  # Cancelled after shift start\n",
    "    elif hours < 4:\n",
    "        return 'Late (<4hrs)'\n",
    "    elif hours < 24:\n",
    "        return 'Same Day'\n",
    "    elif hours < 72:\n",
    "        return 'Advance (<3 days)'\n",
    "    return 'Early (3+ days)'\n",
    "\n",
    "def clean_lead_times(cancellations_df):\n",
    "    \"\"\"\n",
    "    Clean and categorize lead times in cancellation data\n",
    "    \n",
    "    Parameters:\n",
    "        cancellations_df (pd.DataFrame): Raw cancellations dataframe\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned cancellations data with categorized lead times\n",
    "        pd.Series: Statistics about removed records for quality control\n",
    "    \"\"\"\n",
    "    df = cancellations_df.copy()\n",
    "    \n",
    "    # Track data quality issues\n",
    "    quality_stats = {\n",
    "        'original_rows': len(df),\n",
    "        'null_lead_times': df['Lead Time'].isnull().sum(),\n",
    "        'infinite_values': (~np.isfinite(df['Lead Time'])).sum()\n",
    "    }\n",
    "    \n",
    "    # Only remove truly invalid data\n",
    "    mask = df['Lead Time'].notnull() & np.isfinite(df['Lead Time'])\n",
    "    df = df[mask]\n",
    "    \n",
    "    # Add cleaned lead time without filtering extremes\n",
    "    df['clean_lead_time'] = df['Lead Time']\n",
    "    \n",
    "    # Categorize all lead times\n",
    "    df['cancellation_category'] = df['clean_lead_time'].apply(categorize_lead_time)\n",
    "    \n",
    "    # Add flags for extreme values for analysis\n",
    "    df['is_extreme_negative'] = df['Lead Time'] < -72  # Flag cancellations >3 days after\n",
    "    df['is_extreme_positive'] = df['Lead Time'] > 1000 # Flag cancellations >41 days before\n",
    "    \n",
    "    quality_stats['final_rows'] = len(df)\n",
    "    quality_stats['removed_rows'] = quality_stats['original_rows'] - quality_stats['final_rows']\n",
    "    \n",
    "    return df, pd.Series(quality_stats)\n",
    "\n",
    "# Data Summary Storage Class\n",
    "class DataSummary:\n",
    "    \"\"\"Class to store and manage analysis results\"\"\"\n",
    "    def __init__(self):\n",
    "        self.summaries = {}\n",
    "    \n",
    "    def add_summary(self, dataset_name, summary_type, data):\n",
    "        \"\"\"Add summary statistics to storage\"\"\"\n",
    "        if dataset_name not in self.summaries:\n",
    "            self.summaries[dataset_name] = {}\n",
    "        self.summaries[dataset_name][summary_type] = data\n",
    "    \n",
    "    def get_summary(self, dataset_name, summary_type=None):\n",
    "        \"\"\"Retrieve stored summary statistics\"\"\"\n",
    "        if summary_type:\n",
    "            return self.summaries.get(dataset_name, {}).get(summary_type)\n",
    "        return self.summaries.get(dataset_name)\n",
    "    \n",
    "    def print_summary(self, dataset_name):\n",
    "        \"\"\"Print stored summaries for a dataset\"\"\"\n",
    "        if dataset_name in self.summaries:\n",
    "            print(f\"\\nSummary for {dataset_name}:\")\n",
    "            for summary_type, data in self.summaries[dataset_name].items():\n",
    "                print(f\"\\n{summary_type}:\")\n",
    "                print(data)\n",
    "\n",
    "# Initialize summary storage\n",
    "summary = DataSummary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary explorer helper functions \n",
    "def explore_summary(summary, indent=0, max_display_length=100):\n",
    "    \"\"\"\n",
    "    Recursively explore and display the contents of the DataSummary object.\n",
    "    \n",
    "    Parameters:\n",
    "        summary: The DataSummary object or a nested dictionary/value to explore\n",
    "        indent: Current indentation level (default: 0)\n",
    "        max_display_length: Maximum length for displayed values (default: 100)\n",
    "    \"\"\"\n",
    "    def format_value(value):\n",
    "        \"\"\"Format a value for display, truncating if too long\"\"\"\n",
    "        str_value = str(value)\n",
    "        if len(str_value) > max_display_length:\n",
    "            return str_value[:max_display_length] + '...'\n",
    "        return str_value\n",
    "    \n",
    "    def print_indented(text, indent):\n",
    "        \"\"\"Print text with proper indentation\"\"\"\n",
    "        print('    ' * indent + text)\n",
    "\n",
    "    if isinstance(summary, DataSummary):\n",
    "        # If we're starting with a DataSummary object, explore its summaries\n",
    "        print(\"\\n=== Complete Summary Contents ===\\n\")\n",
    "        explore_summary(summary.summaries, indent)\n",
    "    \n",
    "    elif isinstance(summary, dict):\n",
    "        # Recursively explore dictionary contents\n",
    "        for key, value in summary.items():\n",
    "            if isinstance(value, dict):\n",
    "                print_indented(f\"{key}:\", indent)\n",
    "                explore_summary(value, indent + 1)\n",
    "            elif isinstance(value, pd.Series) or isinstance(value, pd.DataFrame):\n",
    "                print_indented(f\"{key}: [pandas {type(value).__name__}]\", indent)\n",
    "                str_repr = str(value)\n",
    "                for line in str_repr.split('\\n')[:5]:  # Show first 5 lines\n",
    "                    print_indented(line, indent + 1)\n",
    "                if len(str_repr.split('\\n')) > 5:\n",
    "                    print_indented('...', indent + 1)\n",
    "            else:\n",
    "                print_indented(f\"{key}: {format_value(value)}\", indent)\n",
    "    \n",
    "    else:\n",
    "        # Base case: print the value\n",
    "        print_indented(format_value(summary), indent)\n",
    "\n",
    "def get_summary_structure(summary):\n",
    "    \"\"\"\n",
    "    Print just the structure of the summary without all the data values.\n",
    "    \n",
    "    Parameters:\n",
    "        summary: The DataSummary object\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Summary Structure ===\\n\")\n",
    "    for dataset_name in summary.summaries:\n",
    "        print(f\"\\nDataset: {dataset_name}\")\n",
    "        for summary_type in summary.summaries[dataset_name]:\n",
    "            print(f\"  └── {summary_type}\")\n",
    "\n",
    "# Example usage:\n",
    "print(\"First, let's see the overall structure of your summary:\")\n",
    "get_summary_structure(summary)\n",
    "\n",
    "print(\"\\nWould you like to see the complete contents of any specific dataset?\")\n",
    "print(\"You can view them using:\")\n",
    "print(\"explore_summary(summary.get_summary('dataset_name'))\")\n",
    "\n",
    "# To view everything:\n",
    "print(\"\\nOr view all contents with:\")\n",
    "print(\"explore_summary(summary)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Loading and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load and Prepare All Datasets ===\n",
    "print(\"Loading and preparing all datasets...\")\n",
    "\n",
    "# Load all datasets\n",
    "shifts_df = pd.read_csv('data/cleveland_shifts_large.csv')\n",
    "bookings_df = pd.read_csv('data/booking_logs_large.csv')\n",
    "cancellations_df = pd.read_csv('data/cancel_logs_large.csv')\n",
    "\n",
    "def get_overlapping_date_range(shifts_df, bookings_df, cancellations_df):\n",
    "    \"\"\"\n",
    "    Determine the overlapping date range across all three datasets.\n",
    "    Returns the start and end dates that represent the period where we have complete data.\n",
    "    \n",
    "    The overlapping range is determined by:\n",
    "    - Latest start date among all datasets (to ensure we have data from all sources)\n",
    "    - Earliest end date among all datasets (to ensure we don't exceed any dataset's range)\n",
    "    \"\"\"\n",
    "    # Get date ranges for each dataset\n",
    "    shifts_range = {\n",
    "        'start': shifts_df['Created At'].min(),\n",
    "        'end': shifts_df['Created At'].max()\n",
    "    }\n",
    "    bookings_range = {\n",
    "        'start': bookings_df['Created At'].min(),\n",
    "        'end': bookings_df['Created At'].max()\n",
    "    }\n",
    "    cancellations_range = {\n",
    "        'start': cancellations_df['Created At'].min(),\n",
    "        'end': cancellations_df['Created At'].max()\n",
    "    }\n",
    "    \n",
    "    # Find overlapping range\n",
    "    overlap_start = max(\n",
    "        shifts_range['start'],\n",
    "        bookings_range['start'],\n",
    "        cancellations_range['start']\n",
    "    )\n",
    "    \n",
    "    overlap_end = min(\n",
    "        shifts_range['end'],\n",
    "        bookings_range['end'],\n",
    "        cancellations_range['end']\n",
    "    )\n",
    "    \n",
    "    return overlap_start, overlap_end\n",
    "\n",
    "def filter_to_overlap_period(df, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Filter a dataframe to only include rows within the overlapping date range.\n",
    "    \"\"\"\n",
    "    return df[\n",
    "        (df['Created At'] >= start_date) & \n",
    "        (df['Created At'] <= end_date)\n",
    "    ]\n",
    "\n",
    "# After your existing data loading code, add:\n",
    "# Find overlapping period\n",
    "overlap_start, overlap_end = get_overlapping_date_range(shifts_df, bookings_df, cancellations_df)\n",
    "\n",
    "# Filter all datasets to overlapping period\n",
    "shifts_df = filter_to_overlap_period(shifts_df, overlap_start, overlap_end)\n",
    "bookings_df = filter_to_overlap_period(bookings_df, overlap_start, overlap_end)\n",
    "cancellations_df = filter_to_overlap_period(cancellations_df, overlap_start, overlap_end)\n",
    "\n",
    "# Print information about the filtering\n",
    "print(\"\\n=== Data Filtering Summary ===\")\n",
    "print(f\"Analysis Period: {overlap_start} to {overlap_end}\")\n",
    "print(\"\\nDataset sizes after filtering:\")\n",
    "print(f\"Shifts: {len(shifts_df):,} records\")\n",
    "print(f\"Bookings: {len(bookings_df):,} records\")\n",
    "print(f\"Cancellations: {len(cancellations_df):,} records\")\n",
    "\n",
    "# Store filtering info in summary\n",
    "summary.add_summary('data_filtering', 'overlap_period', {\n",
    "    'start': overlap_start,\n",
    "    'end': overlap_end,\n",
    "    'original_sizes': {\n",
    "        'shifts': len(shifts_df),\n",
    "        'bookings': len(bookings_df),\n",
    "        'cancellations': len(cancellations_df)\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and prepare the data\n",
    "shifts_df = load_and_clean_shifts(shifts_df)\n",
    "bookings_df = load_and_clean_bookings(bookings_df)\n",
    "cancellations_df = load_and_clean_cancellations(cancellations_df)\n",
    "\n",
    "# Function to analyze and summarize a dataset\n",
    "def analyze_dataset(df, dataset_name, summary):\n",
    "    print(f\"\\n=== {dataset_name} Data Overview ===\")\n",
    "    print(\"Dataset Shape:\", df.shape)\n",
    "    print(\"\\nColumns:\", df.columns.tolist())\n",
    "    print(\"\\nData Types:\\n\", df.dtypes)\n",
    "    \n",
    "    # Missing value analysis\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(\"\\nMissing Values:\\n\", missing_values)\n",
    "    \n",
    "    # Display sample data\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Store findings in summary\n",
    "    summary.add_summary(dataset_name.lower(), 'shape', df.shape)\n",
    "    summary.add_summary(dataset_name.lower(), 'dtypes', df.dtypes)\n",
    "    summary.add_summary(dataset_name.lower(), 'missing_values', missing_values)\n",
    "    \n",
    "    # Additional temporal analysis\n",
    "    date_range = {\n",
    "        'start_date': pd.to_datetime(df['Created At']).min(),\n",
    "        'end_date': pd.to_datetime(df['Created At']).max(),\n",
    "        'total_days': (pd.to_datetime(df['Created At']).max() - pd.to_datetime(df['Created At']).min()).days\n",
    "    }\n",
    "    summary.add_summary(dataset_name.lower(), 'date_range', date_range)\n",
    "    \n",
    "    # Record unique IDs count\n",
    "    unique_ids = df['ID'].nunique()\n",
    "    summary.add_summary(dataset_name.lower(), 'unique_ids', unique_ids)\n",
    "    \n",
    "    return date_range, unique_ids\n",
    "\n",
    "# Analyze each dataset\n",
    "datasets = {\n",
    "    'Shifts': shifts_df,\n",
    "    'Bookings': bookings_df,\n",
    "    'Cancellations': cancellations_df\n",
    "}\n",
    "\n",
    "print(\"\\n=== Dataset Analysis ===\")\n",
    "for name, df in datasets.items():\n",
    "    date_range, unique_ids = analyze_dataset(df, name, summary)\n",
    "    print(f\"\\n{name} Dataset Summary:\")\n",
    "    print(f\"Date Range: {date_range['start_date']} to {date_range['end_date']} ({date_range['total_days']} days)\")\n",
    "    print(f\"Unique IDs: {unique_ids}\")\n",
    "\n",
    "# Cross-dataset validation\n",
    "print(\"\\n=== Cross-Dataset Validation ===\")\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{name} Dataset:\")\n",
    "    print(f\"Total Records: {len(df)}\")\n",
    "    print(f\"Records per Day: {len(df) / (pd.to_datetime(df['Created At']).max() - pd.to_datetime(df['Created At']).min()).days:.2f}\")\n",
    "\n",
    "# Check for overlapping IDs between datasets\n",
    "print(\"\\n=== ID Overlap Analysis ===\")\n",
    "shifts_ids = set(shifts_df['ID'])\n",
    "bookings_ids = set(bookings_df['ID'])\n",
    "cancellations_ids = set(cancellations_df['ID'])\n",
    "\n",
    "overlap_analysis = {\n",
    "    'shifts_bookings': len(shifts_ids.intersection(bookings_ids)),\n",
    "    'shifts_cancellations': len(shifts_ids.intersection(cancellations_ids)),\n",
    "    'bookings_cancellations': len(bookings_ids.intersection(cancellations_ids))\n",
    "}\n",
    "\n",
    "summary.add_summary('cross_validation', 'id_overlaps', overlap_analysis)\n",
    "\n",
    "print(\"ID Overlaps:\")\n",
    "print(f\"Shifts-Bookings: {overlap_analysis['shifts_bookings']}\")\n",
    "print(f\"Shifts-Cancellations: {overlap_analysis['shifts_cancellations']}\")\n",
    "print(f\"Bookings-Cancellations: {overlap_analysis['bookings_cancellations']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note look here \n",
    "# best and worse facilities and hcps \n",
    "# to do check this, doesn't seem right \n",
    "# once checked, add to summary \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def analyze_hcp_patterns(shifts_df, bookings_df, cancellations_df):\n",
    "    \"\"\"\n",
    "    Analyze HCP patterns across all datasets\n",
    "    \"\"\"\n",
    "    # Count HCP appearances in each dataset\n",
    "    shifts_hcp_counts = shifts_df['Agent ID'].value_counts()\n",
    "    bookings_hcp_counts = bookings_df['Worker ID'].value_counts()\n",
    "    cancellations_hcp_counts = cancellations_df['Worker ID'].value_counts()\n",
    "    \n",
    "    # Get unique HCP counts\n",
    "    unique_hcps = {\n",
    "        'shifts': shifts_df['Agent ID'].nunique(),\n",
    "        'bookings': bookings_df['Worker ID'].nunique(),\n",
    "        'cancellations': cancellations_df['Worker ID'].nunique()\n",
    "    }\n",
    "    \n",
    "    # Find overlapping HCPs\n",
    "    shifts_hcps = set(shifts_df['Agent ID'].dropna())\n",
    "    bookings_hcps = set(bookings_df['Worker ID'].dropna())\n",
    "    cancellations_hcps = set(cancellations_df['Worker ID'].dropna())\n",
    "    \n",
    "    hcp_overlaps = {\n",
    "        'shifts_bookings': len(shifts_hcps.intersection(bookings_hcps)),\n",
    "        'shifts_cancellations': len(shifts_hcps.intersection(cancellations_hcps)),\n",
    "        'bookings_cancellations': len(bookings_hcps.intersection(cancellations_hcps))\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'hcp_counts': {\n",
    "            'shifts': shifts_hcp_counts,\n",
    "            'bookings': bookings_hcp_counts,\n",
    "            'cancellations': cancellations_hcp_counts\n",
    "        },\n",
    "        'unique_counts': unique_hcps,\n",
    "        'overlaps': hcp_overlaps\n",
    "    }\n",
    "\n",
    "def analyze_facility_patterns(shifts_df, bookings_df, cancellations_df):\n",
    "    \"\"\"\n",
    "    Analyze facility patterns across all datasets\n",
    "    \"\"\"\n",
    "    # Count shifts per facility\n",
    "    facility_shift_counts = shifts_df['Facility ID'].value_counts()\n",
    "    \n",
    "    # Count bookings per facility\n",
    "    facility_booking_counts = bookings_df['Facility ID'].value_counts()\n",
    "    \n",
    "    # Count cancellations per facility\n",
    "    facility_cancel_counts = cancellations_df['Facility ID'].value_counts()\n",
    "    \n",
    "    # Calculate cancellation rates per facility\n",
    "    facility_stats = pd.DataFrame({\n",
    "        'total_shifts': facility_shift_counts,\n",
    "        'total_bookings': facility_booking_counts.reindex(facility_shift_counts.index).fillna(0),\n",
    "        'total_cancellations': facility_cancel_counts.reindex(facility_shift_counts.index).fillna(0)\n",
    "    })\n",
    "    \n",
    "    facility_stats['cancellation_rate'] = (\n",
    "        facility_stats['total_cancellations'] / facility_stats['total_bookings']\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Get unique facility counts\n",
    "    unique_facilities = {\n",
    "        'shifts': shifts_df['Facility ID'].nunique(),\n",
    "        'bookings': bookings_df['Facility ID'].nunique(),\n",
    "        'cancellations': cancellations_df['Facility ID'].nunique()\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'facility_stats': facility_stats,\n",
    "        'unique_counts': unique_facilities\n",
    "    }\n",
    "\n",
    "# Run the analysis\n",
    "hcp_analysis = analyze_hcp_patterns(shifts_df, bookings_df, cancellations_df)\n",
    "facility_analysis = analyze_facility_patterns(shifts_df, bookings_df, cancellations_df)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n=== HCP Analysis ===\")\n",
    "print(f\"Unique HCPs in shifts: {hcp_analysis['unique_counts']['shifts']}\")\n",
    "print(f\"Unique HCPs in bookings: {hcp_analysis['unique_counts']['bookings']}\")\n",
    "print(f\"Unique HCPs in cancellations: {hcp_analysis['unique_counts']['cancellations']}\")\n",
    "print(\"\\nHCP Overlaps:\")\n",
    "for overlap_type, count in hcp_analysis['overlaps'].items():\n",
    "    print(f\"{overlap_type}: {count}\")\n",
    "\n",
    "print(\"\\n=== Facility Analysis ===\")\n",
    "print(f\"Unique facilities in shifts: {facility_analysis['unique_counts']['shifts']}\")\n",
    "print(f\"Unique facilities in bookings: {facility_analysis['unique_counts']['bookings']}\")\n",
    "print(f\"Unique facilities in cancellations: {facility_analysis['unique_counts']['cancellations']}\")\n",
    "\n",
    "print(\"\\nFacility Statistics Summary:\")\n",
    "print(facility_analysis['facility_stats'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look here for patterns of the worst and best HCPs and Facilities\n",
    "# to do check this and move it. numbers don't seem right \n",
    "# once checked, add to summary \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def analyze_hcp_reliability(shifts_df, bookings_df, cancellations_df):\n",
    "    \"\"\"\n",
    "    Analyze HCP reliability patterns, identifying best and worst performers\n",
    "    \n",
    "    Returns DataFrame with metrics per HCP including:\n",
    "    - Total shifts worked\n",
    "    - Cancellation rate\n",
    "    - No-show rate\n",
    "    - Average lead time for cancellations\n",
    "    - Types of shifts typically worked/cancelled\n",
    "    \"\"\"\n",
    "    # Prepare HCP metrics\n",
    "    hcp_metrics = pd.DataFrame()\n",
    "    \n",
    "    # Count verified shifts per HCP\n",
    "    verified_shifts = shifts_df[shifts_df['Verified'] == True].groupby('Agent ID').size()\n",
    "    hcp_metrics['total_shifts_worked'] = verified_shifts\n",
    "    \n",
    "    # Calculate cancellation metrics per HCP\n",
    "    cancellations_by_hcp = cancellations_df.groupby('Worker ID').agg({\n",
    "        'ID': 'count',  # Total cancellations\n",
    "        'Lead Time': ['mean', 'median'],  # Timing patterns\n",
    "        'Action': lambda x: (x == 'NO_CALL_NO_SHOW').mean()  # No-show rate\n",
    "    })\n",
    "    \n",
    "    cancellations_by_hcp.columns = ['total_cancellations', 'avg_lead_time', 'median_lead_time', 'no_show_rate']\n",
    "    \n",
    "    # Merge metrics\n",
    "    hcp_metrics = hcp_metrics.join(cancellations_by_hcp, how='outer')\n",
    "    \n",
    "    # Calculate reliability score\n",
    "    hcp_metrics['cancellation_rate'] = hcp_metrics['total_cancellations'] / (hcp_metrics['total_shifts_worked'] + hcp_metrics['total_cancellations'])\n",
    "    hcp_metrics['reliability_score'] = (1 - hcp_metrics['cancellation_rate']) * (1 - hcp_metrics['no_show_rate'])\n",
    "    \n",
    "    # Fill NaN values appropriately\n",
    "    hcp_metrics = hcp_metrics.fillna({\n",
    "        'total_shifts_worked': 0,\n",
    "        'total_cancellations': 0,\n",
    "        'cancellation_rate': 0,\n",
    "        'no_show_rate': 0,\n",
    "        'reliability_score': 1  # Perfect score for those with no cancellations\n",
    "    })\n",
    "    \n",
    "    return hcp_metrics\n",
    "\n",
    "def analyze_facility_reliability(shifts_df, bookings_df, cancellations_df):\n",
    "    \"\"\"\n",
    "    Analyze facility patterns, identifying stable and unstable facilities\n",
    "    \n",
    "    Returns DataFrame with metrics per facility including:\n",
    "    - Total shifts posted\n",
    "    - Deletion rate\n",
    "    - Booking rate\n",
    "    - Average time to fill shifts\n",
    "    - Cancellation exposure\n",
    "    \"\"\"\n",
    "    facility_metrics = pd.DataFrame()\n",
    "    \n",
    "    # Analyze shift postings and deletions\n",
    "    facility_shifts = shifts_df.groupby('Facility ID').agg({\n",
    "        'ID': 'count',  # Total shifts posted\n",
    "        'Deleted': lambda x: x.notna().sum(),  # Deleted shifts\n",
    "        'Verified': 'sum',  # Worked shifts\n",
    "        'Charge': 'mean'  # Average pay rate\n",
    "    })\n",
    "    \n",
    "    facility_metrics['total_shifts_posted'] = facility_shifts['ID']\n",
    "    facility_metrics['deleted_shifts'] = facility_shifts['Deleted']\n",
    "    facility_metrics['verified_shifts'] = facility_shifts['Verified']\n",
    "    facility_metrics['avg_pay_rate'] = facility_shifts['Charge']\n",
    "    \n",
    "    # Calculate deletion rate\n",
    "    facility_metrics['deletion_rate'] = facility_metrics['deleted_shifts'] / facility_metrics['total_shifts_posted']\n",
    "    \n",
    "    # Calculate booking success (excluding deleted shifts)\n",
    "    active_shifts = facility_metrics['total_shifts_posted'] - facility_metrics['deleted_shifts']\n",
    "    facility_metrics['booking_rate'] = facility_metrics['verified_shifts'] / active_shifts\n",
    "    \n",
    "    # Analyze cancellations exposure\n",
    "    cancellations_by_facility = cancellations_df.groupby('Facility ID').agg({\n",
    "        'ID': 'count',  # Total cancellations\n",
    "        'Action': lambda x: (x == 'NO_CALL_NO_SHOW').sum()  # No-shows\n",
    "    })\n",
    "    \n",
    "    facility_metrics['total_cancellations'] = cancellations_by_facility['ID']\n",
    "    facility_metrics['total_no_shows'] = cancellations_by_facility['Action']\n",
    "    \n",
    "    # Fill NaN values\n",
    "    facility_metrics = facility_metrics.fillna({\n",
    "        'total_cancellations': 0,\n",
    "        'total_no_shows': 0\n",
    "    })\n",
    "    \n",
    "    # Calculate overall stability score\n",
    "    facility_metrics['stability_score'] = (\n",
    "        (1 - facility_metrics['deletion_rate']) * \n",
    "        facility_metrics['booking_rate'] * \n",
    "        (1 - facility_metrics['total_cancellations'] / facility_metrics['total_shifts_posted'])\n",
    "    )\n",
    "    \n",
    "    return facility_metrics\n",
    "\n",
    "# Run analyses\n",
    "hcp_performance = analyze_hcp_reliability(shifts_df, bookings_df, cancellations_df)\n",
    "facility_performance = analyze_facility_reliability(shifts_df, bookings_df, cancellations_df)\n",
    "\n",
    "# Print summary of top and bottom performers\n",
    "print(\"\\n=== Top Performing HCPs (Minimum 10 shifts) ===\")\n",
    "reliable_hcps = hcp_performance[hcp_performance['total_shifts_worked'] >= 10]\n",
    "print(reliable_hcps.nlargest(5, 'reliability_score'))\n",
    "\n",
    "print(\"\\n=== Struggling HCPs (Minimum 10 shifts) ===\")\n",
    "print(reliable_hcps.nsmallest(5, 'reliability_score'))\n",
    "\n",
    "print(\"\\n=== Most Stable Facilities (Minimum 20 shifts) ===\")\n",
    "active_facilities = facility_performance[facility_performance['total_shifts_posted'] >= 20]\n",
    "print(active_facilities.nlargest(5, 'stability_score'))\n",
    "\n",
    "print(\"\\n=== Less Stable Facilities (Minimum 20 shifts) ===\")\n",
    "print(active_facilities.nsmallest(5, 'stability_score'))\n",
    "\n",
    "# Calculate key statistics for potential interventions\n",
    "print(\"\\n=== Key Marketplace Statistics ===\")\n",
    "print(f\"Overall marketplace cancellation rate: {(cancellations_df['ID'].count() / bookings_df['ID'].count()):.2%}\")\n",
    "print(f\"Overall shift deletion rate: {(shifts_df['Deleted'].notna().sum() / len(shifts_df)):.2%}\")\n",
    "print(f\"No-show percentage of cancellations: {(cancellations_df['Action'] == 'NO_CALL_NO_SHOW').mean():.2%}\")\n",
    "\n",
    "# Additional pattern analysis\n",
    "hcp_performance['shift_volume_category'] = pd.qcut(\n",
    "    hcp_performance['total_shifts_worked'], \n",
    "    q=4, \n",
    "    labels=['Low', 'Medium-Low', 'Medium-High', 'High']\n",
    ")\n",
    "\n",
    "volume_reliability = hcp_performance.groupby('shift_volume_category')['reliability_score'].mean()\n",
    "print(\"\\n=== Reliability by HCP Volume ===\")\n",
    "print(volume_reliability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Dive (formerly Data Quality Analysis) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancellations and Lead time analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note date ranges are all equal now \n",
    "# \n",
    "# 41k unique shifts, 127k unique bookings, 78k unique cancels -> 41 + 78 ~ 120 seems reasonable but lots of \n",
    "# this means each shift is booked 3 times and canceled twice? \n",
    "\n",
    "# === Lead Time Analysis ===\n",
    "print(\"Analyzing lead times and cancellation patterns...\")\n",
    "\n",
    "# Clean lead times and get quality stats\n",
    "clean_cancellations, quality_stats = clean_lead_times(cancellations_df)\n",
    "\n",
    "# Basic cancellation metrics \n",
    "shifts_with_cancellations = len(set(shifts_df['ID']) & set(cancellations_df['Shift ID']))\n",
    "print(f\"Shifts with cancellations: {shifts_with_cancellations}\")\n",
    "# to do - this may be incorrect due to date overlap \n",
    "print(f\"Percentage of shifts cancelled: {(shifts_with_cancellations/len(shifts_df))*100:.2f}%\")\n",
    "\n",
    "# start \n",
    "print(f\"Overlap only here\")\n",
    "# Find overlapping date range\n",
    "shifts_start = shifts_df['Created At'].min()\n",
    "shifts_end = shifts_df['Created At'].max()\n",
    "cancellations_start = cancellations_df['Created At'].min()\n",
    "cancellations_end = cancellations_df['Created At'].max()\n",
    "\n",
    "# Get the overlapping range\n",
    "overlap_start = max(shifts_start, cancellations_start)\n",
    "overlap_end = min(shifts_end, cancellations_end)\n",
    "\n",
    "# Filter both datasets to overlapping period\n",
    "shifts_in_range = shifts_df[\n",
    "    (shifts_df['Created At'] >= overlap_start) & \n",
    "    (shifts_df['Created At'] <= overlap_end)\n",
    "]\n",
    "\n",
    "cancellations_in_range = cancellations_df[\n",
    "    (cancellations_df['Created At'] >= overlap_start) & \n",
    "    (cancellations_df['Created At'] <= overlap_end)\n",
    "]\n",
    "\n",
    "# Calculate metrics using filtered data\n",
    "shifts_with_cancellations = len(set(shifts_in_range['ID']) & set(cancellations_in_range['Shift ID']))\n",
    "total_shifts_in_range = len(shifts_in_range)\n",
    "\n",
    "cancellation_percentage = (shifts_with_cancellations / total_shifts_in_range) * 100\n",
    "\n",
    "print(f\"Date range analyzed: {overlap_start.date()} to {overlap_end.date()}\")\n",
    "print(f\"Shifts with cancellations: {shifts_with_cancellations}\")\n",
    "print(f\"Total shifts in range: {total_shifts_in_range}\")\n",
    "print(f\"Percentage of shifts cancelled: {cancellation_percentage:.2f}%\")\n",
    "#end \n",
    "\n",
    "\n",
    "print(\"\\n=== Data Quality Statistics ===\")\n",
    "print(quality_stats)\n",
    "\n",
    "print(\"\\n=== Lead Time Distribution ===\")\n",
    "print(\"Overall Lead Time Statistics:\")\n",
    "print(cancellations_df['Lead Time'].describe().round(2))\n",
    "\n",
    "print(\"\\n=== Cancellation Categories ===\")\n",
    "print(clean_cancellations['cancellation_category'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n=== Extreme Values Analysis ===\")\n",
    "print(f\"Very Late Cancellations (>3 days after): {clean_cancellations['is_extreme_negative'].sum()}\")\n",
    "print(f\"Very Early Cancellations (>41 days before): {clean_cancellations['is_extreme_positive'].sum()}\")\n",
    "\n",
    "# Distribution of lead times for extreme cases\n",
    "if clean_cancellations['is_extreme_negative'].any():\n",
    "    print(\"\\nVery Late Cancellation Stats:\")\n",
    "    print(clean_cancellations[clean_cancellations['is_extreme_negative']]['Lead Time'].describe())\n",
    "\n",
    "if clean_cancellations['is_extreme_positive'].any():\n",
    "    print(\"\\nVery Early Cancellation Stats:\")\n",
    "    print(clean_cancellations[clean_cancellations['is_extreme_positive']]['Lead Time'].describe())\n",
    "\n",
    "# Store results in summary\n",
    "summary.add_summary('cancellations', 'quality_stats', quality_stats.to_dict())\n",
    "summary.add_summary('cancellations', 'extreme_values', {\n",
    "    'very_late': clean_cancellations['is_extreme_negative'].sum(),\n",
    "    'very_early': clean_cancellations['is_extreme_positive'].sum()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move this one too\n",
    "def analyze_temporal_patterns(cancellations_df, shifts_df):\n",
    "    \"\"\"Analyze patterns related to time in cancellations\"\"\"\n",
    "    \n",
    "    # Convert timestamps and calculate time-based features\n",
    "    cancellations_df['Created At'] = pd.to_datetime(cancellations_df['Created At'])\n",
    "    cancellations_df['hour'] = cancellations_df['Created At'].dt.hour\n",
    "    cancellations_df['day_of_week'] = cancellations_df['Created At'].dt.day_name()\n",
    "    cancellations_df['Lead Time'] = pd.to_numeric(cancellations_df['Lead Time'])\n",
    "    \n",
    "    # Analyze cancellations by hour\n",
    "    hourly_cancellations = cancellations_df.groupby('hour').size()\n",
    "    hourly_rate = hourly_cancellations / len(cancellations_df) * 100\n",
    "    \n",
    "    # Analyze cancellations by day of week\n",
    "    daily_cancellations = cancellations_df.groupby('day_of_week').size()\n",
    "    \n",
    "    # Analyze lead time distribution\n",
    "    lead_time_stats = {\n",
    "        'mean_lead_time': cancellations_df['Lead Time'].mean(),\n",
    "        'median_lead_time': cancellations_df['Lead Time'].median(),\n",
    "        'late_cancels': (cancellations_df['Lead Time'] < 4).sum(),\n",
    "        'late_cancel_rate': (cancellations_df['Lead Time'] < 4).mean() * 100\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'hourly_patterns': hourly_rate,\n",
    "        'daily_patterns': daily_cancellations,\n",
    "        'lead_time_stats': lead_time_stats\n",
    "    }\n",
    "\n",
    "def analyze_worker_patterns(cancellations_df):\n",
    "    \"\"\"Analyze patterns in worker behavior\"\"\"\n",
    "    \n",
    "    # Calculate cancellations per worker\n",
    "    worker_cancellations = cancellations_df.groupby('Worker ID').size()\n",
    "    \n",
    "    # Analyze repeat cancellation behavior\n",
    "    cancellation_frequency = {\n",
    "        'single_cancel': (worker_cancellations == 1).sum(),\n",
    "        'repeat_cancels': (worker_cancellations > 1).sum(),\n",
    "        'high_frequency': (worker_cancellations > 5).sum(),\n",
    "        'max_cancels': worker_cancellations.max()\n",
    "    }\n",
    "    \n",
    "    # Analyze NCNS patterns\n",
    "    ncns_workers = cancellations_df[cancellations_df['Action'] == 'NO_CALL_NO_SHOW']['Worker ID'].nunique()\n",
    "    \n",
    "    return {\n",
    "        'cancellation_frequency': cancellation_frequency,\n",
    "        'ncns_unique_workers': ncns_workers\n",
    "    }\n",
    "\n",
    "def analyze_shift_characteristics(shifts_df, cancellations_df):\n",
    "    \"\"\"Analyze patterns in shift characteristics that lead to cancellations\"\"\"\n",
    "    \n",
    "    # Identify cancelled shifts\n",
    "    cancelled_shift_ids = cancellations_df['Shift ID'].unique()\n",
    "    cancelled_shifts = shifts_df[shifts_df['ID'].isin(cancelled_shift_ids)]\n",
    "    non_cancelled_shifts = shifts_df[~shifts_df['ID'].isin(cancelled_shift_ids)]\n",
    "    \n",
    "    # Analyze pay rates\n",
    "    pay_comparison = {\n",
    "        'cancelled_avg_pay': cancelled_shifts['Charge'].mean(),\n",
    "        'non_cancelled_avg_pay': non_cancelled_shifts['Charge'].mean(),\n",
    "        'pay_difference': cancelled_shifts['Charge'].mean() - non_cancelled_shifts['Charge'].mean()\n",
    "    }\n",
    "    \n",
    "    # Analyze shift duration\n",
    "    duration_comparison = {\n",
    "        'cancelled_avg_duration': cancelled_shifts['Time'].mean(),\n",
    "        'non_cancelled_avg_duration': non_cancelled_shifts['Time'].mean()\n",
    "    }\n",
    "    \n",
    "    # Analyze shift types\n",
    "    shift_type_rates = pd.DataFrame({\n",
    "        'cancelled': cancelled_shifts['Shift Type'].value_counts(normalize=True),\n",
    "        'non_cancelled': non_cancelled_shifts['Shift Type'].value_counts(normalize=True)\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        'pay_patterns': pay_comparison,\n",
    "        'duration_patterns': duration_comparison,\n",
    "        'shift_type_patterns': shift_type_rates\n",
    "    }\n",
    "\n",
    "def analyze_facility_impact(shifts_df, cancellations_df):\n",
    "    \"\"\"Analyze patterns in facility impact\"\"\"\n",
    "    \n",
    "    # Calculate cancellation rates by facility\n",
    "    facility_cancellations = cancellations_df.groupby('Facility ID').size()\n",
    "    facility_shifts = shifts_df.groupby('Facility ID').size()\n",
    "    \n",
    "    facility_cancel_rates = (facility_cancellations / facility_shifts * 100).fillna(0)\n",
    "    \n",
    "    # Analyze facilities with high cancellation rates\n",
    "    high_impact_facilities = facility_cancel_rates[facility_cancel_rates > facility_cancel_rates.median()]\n",
    "    \n",
    "    return {\n",
    "        'facility_cancel_rates': facility_cancel_rates.describe(),\n",
    "        'high_impact_count': len(high_impact_facilities)\n",
    "    }\n",
    "\n",
    "# Run all analyses\n",
    "temporal_patterns = analyze_temporal_patterns(cancellations_df, shifts_df)\n",
    "worker_patterns = analyze_worker_patterns(cancellations_df)\n",
    "shift_patterns = analyze_shift_characteristics(shifts_df, cancellations_df)\n",
    "facility_impact = analyze_facility_impact(shifts_df, cancellations_df)\n",
    "\n",
    "# Visualization of key patterns\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Plot 1: Hourly cancellation distribution\n",
    "plt.subplot(2, 2, 1)\n",
    "temporal_patterns['hourly_patterns'].plot(kind='bar')\n",
    "plt.title('Cancellations by Hour of Day')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Percentage of Cancellations')\n",
    "\n",
    "# Plot 2: Lead time distribution\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(cancellations_df['Lead Time'], bins=50)\n",
    "plt.title('Distribution of Cancellation Lead Times')\n",
    "plt.xlabel('Lead Time (hours)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Plot 3: Worker cancellation frequency\n",
    "plt.subplot(2, 2, 3)\n",
    "worker_cancel_counts = cancellations_df.groupby('Worker ID').size()\n",
    "plt.hist(worker_cancel_counts, bins=30)\n",
    "plt.title('Distribution of Cancellations per Worker')\n",
    "plt.xlabel('Number of Cancellations')\n",
    "plt.ylabel('Number of Workers')\n",
    "\n",
    "# Plot 4: Pay rate comparison\n",
    "plt.subplot(2, 2, 4)\n",
    "cancelled_shifts = shifts_df[shifts_df['ID'].isin(cancellations_df['Shift ID'])]\n",
    "non_cancelled_shifts = shifts_df[~shifts_df['ID'].isin(cancellations_df['Shift ID'])]\n",
    "plt.boxplot([cancelled_shifts['Charge'], non_cancelled_shifts['Charge']], \n",
    "            labels=['Cancelled Shifts', 'Completed Shifts'])\n",
    "plt.title('Pay Rate Distribution: Cancelled vs Completed Shifts')\n",
    "plt.ylabel('Charge Rate')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_cancel_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move this to later \n",
    "def analyze_cancellation_patterns(shifts_df, cancellations_df, bookings_df):\n",
    "    # Convert timestamps to datetime\n",
    "    for df in [shifts_df, cancellations_df, bookings_df]:\n",
    "        df['Created At'] = pd.to_datetime(df['Created At'])\n",
    "    \n",
    "    shifts_df['Start'] = pd.to_datetime(shifts_df['Start'])\n",
    "    shifts_df['End'] = pd.to_datetime(shifts_df['End'])\n",
    "    \n",
    "    # Calculate key metrics\n",
    "    total_shifts = len(shifts_df)\n",
    "    cancelled_shifts = len(cancellations_df)\n",
    "    cancellation_rate = cancelled_shifts / total_shifts * 100\n",
    "    \n",
    "    # Analyze cancellation lead times\n",
    "    cancellations_df['Lead Time'] = pd.to_numeric(cancellations_df['Lead Time'])\n",
    "    late_cancellations = cancellations_df[cancellations_df['Lead Time'] < 4]\n",
    "    ncns = cancellations_df[cancellations_df['Action'] == 'NO_CALL_NO_SHOW']\n",
    "    \n",
    "    # Calculate repeat cancellation behavior\n",
    "    worker_cancellation_counts = cancellations_df.groupby('Worker ID').size()\n",
    "    repeat_cancellers = (worker_cancellation_counts > 1).sum()\n",
    "    \n",
    "    # Analyze cancellation patterns by time\n",
    "    cancellations_df['Hour'] = cancellations_df['Created At'].dt.hour\n",
    "    cancellations_by_hour = cancellations_df.groupby('Hour').size()\n",
    "    \n",
    "    # Analyze shift characteristics that lead to cancellations\n",
    "    cancelled_shift_ids = cancellations_df['Shift ID'].unique()\n",
    "    cancelled_shifts_data = shifts_df[shifts_df['ID'].isin(cancelled_shift_ids)]\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_charge_cancelled = cancelled_shifts_data['Charge'].mean()\n",
    "    avg_charge_all = shifts_df['Charge'].mean()\n",
    "    \n",
    "    # Analyze time between booking and cancellation\n",
    "    merged_data = pd.merge(\n",
    "        cancellations_df,\n",
    "        bookings_df,\n",
    "        on=['Shift ID', 'Worker ID'],\n",
    "        suffixes=('_cancel', '_book')\n",
    "    )\n",
    "    merged_data['booking_to_cancel_hours'] = (\n",
    "        merged_data['Created At_cancel'] - merged_data['Created At_book']\n",
    "    ).dt.total_seconds() / 3600 #number of seconds in an hour\n",
    "    \n",
    "    # Generate summary statistics\n",
    "    summary_stats = {\n",
    "        'total_shifts': total_shifts,\n",
    "        'cancellation_rate': cancellation_rate,\n",
    "        'late_cancellation_rate': len(late_cancellations) / cancelled_shifts * 100,\n",
    "        'ncns_rate': len(ncns) / cancelled_shifts * 100,\n",
    "        'repeat_canceller_rate': repeat_cancellers / len(worker_cancellation_counts) * 100,\n",
    "        'avg_lead_time': cancellations_df['Lead Time'].mean(),\n",
    "        'median_lead_time': cancellations_df['Lead Time'].median(),\n",
    "        'avg_charge_diff': avg_charge_cancelled - avg_charge_all,\n",
    "        'avg_booking_to_cancel': merged_data['booking_to_cancel_hours'].mean()\n",
    "    }\n",
    "    \n",
    "    return summary_stats\n",
    "\n",
    "# Function to visualize patterns\n",
    "def plot_cancellation_patterns(cancellations_df):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Cancellations by hour\n",
    "    plt.subplot(2, 2, 1)\n",
    "    cancellations_df['Hour'] = cancellations_df['Created At'].dt.hour\n",
    "    sns.histplot(data=cancellations_df, x='Hour', bins=24)\n",
    "    plt.title('Cancellations by Hour of Day')\n",
    "    \n",
    "    # Plot 2: Lead time distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.histplot(data=cancellations_df, x='Lead Time', bins=50)\n",
    "    plt.title('Distribution of Cancellation Lead Times')\n",
    "    \n",
    "    # Plot 3: Cancellations by day of week\n",
    "    plt.subplot(2, 2, 3)\n",
    "    cancellations_df['Day'] = cancellations_df['Created At'].dt.day_name()\n",
    "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    sns.countplot(data=cancellations_df, x='Day', order=day_order)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Cancellations by Day of Week')\n",
    "    \n",
    "    # Plot 4: Cumulative distribution of lead times\n",
    "    plt.subplot(2, 2, 4)\n",
    "    lead_times = pd.to_numeric(cancellations_df['Lead Time'])\n",
    "    lead_times.sort_values().plot(kind='line')\n",
    "    plt.title('Cumulative Distribution of Lead Times')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "# Run analysis\n",
    "# note: what is this?\n",
    "summary_stats = analyze_cancellation_patterns(shifts_df, cancellations_df, bookings_df)\n",
    "plots = plot_cancellation_patterns(cancellations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Comprehensive Cancellation Analysis ===\n",
    "print(\"=== Cancellation Type Analysis ===\")\n",
    "# Basic cancellation types\n",
    "cancellation_types = cancellations_df['Action'].value_counts()\n",
    "print(\"\\nCancellation Action Types:\")\n",
    "print(cancellation_types)\n",
    "\n",
    "# Detailed Pattern Analysis\n",
    "def analyze_cancellation_patterns(clean_cancellations, shifts_df):\n",
    "    \"\"\"\n",
    "    Analyze patterns in cancellations, including:\n",
    "    - Types of cancellations (NCNS vs Regular)\n",
    "    - Role-based patterns\n",
    "    - Shift type patterns\n",
    "    \"\"\"\n",
    "    # Merge with shifts to analyze by role\n",
    "    cancellations_with_shifts = pd.merge(\n",
    "        clean_cancellations,\n",
    "        shifts_df[['ID', 'Agent Req', 'Shift Type', 'Charge']],\n",
    "        left_on='Shift ID',\n",
    "        right_on='ID',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Cancellations by Role ===\")\n",
    "    role_cancels = pd.crosstab(\n",
    "        cancellations_with_shifts['Agent Req'],\n",
    "        cancellations_with_shifts['cancellation_category'],\n",
    "        normalize='index'\n",
    "    ).round(3) * 100\n",
    "    print(role_cancels)\n",
    "    \n",
    "    print(\"\\n=== Cancellations by Shift Type ===\")\n",
    "    shift_cancels = pd.crosstab(\n",
    "        cancellations_with_shifts['Shift Type'],\n",
    "        cancellations_with_shifts['cancellation_category'],\n",
    "        normalize='index'\n",
    "    ).round(3) * 100\n",
    "    print(shift_cancels)\n",
    "    \n",
    "    # Role-based impact analysis\n",
    "    role_impact = cancellations_with_shifts.groupby('Agent Req').agg({\n",
    "        'Shift ID': 'count',\n",
    "        'Lead Time': ['mean', 'std'],\n",
    "        'Action': lambda x: (x == 'NO_CALL_NO_SHOW').mean() * 100\n",
    "    }).round(2)\n",
    "    role_impact.columns = ['Total Cancellations', 'Avg Lead Time', 'Lead Time Std', 'NCNS Rate']\n",
    "    print(\"\\n=== Role-Based Impact ===\")\n",
    "    print(role_impact)\n",
    "    \n",
    "    # Store results\n",
    "    summary.add_summary('cancellations', 'action_types', cancellation_types.to_dict())\n",
    "    summary.add_summary('cancellations', 'role_patterns', role_cancels.to_dict())\n",
    "    summary.add_summary('cancellations', 'shift_patterns', shift_cancels.to_dict())\n",
    "    summary.add_summary('cancellations', 'role_impact', role_impact.to_dict())\n",
    "    \n",
    "    return cancellations_with_shifts\n",
    "\n",
    "# Run the analysis\n",
    "cancellations_with_shifts = analyze_cancellation_patterns(clean_cancellations, shifts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shifts Data Dive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Shift Data Numerical Analysis ===\n",
    "print(\"\\n=== Numerical Analysis: Basic statistics for numerical columns ===\")\n",
    "# Basic statistics for numerical columns\n",
    "numeric_stats = shifts_df[['Charge', 'Time']].describe()\n",
    "print(\"\\nNumerical Statistics:\")\n",
    "print(numeric_stats)\n",
    "\n",
    "# Additional numeric insights\n",
    "print(\"\\nCharge Rate Analysis:\")\n",
    "print(f\"Shifts with zero charge: {(shifts_df['Charge'] == 0).sum()}\")\n",
    "print(f\"Average charge by agent type:\")\n",
    "print(shifts_df.groupby('Agent Req')['Charge'].mean().round(2))\n",
    "\n",
    "# === Categorical Analysis ===\n",
    "print(\"\\n=== Categorical Analysis ===\")\n",
    "# Shift type distribution\n",
    "print(\"\\nShift Type Distribution:\")\n",
    "shift_type_dist = shifts_df['Shift Type'].value_counts(dropna=True)\n",
    "print(shift_type_dist)\n",
    "\n",
    "# Agent requirements\n",
    "print(\"\\nAgent Requirement Distribution:\")\n",
    "agent_req_dist = shifts_df['Agent Req'].value_counts(dropna=True)\n",
    "print(agent_req_dist)\n",
    "\n",
    "# Cross-tabulation of shift types and agent requirements\n",
    "print(\"\\nShift Types by Agent Requirements:\")\n",
    "print(pd.crosstab(shifts_df['Shift Type'], shifts_df['Agent Req']))\n",
    "\n",
    "# === Data Completeness Analysis ===\n",
    "print(\"\\n=== Data Completeness Analysis ===\")\n",
    "complete_rows = shifts_df.dropna().shape[0]\n",
    "print(f\"Complete rows: {complete_rows} out of {shifts_df.shape[0]}\")\n",
    "print(f\"Completion rate: {(complete_rows/shifts_df.shape[0]*100):.2f}%\")\n",
    "\n",
    "# === Time-Based Analysis ===\n",
    "print(\"\\n=== Time-Based Analysis ===\")\n",
    "# Extract time components\n",
    "shifts_df['Hour'] = shifts_df['Start'].dt.hour\n",
    "shifts_df['Day'] = shifts_df['Start'].dt.day_name()\n",
    "shifts_df['Month'] = shifts_df['Start'].dt.month\n",
    "shifts_df['Shift_Length'] = (shifts_df['End'] - shifts_df['Start']).dt.total_seconds() / 3600\n",
    "# Time patterns\n",
    "print(\"\\nShifts by Hour:\")\n",
    "hour_dist = shifts_df['Hour'].value_counts().sort_index()\n",
    "print(hour_dist)\n",
    "\n",
    "print(\"\\nShifts by Day of Week:\")\n",
    "day_dist = shifts_df['Day'].value_counts()\n",
    "print(day_dist)\n",
    "\n",
    "print(\"\\nShift Length Distribution:\")\n",
    "print(shifts_df['Shift_Length'].describe().round(2))\n",
    "\n",
    "# === Facility Analysis ===\n",
    "print(\"\\n=== Facility Analysis ===\")\n",
    "facility_stats = shifts_df.groupby('Facility ID').agg({\n",
    "    'ID': 'count',\n",
    "    'Charge': 'mean',\n",
    "    'Time': 'mean'\n",
    "}).rename(columns={\n",
    "    'ID': 'Number of Shifts',\n",
    "    'Charge': 'Average Charge',\n",
    "    'Time': 'Average Shift Length'\n",
    "})\n",
    "print(\"\\nFacility Statistics:\")\n",
    "print(facility_stats.head())\n",
    "print(f\"\\nTotal unique facilities: {shifts_df['Facility ID'].nunique()}\")\n",
    "\n",
    "# Store all results\n",
    "summary.add_summary('shifts', 'numeric_stats', numeric_stats)\n",
    "summary.add_summary('shifts', 'shift_types', shift_type_dist.to_dict())\n",
    "summary.add_summary('shifts', 'agent_types', agent_req_dist.to_dict())\n",
    "summary.add_summary('shifts', 'hour_distribution', hour_dist.to_dict())\n",
    "summary.add_summary('shifts', 'day_distribution', day_dist.to_dict())\n",
    "summary.add_summary('shifts', 'facility_stats', facility_stats.to_dict())\n",
    "\n",
    "# Optional: Create visualizations\n",
    "# We can add matplotlib/seaborn plots here if you'd like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship analysis\n",
    "# First, let's see how many shifts had cancellations\n",
    "shifts_with_cancellations = len(set(shifts_df['ID']) & set(cancellations_df['Shift ID']))\n",
    "print(f\"Shifts with cancellations: {shifts_with_cancellations}\")\n",
    "print(f\"Percentage of shifts cancelled: {(shifts_with_cancellations/len(shifts_df))*100:.2f}%\")\n",
    "\n",
    "# Analyze cancellation lead times\n",
    "cancellations_df['Lead Time'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Booking Data Dive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Booking Pattern Analysis ===\n",
    "def analyze_booking_patterns(bookings_df, shifts_df, clean_cancellations):\n",
    "    \"\"\"\n",
    "    Analyze patterns in shift bookings, including:\n",
    "    - Time from posting to booking\n",
    "    - Successful vs cancelled bookings\n",
    "    - Rebooking patterns after cancellations\n",
    "    \n",
    "    Parameters:\n",
    "        bookings_df (pd.DataFrame): Booking logs data\n",
    "        shifts_df (pd.DataFrame): Shifts data\n",
    "        clean_cancellations (pd.DataFrame): Cleaned cancellations data\n",
    "    \"\"\"\n",
    "    print(\"=== Booking Success Analysis ===\")\n",
    "\n",
    "    # Calculate time to fill (from shift creation to booking)\n",
    "    bookings_with_shifts = pd.merge(\n",
    "        bookings_df,\n",
    "        shifts_df[['ID', 'Created At', 'Agent Req', 'Shift Type', 'Charge']],\n",
    "        left_on='Shift ID',\n",
    "        right_on='ID',\n",
    "        how='left',\n",
    "        suffixes=('_booking', '_shift')\n",
    "    )\n",
    "    \n",
    "    bookings_with_shifts['time_to_fill'] = (\n",
    "        pd.to_datetime(bookings_with_shifts['Created At_booking']) - \n",
    "        pd.to_datetime(bookings_with_shifts['Created At_shift'])\n",
    "    ).dt.total_seconds() / 3600  # Convert to hours\n",
    "    \n",
    "    print(\"\\nTime to Fill Shift from listing Statistics (hours):\")\n",
    "    print(bookings_with_shifts['time_to_fill'].describe().round(2))\n",
    "    \n",
    "    # Analyze bookings by role and shift type\n",
    "    print(\"\\n=== Bookings by Role ===\")\n",
    "    role_bookings = bookings_with_shifts.groupby('Agent Req').agg({\n",
    "        'Shift ID': 'count',  # Changed from 'ID' to 'Shift ID'\n",
    "        'time_to_fill': 'mean',\n",
    "        'Charge': 'mean'\n",
    "    }).round(2)\n",
    "    role_bookings.columns = ['Number of Bookings', 'Avg Time to Fill', 'Avg Charge']\n",
    "    print(role_bookings)\n",
    "    \n",
    "    # Look at shifts that got cancelled and rebooked\n",
    "    rebooked_cancellations = clean_cancellations['Shift ID'].value_counts()\n",
    "    \n",
    "    print(\"\\n=== Rebooking Analysis ===\")\n",
    "    print(f\"Shifts cancelled multiple times: {(rebooked_cancellations > 1).sum()}\")\n",
    "    print(f\"Maximum cancellations for a single shift: {rebooked_cancellations.max()}\")\n",
    "    \n",
    "    # Additional timing analysis\n",
    "    print(\"\\n=== Booking Time Patterns ===\")\n",
    "    bookings_with_shifts['booking_hour'] = pd.to_datetime(bookings_with_shifts['Created At_booking']).dt.hour\n",
    "    bookings_with_shifts['booking_day'] = pd.to_datetime(bookings_with_shifts['Created At_booking']).dt.day_name()\n",
    "    \n",
    "    print(\"\\nBookings by Hour of Day:\")\n",
    "    print(bookings_with_shifts['booking_hour'].value_counts().sort_index())\n",
    "    \n",
    "    print(\"\\nBookings by Day of Week:\")\n",
    "    print(bookings_with_shifts['booking_day'].value_counts())\n",
    "    \n",
    "    # Store results\n",
    "    summary.add_summary('bookings', 'time_to_fill', \n",
    "                       bookings_with_shifts['time_to_fill'].describe().to_dict())\n",
    "    summary.add_summary('bookings', 'role_patterns', role_bookings.to_dict())\n",
    "    summary.add_summary('bookings', 'rebooking_stats', {\n",
    "        'multiple_cancellations': (rebooked_cancellations > 1).sum(),\n",
    "        'max_cancellations': rebooked_cancellations.max()\n",
    "    })\n",
    "    \n",
    "    return bookings_with_shifts\n",
    "\n",
    "# Run the analysis\n",
    "bookings_with_shifts = analyze_booking_patterns(bookings_df, shifts_df, clean_cancellations)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Economic Impact Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Economic Impact Analysis ===\n",
    "def analyze_economic_impact(shifts_df, cancellations_with_shifts):\n",
    "    \"\"\"\n",
    "    Analyze economic impact of cancellations, including:\n",
    "    - Revenue loss from cancellations\n",
    "    - Impact by facility and role type\n",
    "    - Patterns in high-cost cancellations\n",
    "    \"\"\"\n",
    "    print(\"=== Economic Impact Analysis ===\")\n",
    "    \n",
    "    # First, ensure we have all needed columns by merging with shifts data if needed\n",
    "    if 'Time' not in cancellations_with_shifts.columns:\n",
    "        cancellations_with_shifts = pd.merge(\n",
    "            cancellations_with_shifts,\n",
    "            shifts_df[['ID', 'Time', 'Charge']],\n",
    "            left_on='Shift ID',\n",
    "            right_on='ID',\n",
    "            how='left',\n",
    "            suffixes=('', '_shift')\n",
    "        )\n",
    "\n",
    "    # Calculate baseline metrics\n",
    "    total_revenue = (shifts_df['Charge'] * shifts_df['Time']).sum()\n",
    "    avg_hourly_revenue = shifts_df['Charge'].mean()\n",
    "    \n",
    "    # Analyze cancelled shifts\n",
    "    cancelled_revenue = (cancellations_with_shifts['Charge'] * \n",
    "                        cancellations_with_shifts['Time']).sum()\n",
    "    \n",
    "    print(\"\\nBaseline Metrics:\")\n",
    "    print(f\"Total Potential Revenue: ${total_revenue:,.2f}\")\n",
    "    print(f\"Average Hourly Rate: ${avg_hourly_revenue:.2f}\")\n",
    "    print(f\"Lost Revenue from Cancellations: ${cancelled_revenue:,.2f}\")\n",
    "    if total_revenue > 0:  # Avoid division by zero\n",
    "        print(f\"Percentage of Revenue Lost: {(cancelled_revenue/total_revenue)*100:.2f}%\")\n",
    "\n",
    "    # Analysis by role type\n",
    "    print(\"\\n=== Impact by Role Type ===\")\n",
    "    role_impact = cancellations_with_shifts.groupby('Agent Req').agg({\n",
    "        'Shift ID': 'count',\n",
    "        'Charge': ['mean', 'sum'],\n",
    "        'Time': 'sum'\n",
    "    }).round(2)\n",
    "    role_impact.columns = ['Cancellations', 'Avg Rate', 'Total Charge', 'Total Hours']\n",
    "    role_impact['Est. Revenue Loss'] = role_impact['Avg Rate'] * role_impact['Total Hours']\n",
    "    print(role_impact.sort_values('Est. Revenue Loss', ascending=False))\n",
    "\n",
    "    # Analysis by cancellation type\n",
    "    print(\"\\n=== Impact by Cancellation Type ===\")\n",
    "    type_impact = cancellations_with_shifts.groupby('cancellation_category').agg({\n",
    "        'Shift ID': 'count',\n",
    "        'Charge': ['mean', 'sum'],\n",
    "        'Time': 'sum'\n",
    "    }).round(2)\n",
    "    type_impact.columns = ['Cancellations', 'Avg Rate', 'Total Charge', 'Total Hours']\n",
    "    type_impact['Est. Revenue Loss'] = type_impact['Avg Rate'] * type_impact['Total Hours']\n",
    "    print(type_impact.sort_values('Est. Revenue Loss', ascending=False))\n",
    "\n",
    "    # Calculate impact by facility\n",
    "    print(\"\\n=== Top 5 Facilities by Revenue Loss ===\")\n",
    "    facility_impact = cancellations_with_shifts.groupby('Facility ID').agg({\n",
    "        'Shift ID': 'count',\n",
    "        'Charge': ['mean', 'sum'],\n",
    "        'Time': 'sum'\n",
    "    }).round(2)\n",
    "    facility_impact.columns = ['Cancellations', 'Avg Rate', 'Total Charge', 'Total Hours']\n",
    "    facility_impact['Est. Revenue Loss'] = facility_impact['Avg Rate'] * facility_impact['Total Hours']\n",
    "    print(facility_impact.nlargest(5, 'Est. Revenue Loss'))\n",
    "\n",
    "    # Store results\n",
    "    summary.add_summary('economic', 'overall_impact', {\n",
    "        'total_revenue': total_revenue,\n",
    "        'cancelled_revenue': cancelled_revenue,\n",
    "        'avg_hourly_rate': avg_hourly_revenue\n",
    "    })\n",
    "    summary.add_summary('economic', 'role_impact', role_impact.to_dict())\n",
    "    summary.add_summary('economic', 'type_impact', type_impact.to_dict())\n",
    "\n",
    "    return role_impact, type_impact, facility_impact\n",
    "\n",
    "# Run the analysis\n",
    "role_impact, type_impact, facility_impact = analyze_economic_impact(shifts_df, cancellations_with_shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audit_data_quality(shifts_df, cancellations_df, bookings_df):\n",
    "    \"\"\"\n",
    "    Comprehensive data quality audit focusing on business-critical issues\n",
    "    \n",
    "    Parameters:\n",
    "    - shifts_df: DataFrame containing shift data\n",
    "    - cancellations_df: DataFrame containing cancellation logs\n",
    "    - bookings_df: DataFrame containing booking logs\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing quality issues by dataset\n",
    "    \"\"\"\n",
    "    quality_issues = {\n",
    "        'shifts': {},\n",
    "        'cancellations': {},\n",
    "        'bookings': {}\n",
    "    }\n",
    "    \n",
    "    # Shifts Analysis\n",
    "    shifts_issues = {\n",
    "        # Financial data issues\n",
    "        'zero_charge': (shifts_df['Charge'] == 0).sum(),\n",
    "        'negative_charge': (shifts_df['Charge'] < 0).sum(),\n",
    "        \n",
    "        # Time-related issues\n",
    "        'zero_time': (shifts_df['Time'] == 0).sum(),\n",
    "        'negative_time': (shifts_df['Time'] < 0).sum(),\n",
    "        'end_before_start': (shifts_df['End'] < shifts_df['Start']).sum(),\n",
    "        \n",
    "        # Missing data\n",
    "        'missing_agent': shifts_df['Agent ID'].isnull().sum(),\n",
    "        'missing_facility': shifts_df['Facility ID'].isnull().sum(),\n",
    "        'missing_shift_type': shifts_df['Shift Type'].isnull().sum(),\n",
    "        \n",
    "        # Verification issues\n",
    "        'unverified_completed': ((shifts_df['End'] < pd.Timestamp.now()) & \n",
    "                                (shifts_df['Verified'].isnull())).sum(),\n",
    "        \n",
    "        # Invalid shift types\n",
    "        'invalid_shift_types': shifts_df[~shifts_df['Shift Type'].isin(['am', 'pm', 'noc', 'custom'])].shape[0]\n",
    "    }\n",
    "    \n",
    "    # Cancellations Analysis\n",
    "    cancel_issues = {\n",
    "        # Lead time issues\n",
    "        'invalid_lead_time': (cancellations_df['Lead Time'].isnull() | \n",
    "                            ~np.isfinite(cancellations_df['Lead Time'])).sum(),\n",
    "        'extreme_negative_lead': (cancellations_df['Lead Time'] < -72).sum(),  # More than 3 days after start\n",
    "        'extreme_positive_lead': (cancellations_df['Lead Time'] > 720).sum(),  # More than 30 days before\n",
    "        \n",
    "        # Missing data\n",
    "        'missing_worker': cancellations_df['Worker ID'].isnull().sum(),\n",
    "        'missing_facility': cancellations_df['Facility ID'].isnull().sum(),\n",
    "        \n",
    "        # Duplicate issues\n",
    "        'duplicate_cancels': cancellations_df.groupby('Shift ID').size().gt(1).sum(),\n",
    "        \n",
    "        # Action type validation\n",
    "        'invalid_actions': cancellations_df[~cancellations_df['Action'].isin(\n",
    "            ['WORKER_CANCEL', 'NO_CALL_NO_SHOW'])].shape[0]\n",
    "    }\n",
    "    \n",
    "    # Bookings Analysis\n",
    "    bookings_issues = {\n",
    "        # Missing data\n",
    "        'missing_worker': bookings_df['Worker ID'].isnull().sum(),\n",
    "        'missing_facility': bookings_df['Facility ID'].isnull().sum(),\n",
    "        \n",
    "        # Lead time issues (time between booking and shift start)\n",
    "        'invalid_lead_time': (bookings_df['Lead Time'].isnull() | \n",
    "                            ~np.isfinite(bookings_df['Lead Time'])).sum(),\n",
    "        \n",
    "        # Action validation\n",
    "        'invalid_actions': bookings_df[bookings_df['Action'] != 'SHIFT_CLAIM'].shape[0]\n",
    "    }\n",
    "    \n",
    "    # Cross-dataset validation\n",
    "    cross_validation = {\n",
    "        'orphaned_cancels': cancellations_df[~cancellations_df['Shift ID'].isin(shifts_df['ID'])].shape[0],\n",
    "        'orphaned_bookings': bookings_df[~bookings_df['Shift ID'].isin(shifts_df['ID'])].shape[0],\n",
    "        'multiple_workers': shifts_df.groupby('ID')['Agent ID'].nunique().gt(1).sum(),\n",
    "        'booking_cancel_mismatch': len(\n",
    "            set(cancellations_df[cancellations_df['Action'] == 'WORKER_CANCEL']['Shift ID']) - \n",
    "            set(bookings_df['Shift ID'])\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    quality_issues['shifts'] = shifts_issues\n",
    "    quality_issues['cancellations'] = cancel_issues\n",
    "    quality_issues['bookings'] = bookings_issues\n",
    "    quality_issues['cross_validation'] = cross_validation\n",
    "    \n",
    "    return quality_issues\n",
    "\n",
    "# Function to display audit results in a readable format\n",
    "def display_audit_results(audit_results):\n",
    "    \"\"\"\n",
    "    Display audit results in a clear, organized format\n",
    "    \"\"\"\n",
    "    for dataset, issues in audit_results.items():\n",
    "        print(f\"\\n=== {dataset.upper()} QUALITY ISSUES ===\")\n",
    "        for issue, count in issues.items():\n",
    "            print(f\"{issue}: {count:,}\")\n",
    "\n",
    "\n",
    "# === In Initial Data Loading and Validation Section ===\n",
    "\n",
    "print(\"Performing data quality audit...\")\n",
    "# Run the audit\n",
    "audit_results = audit_data_quality(shifts_df, cancellations_df, bookings_df)\n",
    "\n",
    "# Display results\n",
    "display_audit_results(audit_results)\n",
    "\n",
    "# Store results in summary\n",
    "summary.add_summary('data_quality', 'audit_results', audit_results)\n",
    "\n",
    "# Optional: Display specific issues that need attention\n",
    "significant_issues = {\n",
    "    dataset: {issue: count for issue, count in issues.items() if count > 0}\n",
    "    for dataset, issues in audit_results.items()\n",
    "}\n",
    "\n",
    "print(\"\\nSignificant issues requiring attention:\")\n",
    "for dataset, issues in significant_issues.items():\n",
    "    if issues:  # Only show datasets with issues\n",
    "        print(f\"\\n{dataset}:\")\n",
    "        for issue, count in issues.items():\n",
    "            print(f\"- {issue}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This suggests that:\n",
    "\n",
    "Many cancellations and bookings don't link to shifts in our dataset\n",
    "Could be due to date range mismatches or data completeness issues\n",
    "Critical for understanding true cancellation rates\n",
    "\n",
    "\n",
    "Worker/Agent Data Gaps\n",
    "\n",
    "Copymissing_agent: 20,035 (shifts)\n",
    "missing_worker: 191 (cancellations)\n",
    "missing_worker: 140 (bookings)\n",
    "This matches what we saw earlier but gives us a more complete picture. Particularly important because:\n",
    "\n",
    "About half of shifts are missing agent IDs\n",
    "Affects our ability to analyze worker patterns\n",
    "Could impact our ability to track repeat cancellations\n",
    "\n",
    "\n",
    "Lead Time Issues\n",
    "\n",
    "Copyextreme_negative_lead: 4,960\n",
    "extreme_positive_lead: 741\n",
    "This provides more granular insight than our earlier analysis. Important because:\n",
    "\n",
    "Shows significant number of very late cancellations (>3 days after start)\n",
    "Identifies early cancellations that might need different handling\n",
    "Relevant to the attendance policy analysis\n",
    "\n",
    "\n",
    "Financial Data Quality\n",
    "\n",
    "Copyzero_charge: 5,019\n",
    "zero_time: 79\n",
    "negative_time: 22\n",
    "Matches our earlier findings but gives more context about potential revenue impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point realized the data doesn't overlap in dates \n",
    "# went back to the start to fix that \n",
    "def analyze_data_coverage():\n",
    "    \"\"\"\n",
    "    Analyze the time coverage and relationships between datasets\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing date ranges and overlap analysis for each dataset\n",
    "    \"\"\"\n",
    "    # Get date ranges for each dataset\n",
    "    print(\"Analyzing dataset date coverage...\")\n",
    "    \n",
    "    shift_dates = shifts_df['Start'].dt.date.value_counts().sort_index()\n",
    "    cancel_dates = cancellations_df['Created At'].dt.date.value_counts().sort_index()\n",
    "    booking_dates = bookings_df['Created At'].dt.date.value_counts().sort_index()\n",
    "    \n",
    "    # Analyze overlap periods\n",
    "    date_ranges = {\n",
    "        'shifts': {\n",
    "            'start': shift_dates.index.min(),\n",
    "            'end': shift_dates.index.max(),\n",
    "            'total_days': len(shift_dates),\n",
    "            'avg_shifts_per_day': shift_dates.mean()\n",
    "        },\n",
    "        'cancellations': {\n",
    "            'start': cancel_dates.index.min(),\n",
    "            'end': cancel_dates.index.max(),\n",
    "            'total_days': len(cancel_dates),\n",
    "            'avg_cancels_per_day': cancel_dates.mean()\n",
    "        },\n",
    "        'bookings': {\n",
    "            'start': booking_dates.index.min(),\n",
    "            'end': booking_dates.index.max(),\n",
    "            'total_days': len(booking_dates),\n",
    "            'avg_bookings_per_day': booking_dates.mean()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return date_ranges\n",
    "\n",
    "def analyze_missing_data_impact():\n",
    "    \"\"\"\n",
    "    Assess how missing data affects our key metrics\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing comparative analysis of shifts with/without missing data\n",
    "    \"\"\"\n",
    "    print(\"\\nAnalyzing impact of missing data...\")\n",
    "    \n",
    "    # Analyze shifts with/without missing agent IDs\n",
    "    missing_agent_shifts = shifts_df[shifts_df['Agent ID'].isnull()]\n",
    "    complete_shifts = shifts_df[shifts_df['Agent ID'].notnull()]\n",
    "    \n",
    "    # Get cancellation rates\n",
    "    missing_cancels = len(set(missing_agent_shifts['ID']) & set(cancellations_df['Shift ID']))\n",
    "    complete_cancels = len(set(complete_shifts['ID']) & set(cancellations_df['Shift ID']))\n",
    "    \n",
    "    comparison = {\n",
    "        'missing_agent': {\n",
    "            'count': len(missing_agent_shifts),\n",
    "            'avg_charge': missing_agent_shifts['Charge'].mean(),\n",
    "            'avg_duration': missing_agent_shifts['Time'].mean(),\n",
    "            'cancellation_count': missing_cancels,\n",
    "            'cancellation_rate': missing_cancels / len(missing_agent_shifts) if len(missing_agent_shifts) > 0 else 0,\n",
    "            'verified_rate': missing_agent_shifts['Verified'].mean()\n",
    "        },\n",
    "        'complete_data': {\n",
    "            'count': len(complete_shifts),\n",
    "            'avg_charge': complete_shifts['Charge'].mean(),\n",
    "            'avg_duration': complete_shifts['Time'].mean(),\n",
    "            'cancellation_count': complete_cancels,\n",
    "            'cancellation_rate': complete_cancels / len(complete_shifts) if len(complete_shifts) > 0 else 0,\n",
    "            'verified_rate': complete_shifts['Verified'].mean()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "# Run both analyses\n",
    "print(\"Running additional data quality analyses...\\n\")\n",
    "\n",
    "# Analyze data coverage\n",
    "coverage_results = analyze_data_coverage()\n",
    "print(\"\\n=== Dataset Coverage Analysis ===\")\n",
    "for dataset, info in coverage_results.items():\n",
    "    print(f\"\\n{dataset.upper()} Coverage:\")\n",
    "    for metric, value in info.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "# Analyze missing data impact\n",
    "impact_results = analyze_missing_data_impact()\n",
    "print(\"\\n=== Missing Data Impact Analysis ===\")\n",
    "for category, metrics in impact_results.items():\n",
    "    print(f\"\\n{category.replace('_', ' ').title()}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        if 'rate' in metric:\n",
    "            print(f\"{metric}: {value:.2%}\")\n",
    "        else:\n",
    "            print(f\"{metric}: {value:,.2f}\")\n",
    "\n",
    "# Store results in summary\n",
    "summary.add_summary('data_quality', 'coverage_analysis', coverage_results)\n",
    "summary.add_summary('data_quality', 'missing_data_impact', impact_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data_completeness(shifts_df, cancellations_df, bookings_df, verbose=True):\n",
    "    \"\"\"\n",
    "    Analyze dataset completeness and coverage periods.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    shifts_df : pandas.DataFrame\n",
    "        Shift data containing columns: 'Start', 'Agent ID', etc.\n",
    "    cancellations_df : pandas.DataFrame\n",
    "        Cancellation data containing columns: 'Created At', etc.\n",
    "    bookings_df : pandas.DataFrame\n",
    "        Booking data containing columns: 'Created At', etc.\n",
    "    verbose : bool, default=True\n",
    "        If True, prints detailed analysis results\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing coverage analysis and completeness metrics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Dataset Coverage Analysis\n",
    "    coverage = {\n",
    "        'shifts': {\n",
    "            'date_range': (shifts_df['Start'].min(), shifts_df['End'].max()),\n",
    "            'total_records': len(shifts_df),\n",
    "            'daily_average': len(shifts_df) / shifts_df['Start'].dt.date.nunique()\n",
    "        },\n",
    "        'cancellations': {\n",
    "            'date_range': (cancellations_df['Created At'].min(), \n",
    "                         cancellations_df['Created At'].max()),\n",
    "            'total_records': len(cancellations_df),\n",
    "            'daily_average': len(cancellations_df) / cancellations_df['Created At'].dt.date.nunique()\n",
    "        },\n",
    "        'bookings': {\n",
    "            'date_range': (bookings_df['Created At'].min(), \n",
    "                         bookings_df['Created At'].max()),\n",
    "            'total_records': len(bookings_df),\n",
    "            'daily_average': len(bookings_df) / bookings_df['Created At'].dt.date.nunique()\n",
    "        }\n",
    "    }\n",
    "    results['coverage'] = coverage\n",
    "    \n",
    "    # Data Completeness Analysis\n",
    "    completeness = {\n",
    "        'shifts': {\n",
    "            'missing_agent_id': {\n",
    "                'count': shifts_df['Agent ID'].isnull().sum(),\n",
    "                'percentage': (shifts_df['Agent ID'].isnull().sum() / len(shifts_df)) * 100\n",
    "            },\n",
    "            'verified_shifts': {\n",
    "                'count': shifts_df['Verified'].sum(),\n",
    "                'percentage': (shifts_df['Verified'].sum() / len(shifts_df)) * 100\n",
    "            }\n",
    "        },\n",
    "        'cancellations': {\n",
    "            'missing_worker_id': {\n",
    "                'count': cancellations_df['Worker ID'].isnull().sum(),\n",
    "                'percentage': (cancellations_df['Worker ID'].isnull().sum() / len(cancellations_df)) * 100\n",
    "            }\n",
    "        },\n",
    "        'bookings': {\n",
    "            'missing_worker_id': {\n",
    "                'count': bookings_df['Worker ID'].isnull().sum(),\n",
    "                'percentage': (bookings_df['Worker ID'].isnull().sum() / len(bookings_df)) * 100\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    results['completeness'] = completeness\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=== Dataset Coverage Analysis ===\")\n",
    "        print(\"\\nTime Periods:\")\n",
    "        for dataset, info in coverage.items():\n",
    "            print(f\"\\n{dataset.upper()}:\")\n",
    "            print(f\"Date Range: {info['date_range'][0].date()} to {info['date_range'][1].date()}\")\n",
    "            print(f\"Total Records: {info['total_records']:,}\")\n",
    "            print(f\"Daily Average: {info['daily_average']:.2f}\")\n",
    "        \n",
    "        print(\"\\n=== Data Completeness Analysis ===\")\n",
    "        for dataset, metrics in completeness.items():\n",
    "            print(f\"\\n{dataset.upper()} Completeness:\")\n",
    "            for field, values in metrics.items():\n",
    "                print(f\"{field}:\")\n",
    "                print(f\"  Count: {values['count']:,}\")\n",
    "                print(f\"  Percentage: {values['percentage']:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the analysis\n",
    "completeness_results = analyze_data_completeness(shifts_df, cancellations_df, bookings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Critical Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missing_agent_patterns(shifts_df):\n",
    "    \"\"\"\n",
    "    Analyze patterns in shifts with missing Agent IDs\n",
    "    \n",
    "    Parameters:\n",
    "    shifts_df: DataFrame containing shift data\n",
    "    \n",
    "    Returns:\n",
    "    Dictionary containing analysis results\n",
    "    \"\"\"\n",
    "    # Separate shifts with/without Agent IDs\n",
    "    missing_agent = shifts_df[shifts_df['Agent ID'].isnull()]\n",
    "    has_agent = shifts_df[shifts_df['Agent ID'].notnull()]\n",
    "    \n",
    "    analysis = {\n",
    "        'temporal_patterns': {\n",
    "            'missing_by_month': missing_agent['Start'].dt.to_period('M').value_counts().sort_index(),\n",
    "            'missing_by_dow': missing_agent['Start'].dt.day_name().value_counts(),\n",
    "            'missing_by_shift_type': missing_agent['Shift Type'].value_counts()\n",
    "        },\n",
    "        \n",
    "        'verification_status': {\n",
    "            'missing_verified': missing_agent['Verified'].value_counts(),\n",
    "            'has_agent_verified': has_agent['Verified'].value_counts()\n",
    "        },\n",
    "        \n",
    "        'facility_patterns': {\n",
    "            'facilities_missing': missing_agent['Facility ID'].value_counts(),\n",
    "            'missing_rate_by_facility': (\n",
    "                missing_agent.groupby('Facility ID').size() / \n",
    "                shifts_df.groupby('Facility ID').size()\n",
    "            ).sort_values(ascending=False)\n",
    "        },\n",
    "        \n",
    "        'charge_comparison': {\n",
    "            'missing_charges': missing_agent['Charge'].describe(),\n",
    "            'has_agent_charges': has_agent['Charge'].describe()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"=== Analysis of Shifts with Missing Agent IDs ===\\n\")\n",
    "    print(f\"Total Shifts: {len(shifts_df):,}\")\n",
    "    print(f\"Shifts Missing Agent ID: {len(missing_agent):,} ({len(missing_agent)/len(shifts_df):.1%})\")\n",
    "    print(f\"Shifts with Agent ID: {len(has_agent):,} ({len(has_agent)/len(shifts_df):.1%})\")\n",
    "    \n",
    "    print(\"\\n=== Verification Status ===\")\n",
    "    print(\"\\nShifts Missing Agent ID:\")\n",
    "    print(analysis['verification_status']['missing_verified'])\n",
    "    print(\"\\nShifts with Agent ID:\")\n",
    "    print(analysis['verification_status']['has_agent_verified'])\n",
    "    \n",
    "    print(\"\\n=== Shift Type Distribution (Missing Agent ID) ===\")\n",
    "    print(analysis['temporal_patterns']['missing_by_shift_type'])\n",
    "    \n",
    "    print(\"\\n=== Top 5 Facilities with Missing Agent IDs ===\")\n",
    "    print(\"Count:\")\n",
    "    print(analysis['facility_patterns']['facilities_missing'].head())\n",
    "    print(\"\\nRate:\")\n",
    "    print(analysis['facility_patterns']['missing_rate_by_facility'].head())\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "\"\"\"This analysis should help us:\n",
    "\n",
    "Identify patterns in missing Agent IDs\n",
    "See if certain facilities have more missing IDs\n",
    "Compare verification rates\n",
    "Understand if missing IDs are random or systematic \"\"\"\n",
    "# Run the analysis\n",
    "missing_agent_analysis = analyze_missing_agent_patterns(shifts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's verify the data structure\n",
    "overlap_start = shifts_df['Start'].dt.date.min()\n",
    "overlap_end = shifts_df['Start'].dt.date.max()\n",
    "\n",
    "# Print shapes and types for debugging\n",
    "print(\"Overlap period:\", overlap_start, \"to\", overlap_end)\n",
    "print(\"\\nDataset shapes:\")\n",
    "print(f\"Shifts: {overlap_shifts.shape}\")\n",
    "print(f\"Cancellations: {overlap_cancels.shape}\")\n",
    "print(f\"Bookings: {overlap_bookings.shape}\")\n",
    "\n",
    "# Let's check a simpler version of the worker calculation first\n",
    "def analyze_worker_basic(shifts, cancels):\n",
    "    \"\"\"Simplified version to debug the core calculation\"\"\"\n",
    "    # Get workers with shifts\n",
    "    workers = shifts[shifts['Agent ID'].notnull()]['Agent ID'].unique()\n",
    "    \n",
    "    # Create base metrics\n",
    "    metrics = pd.DataFrame(index=workers)\n",
    "    \n",
    "    # Calculate basic stats\n",
    "    shift_counts = shifts[shifts['Agent ID'].notnull()].groupby('Agent ID')['ID'].count()\n",
    "    cancel_counts = cancels.groupby('Worker ID').size()\n",
    "    \n",
    "    # Ensure matching indices\n",
    "    metrics['total_shifts'] = shift_counts\n",
    "    metrics['cancellations'] = cancel_counts.reindex(workers).fillna(0)\n",
    "    metrics['reliability'] = 1 - (metrics['cancellations'] / metrics['total_shifts'])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Try the simplified version\n",
    "test_metrics = analyze_worker_basic(overlap_shifts, overlap_cancels)\n",
    "print(\"\\nTest metrics head:\")\n",
    "print(test_metrics.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_business_patterns(start_date='2021-10-01', end_date='2022-01-31'):\n",
    "    \"\"\"\n",
    "    Analyze business patterns within the core overlapping period\n",
    "    \n",
    "    Parameters:\n",
    "    - start_date: Beginning of analysis period\n",
    "    - end_date: End of analysis period\n",
    "    \"\"\"\n",
    "    # Filter to overlapping period\n",
    "    mask_period = lambda df: (\n",
    "        df['Created At'].dt.date >= pd.to_datetime(start_date).date() &\n",
    "        df['Created At'].dt.date <= pd.to_datetime(end_date).date()\n",
    "    )\n",
    "    \n",
    "    shifts_filtered = shifts_df[shifts_df['Start'].dt.date.between(start_date, end_date)]\n",
    "    cancels_filtered = cancellations_df[mask_period(cancellations_df)]\n",
    "    bookings_filtered = bookings_df[mask_period(bookings_df)]\n",
    "    \n",
    "    # Time-based patterns\n",
    "    time_patterns = {\n",
    "        'hourly_patterns': pd.DataFrame({\n",
    "            'cancellations': cancels_filtered['Created At'].dt.hour.value_counts().sort_index(),\n",
    "            'bookings': bookings_filtered['Created At'].dt.hour.value_counts().sort_index()\n",
    "        }).fillna(0),\n",
    "        \n",
    "        'daily_patterns': pd.DataFrame({\n",
    "            'shifts': shifts_filtered['Start'].dt.day_name().value_counts(),\n",
    "            'cancellations': cancels_filtered['Created At'].dt.day_name().value_counts(),\n",
    "            'bookings': bookings_filtered['Created At'].dt.day_name().value_counts()\n",
    "        }).fillna(0),\n",
    "        \n",
    "        'lead_time_success': bookings_filtered[\n",
    "            ~bookings_filtered['Shift ID'].isin(cancels_filtered['Shift ID'])\n",
    "        ]['Lead Time'].describe()\n",
    "    }\n",
    "    \n",
    "    # Worker reliability - separating by data completeness\n",
    "    worker_patterns = {\n",
    "        'complete_data': analyze_worker_patterns(\n",
    "            shifts_filtered[shifts_filtered['Agent ID'].notnull()],\n",
    "            cancels_filtered,\n",
    "            bookings_filtered\n",
    "        ),\n",
    "        'missing_data': analyze_worker_patterns(\n",
    "            shifts_filtered[shifts_filtered['Agent ID'].isnull()],\n",
    "            cancels_filtered,\n",
    "            bookings_filtered\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Facility analysis\n",
    "    facility_patterns = {\n",
    "        'cancel_rates': calculate_facility_metrics(\n",
    "            shifts_filtered, cancels_filtered, bookings_filtered\n",
    "        ),\n",
    "        'ncns_impact': analyze_ncns_impact(\n",
    "            shifts_filtered, cancels_filtered\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'time_patterns': time_patterns,\n",
    "        'worker_patterns': worker_patterns,\n",
    "        'facility_patterns': facility_patterns\n",
    "    }\n",
    "\n",
    "def analyze_worker_patterns(shifts, cancels, bookings):\n",
    "    \"\"\"\n",
    "    Analyze comprehensive booking and cancellation patterns at the worker level\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    shifts : pd.DataFrame\n",
    "        Shift data including verified status and worker information\n",
    "    cancels : pd.DataFrame\n",
    "        Cancellation data with timing and reason information\n",
    "    bookings : pd.DataFrame\n",
    "        Booking data with lead times and worker details\n",
    "        \n",
    "    Returns:\n",
    "    -----------\n",
    "    dict\n",
    "        Dictionary containing detailed worker behavior analysis\n",
    "    \"\"\"\n",
    "    # Start with valid workers only\n",
    "    active_workers = shifts[shifts['Agent ID'].notnull()]['Agent ID'].unique()\n",
    "    \n",
    "    # Initialize success metrics DataFrame first\n",
    "    success_metrics = pd.DataFrame(index=active_workers)\n",
    "    \n",
    "    # Basic counts\n",
    "    shift_counts = shifts[shifts['Agent ID'].notnull()].groupby('Agent ID')['ID'].count()\n",
    "    cancel_counts = cancels.groupby('Worker ID').size()\n",
    "    ncns_counts = cancels[cancels['Action'] == 'NO_CALL_NO_SHOW'].groupby('Worker ID').size()\n",
    "    \n",
    "    # Add basic metrics with proper index alignment\n",
    "    success_metrics['total_shifts'] = shift_counts\n",
    "    success_metrics['cancellations'] = cancel_counts.reindex(active_workers).fillna(0)\n",
    "    success_metrics['ncns'] = ncns_counts.reindex(active_workers).fillna(0)\n",
    "    \n",
    "    # Calculate derived metrics\n",
    "    success_metrics['reliability'] = 1 - (success_metrics['cancellations'] / success_metrics['total_shifts'])\n",
    "    success_metrics['ncns_rate'] = success_metrics['ncns'] / success_metrics['cancellations'].replace(0, 1)\n",
    "    \n",
    "    # Add verification rate\n",
    "    verify_rate = shifts[shifts['Agent ID'].notnull()].groupby('Agent ID')['Verified'].mean()\n",
    "    success_metrics['completion_rate'] = verify_rate\n",
    "    \n",
    "    # Add charge and time metrics\n",
    "    charge_stats = shifts[shifts['Agent ID'].notnull()].groupby('Agent ID')['Charge'].agg(['mean', 'std'])\n",
    "    time_stats = shifts[shifts['Agent ID'].notnull()].groupby('Agent ID')['Time'].agg(['mean', 'std'])\n",
    "    \n",
    "    success_metrics['avg_charge'] = charge_stats['mean']\n",
    "    success_metrics['consistency'] = 1 - (time_stats['std'] / time_stats['mean'].replace(0, np.inf))\n",
    "    \n",
    "    # Pre-calculate datetime features for patterns\n",
    "    bookings = bookings.copy()\n",
    "    bookings['booking_hour'] = bookings['Created At'].dt.hour\n",
    "    bookings['booking_day'] = bookings['Created At'].dt.day_name()\n",
    "    \n",
    "    # Analyze booking patterns\n",
    "    booking_patterns = {\n",
    "        'time_preferences': {\n",
    "            'booking_hours': bookings.groupby('Worker ID')['booking_hour'].value_counts(),\n",
    "            'booking_days': bookings.groupby('Worker ID')['booking_day'].value_counts(),\n",
    "            'lead_time_stats': bookings.groupby('Worker ID')['Lead Time'].describe()\n",
    "        },\n",
    "        'shift_preferences': {\n",
    "            'shift_types': shifts[shifts['Agent ID'].isin(active_workers)].groupby(\n",
    "                ['Agent ID', 'Shift Type']).size().unstack(fill_value=0),\n",
    "            'facility_choices': shifts[shifts['Agent ID'].isin(active_workers)].groupby(\n",
    "                ['Agent ID', 'Facility ID']).size().unstack(fill_value=0)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Calculate overall score\n",
    "    weights = {\n",
    "        'completion_rate': 0.4,\n",
    "        'reliability': 0.3,\n",
    "        'consistency': 0.2,\n",
    "        'avg_charge': 0.1\n",
    "    }\n",
    "    \n",
    "    # Normalize any missing columns\n",
    "    available_metrics = [m for m in weights.keys() if m in success_metrics.columns]\n",
    "    weight_sum = sum(weights[m] for m in available_metrics)\n",
    "    \n",
    "    success_metrics['overall_score'] = sum(\n",
    "        success_metrics[metric] * (weights[metric] / weight_sum)\n",
    "        for metric in available_metrics\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'success_metrics': success_metrics,\n",
    "        'booking_patterns': booking_patterns\n",
    "    }\n",
    "\"\"\"Worker Profiles:\n",
    "\n",
    "Creates a baseline profile for each worker using shift data\n",
    "Captures basic metrics like total shifts, verification rates, and pricing patterns\n",
    "\n",
    "\n",
    "Booking Patterns:\n",
    "\n",
    "Analyzes when workers prefer to book shifts (time of day, day of week)\n",
    "Examines lead time patterns\n",
    "Identifies preferences for shift types and facilities\n",
    "\n",
    "\n",
    "Cancellation Patterns:\n",
    "\n",
    "Studies when cancellations typically occur\n",
    "Calculates cancellation rates and no-show rates\n",
    "Analyzes lead times for cancellations\n",
    "\n",
    "\n",
    "Success Metrics:\n",
    "\n",
    "Combines multiple factors into an overall worker score\n",
    "Uses weighted metrics for completion, reliability, consistency, and earnings\n",
    "Allows for customization of weights based on business priorities\"\"\"\n",
    "\n",
    "def calculate_facility_metrics(shifts, cancels, bookings):\n",
    "    \"\"\"Calculate key facility metrics\"\"\"\n",
    "    return {\n",
    "        'cancel_rates': (cancels.groupby('Facility ID').size() / \n",
    "                        shifts.groupby('Facility ID').size()),\n",
    "        'rebooking_success': calculate_rebooking_rates(shifts, cancels, bookings),\n",
    "        'shift_fulfillment': calculate_fulfillment_rates(shifts, bookings)\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_worker_reliability(shifts, cancels):\n",
    "    \"\"\"\n",
    "    Calculate reliability scores for workers based on their history\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    shifts : pd.DataFrame\n",
    "        Shift data with worker information\n",
    "    cancels : pd.DataFrame\n",
    "        Cancellation data\n",
    "        \n",
    "    Returns:\n",
    "    -----------\n",
    "    pd.DataFrame\n",
    "        Worker reliability metrics\n",
    "    \"\"\"\n",
    "    worker_metrics = pd.DataFrame()\n",
    "    \n",
    "    # Only analyze workers with valid IDs\n",
    "    valid_workers = shifts[shifts['Agent ID'].notnull()]\n",
    "    \n",
    "    # Calculate basic metrics per worker\n",
    "    worker_metrics = valid_workers.groupby('Agent ID').agg({\n",
    "        'ID': 'count',  # Total shifts\n",
    "        'Verified': 'mean',  # Verification rate\n",
    "        'Charge': 'mean'  # Average charge rate\n",
    "    }).rename(columns={\n",
    "        'ID': 'total_shifts',\n",
    "        'Verified': 'verification_rate',\n",
    "        'Charge': 'avg_charge'\n",
    "    })\n",
    "    \n",
    "    # Add cancellation metrics\n",
    "    cancellation_rates = (\n",
    "        cancels.groupby('Worker ID')\n",
    "        .agg({\n",
    "            'Shift ID': 'count',\n",
    "            'Action': lambda x: (x == 'NO_CALL_NO_SHOW').mean()\n",
    "        })\n",
    "        .rename(columns={\n",
    "            'Shift ID': 'cancellations',\n",
    "            'Action': 'ncns_rate'\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    worker_metrics = worker_metrics.join(\n",
    "        cancellation_rates, \n",
    "        how='left'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Calculate reliability score (you can adjust the formula)\n",
    "    worker_metrics['reliability_score'] = (\n",
    "        worker_metrics['verification_rate'] * 0.4 +\n",
    "        (1 - worker_metrics['ncns_rate']) * 0.4 +\n",
    "        (1 - worker_metrics['cancellations']/worker_metrics['total_shifts']) * 0.2\n",
    "    )\n",
    "    \n",
    "    return worker_metrics.sort_values('reliability_score', ascending=False)\n",
    "\n",
    "def analyze_cancel_timing(cancels):\n",
    "    \"\"\"\n",
    "    Analyze cancellation timing patterns including day/hour distribution,\n",
    "    lead times, and seasonal patterns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cancels : pd.DataFrame\n",
    "        Cancellation data with datetime columns and lead times\n",
    "        \n",
    "    Returns:\n",
    "    -----------\n",
    "    dict : Dictionary containing timing analysis results\n",
    "    \"\"\"\n",
    "    timing_analysis = {\n",
    "        # Time of day patterns\n",
    "        'hourly_distribution': cancels['Created At'].dt.hour.value_counts().sort_index(),\n",
    "        'daily_distribution': cancels['Created At'].dt.day_name().value_counts(),\n",
    "        \n",
    "        # Lead time analysis\n",
    "        'lead_time_stats': cancels['Lead Time'].describe(),\n",
    "        'lead_time_buckets': pd.cut(\n",
    "            cancels['Lead Time'],\n",
    "            bins=[-float('inf'), 0, 4, 24, 72, float('inf')],\n",
    "            labels=['After Start', 'Under 4hrs', '4-24hrs', '1-3 days', 'Over 3 days']\n",
    "        ).value_counts().sort_index(),\n",
    "        \n",
    "        # Action type by timing\n",
    "        'timing_by_action': pd.crosstab(\n",
    "            pd.cut(cancels['Lead Time'], \n",
    "                  bins=[-float('inf'), 0, 4, 24, 72, float('inf')],\n",
    "                  labels=['After Start', 'Under 4hrs', '4-24hrs', '1-3 days', 'Over 3 days']),\n",
    "            cancels['Action']\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return timing_analysis\n",
    "\n",
    "def calculate_rebooking_rates(shifts, cancels, bookings):\n",
    "    \"\"\"\n",
    "    Calculate how successfully cancelled shifts get rebooked\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    shifts : pd.DataFrame\n",
    "        Shift data\n",
    "    cancels : pd.DataFrame\n",
    "        Cancellation data\n",
    "    bookings : pd.DataFrame\n",
    "        Booking data\n",
    "        \n",
    "    Returns:\n",
    "    -----------\n",
    "    dict : Dictionary containing rebooking analysis\n",
    "    \"\"\"\n",
    "    # Get cancelled shifts\n",
    "    cancelled_shifts = cancels['Shift ID'].unique()\n",
    "    \n",
    "    # Look at subsequent bookings for cancelled shifts\n",
    "    rebooking_analysis = {\n",
    "        'total_cancellations': len(cancelled_shifts),\n",
    "        'rebooked_count': sum(\n",
    "            bookings['Shift ID'].isin(cancelled_shifts) &\n",
    "            (bookings['Created At'] > cancels.groupby('Shift ID')['Created At'].first())\n",
    "        ),\n",
    "        'rebooking_lead_times': bookings[\n",
    "            bookings['Shift ID'].isin(cancelled_shifts)\n",
    "        ]['Lead Time'].describe(),\n",
    "        \n",
    "        # Facility level analysis\n",
    "        'facility_rebooking_rates': pd.DataFrame({\n",
    "            'cancellations': cancels.groupby('Facility ID').size(),\n",
    "            'rebookings': bookings[\n",
    "                bookings['Shift ID'].isin(cancelled_shifts)\n",
    "            ].groupby('Facility ID').size()\n",
    "        }).fillna(0)\n",
    "    }\n",
    "    \n",
    "    # Calculate success rate\n",
    "    rebooking_analysis['overall_rebooking_rate'] = (\n",
    "        rebooking_analysis['rebooked_count'] / \n",
    "        rebooking_analysis['total_cancellations']\n",
    "    )\n",
    "    \n",
    "    return rebooking_analysis\n",
    "\n",
    "def calculate_fulfillment_rates(shifts, bookings):\n",
    "    \"\"\"\n",
    "    Calculate shift fulfillment rates and patterns\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    shifts : pd.DataFrame\n",
    "        Shift data including verification status\n",
    "    bookings : pd.DataFrame\n",
    "        Booking data\n",
    "        \n",
    "    Returns:\n",
    "    -----------\n",
    "    dict : Dictionary containing fulfillment analysis\n",
    "    \"\"\"\n",
    "    fulfillment_analysis = {\n",
    "        # Overall fulfillment\n",
    "        'total_shifts': len(shifts),\n",
    "        'booked_shifts': len(shifts[shifts['Agent ID'].notnull()]),\n",
    "        'verified_shifts': shifts['Verified'].sum(),\n",
    "        \n",
    "        # Fulfillment by type\n",
    "        'fulfillment_by_type': pd.DataFrame({\n",
    "            'total': shifts.groupby('Shift Type').size(),\n",
    "            'booked': shifts[shifts['Agent ID'].notnull()].groupby('Shift Type').size(),\n",
    "            'verified': shifts[shifts['Verified']].groupby('Shift Type').size()\n",
    "        }).fillna(0),\n",
    "        \n",
    "        # Fulfillment by role\n",
    "        'fulfillment_by_role': pd.DataFrame({\n",
    "            'total': shifts.groupby('Agent Req').size(),\n",
    "            'booked': shifts[shifts['Agent ID'].notnull()].groupby('Agent Req').size(),\n",
    "            'verified': shifts[shifts['Verified']].groupby('Agent Req').size()\n",
    "        }).fillna(0)\n",
    "    }\n",
    "    \n",
    "    # Calculate rates\n",
    "    fulfillment_analysis['overall_booking_rate'] = (\n",
    "        fulfillment_analysis['booked_shifts'] / \n",
    "        fulfillment_analysis['total_shifts']\n",
    "    )\n",
    "    fulfillment_analysis['overall_verification_rate'] = (\n",
    "        fulfillment_analysis['verified_shifts'] / \n",
    "        fulfillment_analysis['total_shifts']\n",
    "    )\n",
    "    \n",
    "    return fulfillment_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis\n",
    "worker_analysis = analyze_worker_patterns(\n",
    "    overlap_shifts,\n",
    "    overlap_cancels,\n",
    "    overlap_bookings\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n=== Worker Analysis ===\")\n",
    "print(\"\\nMetrics Summary:\")\n",
    "print(worker_analysis['success_metrics'].describe())\n",
    "\n",
    "print(\"\\nTop 5 Most Reliable Workers:\")\n",
    "print(worker_analysis['success_metrics'].nlargest(5, 'reliability')[\n",
    "    ['total_shifts', 'cancellations', 'reliability', 'completion_rate']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "overlap_start = shifts_df['Start'].dt.date.min()\n",
    "overlap_end = shifts_df['Start'].dt.date.max()\n",
    "\n",
    "# Filter data to overlapping period\n",
    "overlap_shifts = shifts_df[shifts_df['Start'].dt.date.between(overlap_start, overlap_end)]\n",
    "overlap_cancels = cancellations_df[\n",
    "    cancellations_df['Created At'].dt.date.between(overlap_start, overlap_end)\n",
    "]\n",
    "overlap_bookings = bookings_df[\n",
    "    bookings_df['Created At'].dt.date.between(overlap_start, overlap_end)\n",
    "]\n",
    "\n",
    "# Run the analysis\n",
    "worker_analysis = analyze_worker_patterns(\n",
    "    overlap_shifts,\n",
    "    overlap_cancels,\n",
    "    overlap_bookings\n",
    ")\n",
    "\n",
    "# Display summary results\n",
    "print(\"\\n=== Worker Pattern Analysis ===\")\n",
    "print(\"\\nTop Performing Workers:\")\n",
    "print(worker_analysis['success_metrics'].nlargest(5, 'overall_score'))\n",
    "\n",
    "print(\"\\nBooking Time Preferences (Top 3 Hours):\")\n",
    "print(worker_analysis['booking_patterns']['time_preferences']['booking_hours']\n",
    "      .groupby(level=0).nlargest(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think critically about this:\n",
    "\n",
    "Understanding the Data Context\n",
    "From the proposal:\n",
    "\n",
    "\n",
    "They explicitly say booking data is a subset\n",
    "They state it's \"OK\" because it's meant to show booking behavior patterns\n",
    "However, they don't mention if cancellation data is complete or a subset\n",
    "The shifts data appears to be the \"source of truth\" (Oct 2021 - Jan 2022)\n",
    "\n",
    "\n",
    "Analysis Implications\n",
    "We should split our analysis into two categories:\n",
    "\n",
    "A. Full Period Analysis (Using Shifts Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Lets think critically about this:\n",
    "\n",
    "Understanding the Data Context\n",
    "From the proposal:\n",
    "\n",
    "\n",
    "They explicitly say booking data is a subset\n",
    "They state it's \"OK\" because it's meant to show booking behavior patterns\n",
    "However, they don't mention if cancellation data is complete or a subset\n",
    "The shifts data appears to be the \"source of truth\" (Oct 2021 - Jan 2022)\n",
    "\n",
    "\n",
    "Analysis Implications\n",
    "We should split our analysis into two categories: \"\"\"\n",
    "# A. Full Period Analysis (Using Shifts Data)\n",
    "#  B: Behavioral Analysis (Using Overlap Period)\n",
    "\n",
    "\"\"\"The key insight is that we should:\n",
    "\n",
    "Use shifts data for absolute metrics\n",
    "Use overlap periods for behavioral analysis\n",
    "Be clear about limitations in our findings\n",
    "Focus on patterns rather than absolute numbers for booking/cancellation behavior\n",
    "\n",
    "This matches their intent while making the best use of available data.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# A. Full Period Analysis (Using Shifts Data)\n",
    "def analyze_shifts_complete():\n",
    "    \"\"\"\n",
    "    Analyze the complete shifts dataset for overall marketplace health\n",
    "    \n",
    "    Note: This analysis uses only the shifts dataset which appears to be \n",
    "    complete for Oct 2021 - Jan 2022.\n",
    "    \"\"\"\n",
    "    shifts_analysis = {\n",
    "        # Basic marketplace metrics\n",
    "        'total_shifts': len(shifts_df),\n",
    "        'shifts_by_type': shifts_df['Shift Type'].value_counts(),\n",
    "        'verification_rate': shifts_df['Verified'].mean(),\n",
    "        \n",
    "        # Financial metrics\n",
    "        'charge_patterns': shifts_df.groupby('Agent Req')['Charge'].describe(),\n",
    "        \n",
    "        # Time patterns\n",
    "        'shift_distribution': {\n",
    "            'by_day': shifts_df['Start'].dt.day_name().value_counts(),\n",
    "            'by_hour': shifts_df['Start'].dt.hour.value_counts().sort_index()\n",
    "        },\n",
    "        \n",
    "        # Facility metrics\n",
    "        'facility_patterns': shifts_df.groupby('Facility ID').agg({\n",
    "            'ID': 'count',\n",
    "            'Verified': 'mean',\n",
    "            'Charge': 'mean'\n",
    "        }).rename(columns={'ID': 'total_shifts'})\n",
    "    }\n",
    "    return shifts_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  B: Behavioral Analysis (Using Overlap Period)\n",
    "def analyze_booking_behavior(start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Analyze HCP booking and cancellation behavior where we have all datasets\n",
    "    \n",
    "    Notes:\n",
    "    - This analysis uses the period where we have overlapping data\n",
    "    - Focus is on understanding behavioral patterns rather than absolute numbers\n",
    "    \"\"\"\n",
    "    # Filter to overlap period if dates provided\n",
    "    if start_date and end_date:\n",
    "        shifts_subset = shifts_df[shifts_df['Start'].dt.date.between(start_date, end_date)]\n",
    "        cancels_subset = cancellations_df[\n",
    "            cancellations_df['Created At'].dt.date.between(start_date, end_date)\n",
    "        ]\n",
    "        bookings_subset = bookings_df[\n",
    "            bookings_df['Created At'].dt.date.between(start_date, end_date)\n",
    "        ]\n",
    "    else:\n",
    "        shifts_subset = shifts_df\n",
    "        cancels_subset = cancellations_df\n",
    "        bookings_subset = bookings_df\n",
    "    \n",
    "    # Cross-reference data\n",
    "    shifts_with_outcomes = shifts_subset.copy()\n",
    "    shifts_with_outcomes['was_booked'] = shifts_subset['ID'].isin(bookings_subset['Shift ID'])\n",
    "    shifts_with_outcomes['was_cancelled'] = shifts_subset['ID'].isin(cancels_subset['Shift ID'])\n",
    "    \n",
    "    behavior_analysis = {\n",
    "        # Booking patterns\n",
    "        'booking_behavior': {\n",
    "            'lead_times': bookings_subset['Lead Time'].describe(),\n",
    "            'booking_times': bookings_subset['Created At'].dt.hour.value_counts().sort_index()\n",
    "        },\n",
    "        \n",
    "        # Cancellation patterns\n",
    "        'cancellation_behavior': {\n",
    "            'cancel_types': cancels_subset['Action'].value_counts(),\n",
    "            'lead_times': cancels_subset['Lead Time'].describe(),\n",
    "            'cancel_times': cancels_subset['Created At'].dt.hour.value_counts().sort_index()\n",
    "        },\n",
    "        \n",
    "        # Shift outcomes\n",
    "        'shift_outcomes': {\n",
    "            'total_shifts': len(shifts_subset),\n",
    "            'booked_count': shifts_with_outcomes['was_booked'].sum(),\n",
    "            'cancelled_count': shifts_with_outcomes['was_cancelled'].sum(),\n",
    "            'booking_rate': shifts_with_outcomes['was_booked'].mean(),\n",
    "            'cancellation_rate': shifts_with_outcomes['was_cancelled'].mean()\n",
    "        }\n",
    "    }\n",
    "    return behavior_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Missing Agent IDs Let's cross-check with both datasets:\n",
    "\n",
    "\n",
    "def analyze_missing_agents_behavior():\n",
    "    \"\"\"\n",
    "    Analyze what happens to shifts with missing Agent IDs\n",
    "    \"\"\"\n",
    "    # Get shifts with/without agents\n",
    "    missing_agent = shifts_df[shifts_df['Agent ID'].isnull()]\n",
    "    has_agent = shifts_df[shifts_df['Agent ID'].notnull()]\n",
    "    \n",
    "    # Cross reference with bookings and cancellations\n",
    "    missing_outcomes = {\n",
    "        'booked': missing_agent['ID'].isin(bookings_df['Shift ID']).mean(),\n",
    "        'cancelled': missing_agent['ID'].isin(cancellations_df['Shift ID']).mean(),\n",
    "        'verified': missing_agent['Verified'].mean()\n",
    "    }\n",
    "    \n",
    "    has_agent_outcomes = {\n",
    "        'booked': has_agent['ID'].isin(bookings_df['Shift ID']).mean(),\n",
    "        'cancelled': has_agent['ID'].isin(cancellations_df['Shift ID']).mean(),\n",
    "        'verified': has_agent['Verified'].mean()\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'missing_agent_outcomes': missing_outcomes,\n",
    "        'has_agent_outcomes': has_agent_outcomes\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all analyses\n",
    "print(\"Running comprehensive analyses...\")\n",
    "\n",
    "# 1. Full Shifts Analysis\n",
    "print(\"\\n=== COMPLETE SHIFTS ANALYSIS (Oct 2021 - Jan 2022) ===\")\n",
    "shifts_analysis = analyze_shifts_complete()\n",
    "print(\"\\nBasic Marketplace Metrics:\")\n",
    "print(f\"Total Shifts: {shifts_analysis['total_shifts']:,}\")\n",
    "print(\"\\nShift Types:\")\n",
    "print(shifts_analysis['shifts_by_type'])\n",
    "print(f\"\\nOverall Verification Rate: {shifts_analysis['verification_rate']:.2%}\")\n",
    "\n",
    "# 2. Behavioral Analysis \n",
    "# Using the overlap period (focusing on patterns rather than absolute numbers)\n",
    "print(\"\\n=== BEHAVIORAL ANALYSIS (Overlap Period) ===\")\n",
    "start_date = shifts_df['Start'].dt.date.min()  # Oct 1, 2021\n",
    "end_date = shifts_df['Start'].dt.date.max()    # Jan 31, 2022\n",
    "behavior_analysis = analyze_booking_behavior(start_date, end_date)\n",
    "\n",
    "print(\"\\nBooking Patterns:\")\n",
    "print(\"Lead Times (hours):\")\n",
    "print(behavior_analysis['booking_behavior']['lead_times'])\n",
    "\n",
    "print(\"\\nCancellation Types:\")\n",
    "print(behavior_analysis['cancellation_behavior']['cancel_types'])\n",
    "\n",
    "print(\"\\nShift Outcomes:\")\n",
    "for metric, value in behavior_analysis['shift_outcomes'].items():\n",
    "    if 'rate' in metric:\n",
    "        print(f\"{metric}: {value:.2%}\")\n",
    "    else:\n",
    "        print(f\"{metric}: {value:,}\")\n",
    "\n",
    "# 3. Missing Agent ID Analysis\n",
    "print(\"\\n=== MISSING AGENT ID ANALYSIS ===\")\n",
    "agent_behavior = analyze_missing_agents_behavior()\n",
    "\n",
    "print(\"\\nShifts with Missing Agent IDs:\")\n",
    "for metric, value in agent_behavior['missing_agent_outcomes'].items():\n",
    "    print(f\"{metric}: {value:.2%}\")\n",
    "\n",
    "print(\"\\nShifts with Agent IDs:\")\n",
    "for metric, value in agent_behavior['has_agent_outcomes'].items():\n",
    "    print(f\"{metric}: {value:.2%}\")\n",
    "\n",
    "# Store results in summary object for later use\n",
    "summary.add_summary('complete_analysis', 'shifts', shifts_analysis)\n",
    "summary.add_summary('complete_analysis', 'behavior', behavior_analysis)\n",
    "summary.add_summary('complete_analysis', 'missing_agents', agent_behavior)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This code:\n",
    "\n",
    "Analyzes the complete shifts dataset first\n",
    "Looks at booking/cancellation behavior in the overlap period\n",
    "Specifically examines shifts with/without Agent IDs\n",
    "Stores all results in our summary object\n",
    "\n",
    "The output will help us understand:\n",
    "\n",
    "Overall marketplace metrics from shifts data\n",
    "Behavioral patterns where we have complete data\n",
    "What missing Agent IDs might mean\n",
    "\n",
    "Each section is clearly labeled, and results are formatted for easy reading. We can use these results to:\n",
    "\n",
    "Identify key patterns\n",
    "Support our findings\n",
    "Guide additional analysis\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 1: SUCCESS PATH ANALYSIS\n",
    "#A. Define and Validate Success Metrics\n",
    "#Deep dive into successful shifts \n",
    "\n",
    "class ShiftSuccessAnalysis:\n",
    "    \"\"\"\n",
    "    Analyzes the complete lifecycle of shifts from posting to completion.\n",
    "    \n",
    "    Core metrics tracked:\n",
    "    - Booking success: Did the shift get booked?\n",
    "    - Retention success: Did the booking stick (no cancellation)?\n",
    "    - Completion success: Was the shift verified as worked?\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, shifts_df, bookings_df, cancellations_df):\n",
    "        \"\"\"Initialize with our three core datasets.\"\"\"\n",
    "        self.shifts_df = shifts_df.copy()\n",
    "        self.bookings_df = bookings_df.copy()\n",
    "        self.cancellations_df = cancellations_df.copy()\n",
    "        self.success_journey = None\n",
    "        \n",
    "        # Verify data compatibility\n",
    "        self._validate_data()\n",
    "        \n",
    "        # Create enhanced dataset\n",
    "        self._create_success_journey()\n",
    "    \n",
    "    def _validate_data(self):\n",
    "        \"\"\"\n",
    "        Ensure data quality and compatibility across datasets.\n",
    "        \"\"\"\n",
    "        # Check required columns\n",
    "        required_columns = {\n",
    "            'shifts': ['ID', 'Start', 'End', 'Verified', 'Agent ID', \n",
    "                      'Facility ID', 'Agent Req', 'Shift Type', 'Charge'],\n",
    "            'bookings': ['Shift ID', 'Created At', 'Worker ID'],\n",
    "            'cancellations': ['Shift ID', 'Created At', 'Action', 'Lead Time']\n",
    "        }\n",
    "        \n",
    "        for df_name, columns in required_columns.items():\n",
    "            df = getattr(self, f\"{df_name}_df\")\n",
    "            missing_cols = [col for col in columns if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing columns in {df_name}: {missing_cols}\")\n",
    "        \n",
    "        # Print coverage analysis\n",
    "        self._analyze_coverage()\n",
    "    \n",
    "    def _analyze_coverage(self):\n",
    "        \"\"\"Analyze data coverage and overlap.\"\"\"\n",
    "        shifts_ids = set(self.shifts_df['ID'])\n",
    "        booking_ids = set(self.bookings_df['Shift ID'])\n",
    "        cancel_ids = set(self.cancellations_df['Shift ID'])\n",
    "        \n",
    "        print(\"\\n=== Data Coverage Analysis ===\")\n",
    "        print(f\"\\nTotal Shifts: {len(shifts_ids):,}\")\n",
    "        print(f\"Shifts with Bookings: {len(shifts_ids & booking_ids):,} \"\n",
    "              f\"({len(shifts_ids & booking_ids)/len(shifts_ids):.1%})\")\n",
    "        print(f\"Shifts with Cancellations: {len(shifts_ids & cancel_ids):,} \"\n",
    "              f\"({len(shifts_ids & cancel_ids)/len(shifts_ids):.1%})\")\n",
    "        \n",
    "        # Analyze potential data quality issues\n",
    "        orphaned_bookings = len(booking_ids - shifts_ids)\n",
    "        orphaned_cancels = len(cancel_ids - shifts_ids)\n",
    "        \n",
    "        if orphaned_bookings or orphaned_cancels:\n",
    "            print(\"\\nPotential Data Quality Issues:\")\n",
    "            print(f\"Orphaned Bookings: {orphaned_bookings:,}\")\n",
    "            print(f\"Orphaned Cancellations: {orphaned_cancels:,}\")\n",
    "    \n",
    "    def _create_success_journey(self):\n",
    "        \"\"\"\n",
    "        Creates enhanced dataset tracking complete shift lifecycle.\n",
    "        \"\"\"\n",
    "        journey = self.shifts_df.copy()\n",
    "        \n",
    "        # Add booking information\n",
    "        booking_times = self.bookings_df.groupby('Shift ID').agg({\n",
    "            'Created At': ['first', 'count']\n",
    "        }).reset_index()\n",
    "        booking_times.columns = ['Shift ID', 'First Booking', 'Booking Count']\n",
    "        \n",
    "        journey = journey.merge(\n",
    "            booking_times, \n",
    "            left_on='ID', \n",
    "            right_on='Shift ID', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Add cancellation information\n",
    "        cancel_info = self.cancellations_df.groupby('Shift ID').agg({\n",
    "            'Created At': 'first',\n",
    "            'Action': 'first',\n",
    "            'Lead Time': 'first'\n",
    "        }).reset_index()\n",
    "        \n",
    "        journey = journey.merge(\n",
    "            cancel_info,\n",
    "            left_on='ID',\n",
    "            right_on='Shift ID',\n",
    "            how='left',\n",
    "            suffixes=('_booking', '_cancel')\n",
    "        )\n",
    "        \n",
    "        # Calculate success metrics\n",
    "        journey['was_booked'] = journey['First Booking'].notnull()\n",
    "        journey['was_cancelled'] = journey['Created At_cancel'].notnull()\n",
    "        journey['was_completed'] = journey['Verified']\n",
    "        \n",
    "        # Calculate time to shift start (from booking)\n",
    "        journey['lead_time'] = (\n",
    "            journey['Start'] - journey['First Booking']\n",
    "        ).dt.total_seconds() / 3600  # Convert to hours\n",
    "        \n",
    "        self.success_journey = journey\n",
    "        \n",
    "        # Print initial success metrics\n",
    "        self._print_success_metrics()\n",
    "    \n",
    "    def _print_success_metrics(self):\n",
    "        \"\"\"Print key success metrics from the journey data.\"\"\"\n",
    "        metrics = self.success_journey.agg({\n",
    "            'was_booked': 'mean',\n",
    "            'was_cancelled': 'mean',\n",
    "            'was_completed': 'mean'\n",
    "        })\n",
    "        \n",
    "        print(\"\\n=== Success Metrics ===\")\n",
    "        print(f\"Booking Rate: {metrics['was_booked']:.1%}\")\n",
    "        print(f\"Cancellation Rate: {metrics['was_cancelled']:.1%}\")\n",
    "        print(f\"Completion Rate: {metrics['was_completed']:.1%}\")\n",
    "    \n",
    "    def analyze_verification_discrepancy(self):\n",
    "        \"\"\"\n",
    "        Investigates why shifts might be verified without appearing in booking logs.\n",
    "        \"\"\"\n",
    "        verified_shifts = self.success_journey[self.success_journey['Verified']]\n",
    "        unbooked_verified = verified_shifts[~verified_shifts['was_booked']]\n",
    "        \n",
    "        results = {\n",
    "            'overview': {\n",
    "                'total_shifts': len(self.success_journey),\n",
    "                'verified_shifts': len(verified_shifts),\n",
    "                'unbooked_verified': len(unbooked_verified),\n",
    "                'verification_rate': len(verified_shifts) / len(self.success_journey),\n",
    "                'unbooked_verified_rate': len(unbooked_verified) / len(verified_shifts)\n",
    "            },\n",
    "            'unbooked_verified_patterns': {\n",
    "                'by_role': unbooked_verified['Agent Req'].value_counts(),\n",
    "                'by_shift_type': unbooked_verified['Shift Type'].value_counts(),\n",
    "                'by_facility': unbooked_verified['Facility ID'].value_counts().head()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        agent_patterns = self.success_journey.groupby(\n",
    "            self.success_journey['Agent ID'].isnull()\n",
    "        ).agg({\n",
    "            'was_booked': 'mean',\n",
    "            'Verified': 'mean',\n",
    "            'was_cancelled': 'mean'\n",
    "        }).round(3)\n",
    "        \n",
    "        results['agent_id_patterns'] = agent_patterns\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_success_patterns(self):\n",
    "        \"\"\"\n",
    "        Analyzes patterns in successfully completed shifts.\n",
    "        \"\"\"\n",
    "        successful = self.success_journey[\n",
    "            (self.success_journey['Verified']) & \n",
    "            (~self.success_journey['was_cancelled'])\n",
    "        ]\n",
    "        \n",
    "        patterns = {\n",
    "            'timing': {\n",
    "                'hour_distribution': successful['Start'].dt.hour.value_counts().sort_index(),\n",
    "                'day_distribution': successful['Start'].dt.day_name().value_counts(),\n",
    "                'lead_times': successful['lead_time'].describe()\n",
    "            },\n",
    "            'characteristics': {\n",
    "                'role_distribution': successful['Agent Req'].value_counts(),\n",
    "                'shift_types': successful['Shift Type'].value_counts(),\n",
    "                'charge_rates': successful.groupby('Agent Req')['Charge'].agg(['mean', 'std'])\n",
    "            },\n",
    "            'facility_patterns': {\n",
    "                'success_rates': (\n",
    "                    self.success_journey.groupby('Facility ID')['Verified'].agg(['mean', 'count'])\n",
    "                    .sort_values('mean', ascending=False)\n",
    "                    .query('count >= 10')  # Only facilities with sufficient data\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis\n",
    "analyzer = ShiftSuccessAnalysis(shifts_df, bookings_df, cancellations_df)\n",
    "\n",
    "# Analyze verification discrepancy\n",
    "discrepancy_results = analyzer.analyze_verification_discrepancy()\n",
    "\n",
    "print(\"\\n=== Verification Discrepancy Analysis ===\")\n",
    "print(\"\\nOverview:\")\n",
    "for metric, value in discrepancy_results['overview'].items():\n",
    "    if 'rate' in metric:\n",
    "        print(f\"{metric}: {value:.1%}\")\n",
    "    else:\n",
    "        print(f\"{metric}: {value:,}\")\n",
    "\n",
    "print(\"\\nUnbooked Verified Shifts by Role:\")\n",
    "print(discrepancy_results['unbooked_verified_patterns']['by_role'])\n",
    "\n",
    "print(\"\\nAgent ID Impact:\")\n",
    "print(discrepancy_results['agent_id_patterns'])\n",
    "\n",
    "# Get success patterns\n",
    "success_patterns = analyzer.get_success_patterns()\n",
    "\n",
    "print(\"\\n=== Success Patterns ===\")\n",
    "print(\"\\nMost Successful Shift Types:\")\n",
    "print(success_patterns['characteristics']['shift_types'])\n",
    "\n",
    "print(\"\\nAverage Charge Rates for Successful Shifts:\")\n",
    "print(success_patterns['characteristics']['charge_rates'])\n",
    "\n",
    "print(\"\\nTop 5 Facilities by Success Rate (min 10 shifts):\")\n",
    "print(success_patterns['facility_patterns']['success_rates'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing a path: backup pool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Backup Pool Estimation\n",
    "# The goal is to calculate the size of the backup pool needed to cover 75% of late cancellations.\n",
    "# Backup Pool Estimation\n",
    "\n",
    "# Assumptions\n",
    "BACKUP_COVERAGE_TARGET = 0.75  # Cover 75% of late cancellations\n",
    "\n",
    "def estimate_backup_pool(shifts_df, cancellations_df):\n",
    "    \"\"\"\n",
    "    Estimate the size of a backup pool needed to cover late cancellations.\n",
    "    \n",
    "    Parameters:\n",
    "    - shifts_df (pd.DataFrame): Shift data.\n",
    "    - cancellations_df (pd.DataFrame): Cancellations data with lead times.\n",
    "    \n",
    "    Returns:\n",
    "    - Estimated pool size needed for target coverage.\n",
    "    - Contextual insights into late cancellations.\n",
    "    \"\"\"\n",
    "    # Step 1: Focus on Late Cancellations (<4 hours)\n",
    "    late_cancellations = cancellations_df[\n",
    "        cancellations_df['Lead Time'] < 4\n",
    "    ]\n",
    "    total_late_cancels = len(late_cancellations)\n",
    "    \n",
    "    print(\"=== Backup Pool Estimation for Late Cancellations ===\")\n",
    "    print(f\"Total Late Cancellations (<4hrs): {total_late_cancels:,}\")\n",
    "    \n",
    "    # Step 2: Estimate coverage required\n",
    "    target_coverage = int(total_late_cancels * BACKUP_COVERAGE_TARGET)\n",
    "    print(f\"Target Coverage (75%): {target_coverage:,} shifts\")\n",
    "\n",
    "    # Step 3: Calculate HCP Availability and Estimate Pool Size\n",
    "    late_cancel_hcps = late_cancellations['Worker ID'].value_counts()\n",
    "    avg_shifts_per_hcp = late_cancel_hcps.mean()\n",
    "    \n",
    "    if avg_shifts_per_hcp > 0:\n",
    "        pool_size = int(np.ceil(target_coverage / avg_shifts_per_hcp))\n",
    "    else:\n",
    "        pool_size = 0\n",
    "    \n",
    "    print(f\"Average Late Cancellations per HCP: {avg_shifts_per_hcp:.2f}\")\n",
    "    print(f\"Estimated Backup Pool Size: {pool_size} HCPs\")\n",
    "    \n",
    "    print(\"\\nContext:\")\n",
    "    print(\"To meet 75% coverage of late cancellations, we estimate needing a pool of pre-vetted,\")\n",
    "    print(f\"reliable HCPs who can cover approximately {avg_shifts_per_hcp:.2f} late cancellations on average.\")\n",
    "    print(\"This estimate assumes that reliable HCPs are distributed evenly across cancellations.\")\n",
    "    \n",
    "    return pool_size\n",
    "\n",
    "def identify_reliable_hcps(bookings_df, cancellations_df, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Identify reliable HCPs with cancellation rates below a given threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - bookings_df (pd.DataFrame): Booking logs with Worker IDs.\n",
    "    - cancellations_df (pd.DataFrame): Cancellations data with Worker IDs.\n",
    "    - threshold (float): Maximum cancellation rate for reliability.\n",
    "    \n",
    "    Returns:\n",
    "    - Reliable HCPs as a DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Reliable HCP Identification ===\")\n",
    "    \n",
    "    # Step 1: Calculate Total Shifts and Cancellations per Worker\n",
    "    total_shifts = bookings_df['Worker ID'].value_counts()\n",
    "    total_cancellations = cancellations_df['Worker ID'].value_counts()\n",
    "    \n",
    "    # Step 2: Calculate Cancellation Rate\n",
    "    reliability_df = pd.DataFrame({\n",
    "        'Total Shifts': total_shifts,\n",
    "        'Cancellations': total_cancellations\n",
    "    }).fillna(0)\n",
    "    reliability_df['Cancellation Rate'] = reliability_df['Cancellations'] / reliability_df['Total Shifts']\n",
    "    \n",
    "    # Step 3: Identify Reliable Workers\n",
    "    reliable_hcps = reliability_df[reliability_df['Cancellation Rate'] <= threshold]\n",
    "    reliable_hcps_sorted = reliable_hcps.sort_values(by='Cancellation Rate')\n",
    "    \n",
    "    print(f\"Total Workers Analyzed: {len(reliability_df):,}\")\n",
    "    print(f\"Workers with Cancellation Rate ≤ {threshold*100:.0f}%: {len(reliable_hcps):,}\")\n",
    "    print(\"\\nTop 5 Most Reliable Workers:\")\n",
    "    print(reliable_hcps_sorted.head())\n",
    "    \n",
    "    print(\"\\nContext:\")\n",
    "    print(\"Reliable HCPs are defined as those with a cancellation rate ≤ 10%.\")\n",
    "    print(\"This pool represents our most dependable workers, making them ideal candidates\")\n",
    "    print(\"for participation in the backup program. They are prioritized based on:\")\n",
    "    print(\"1. Total shifts worked.\")\n",
    "    print(\"2. Low cancellation counts.\")\n",
    "    \n",
    "    return reliable_hcps_sorted\n",
    "\n",
    "# Run the analyses\n",
    "backup_pool_size = estimate_backup_pool(shifts_df, cancellations_df)\n",
    "reliable_hcps = identify_reliable_hcps(bookings_df, cancellations_df)\n",
    "\n",
    "# Display final summary\n",
    "print(\"\\n=== Summary for WBD ===\")\n",
    "print(f\"Estimated Backup Pool Size (75% Late Cancel Coverage): {backup_pool_size} HCPs\")\n",
    "print(f\"Reliable HCPs Identified (Cancellation Rate ≤ 10%): {len(reliable_hcps)} workers\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_summary(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CBH_CS2)",
   "language": "python",
   "name": "cbh_cs2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
