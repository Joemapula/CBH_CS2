{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Configuration\n",
    "\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Environment checks\n",
    "print(f\"Python executable: {sys.executable}\")  \n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"\\nFiles in data directory:\", os.listdir('data'))\n",
    "```\n",
    "\n",
    "    Python executable: C:\\Users\\josep\\miniconda3\\envs\\CBH_CS2\\python.exe\n",
    "    Current working directory: C:\\Users\\josep\\Documents\\programming_projects\\GitHub\\CBH_CS2\n",
    "    \n",
    "    Files in data directory: ['booking_logs_large.csv', 'Booking_logs_sample_1000 - Sheet1.csv', 'Booking_logs_sample_100_rows.csv', 'Booking_logs_sample_header.csv', 'cancel_logs_large.csv', 'Cancel_logs_sample_1000 - Sheet1.csv', 'Cancel_logs_sample_100_rows.csv', 'Cancel_logs_sample_header.csv', 'cleveland_shifts_large.csv', 'Cleveland_shifts_Sample_1000 - Sheet1.csv', 'Cleveland_shifts_Sample_100_rows.csv', 'Cleveland_shifts_Sample_header.csv']\n",
    "    \n",
    "\n",
    "# Helper Functions and Classes\n",
    "\n",
    "\n",
    "```python\n",
    "# Helper Functions for Data Loading and Cleaning\n",
    "# update: now takes varied date ranges into account in loading\n",
    "def load_and_clean_shifts(df):\n",
    "    \"\"\"\n",
    "    Load and clean shifts dataset\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Raw shifts dataframe\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned shifts dataframe with proper datatypes\n",
    "        \n",
    "    Notes:\n",
    "        - Makes a copy to avoid modifying original data\n",
    "        - Converts datetime columns\n",
    "        - Handles potential errors in datetime conversion\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert datetime columns with error handling\n",
    "    datetime_cols = ['Start', 'End', 'Created At']\n",
    "    for col in datetime_cols:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col], format='mixed')\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {col} to datetime: {str(e)}\")\n",
    "                # Log problematic rows for investigation\n",
    "                problematic_rows = df[pd.to_datetime(df[col], format='mixed', errors='coerce').isna()]\n",
    "                if not problematic_rows.empty:\n",
    "                    print(f\"Problematic rows in {col}:\")\n",
    "                    print(problematic_rows[col].head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_and_clean_bookings(df):\n",
    "    \"\"\"\n",
    "    Load and clean booking logs dataset\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Raw bookings dataframe\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned bookings dataframe with proper datatypes\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Convert datetime columns\n",
    "    try:\n",
    "        df['Created At'] = pd.to_datetime(df['Created At'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting Created At: {str(e)}\")\n",
    "    return df\n",
    "\n",
    "def load_and_clean_cancellations(df):\n",
    "    \"\"\"\n",
    "    Load and clean cancellation logs dataset\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Raw cancellations dataframe\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned cancellations dataframe with proper datatypes\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Convert datetime columns with flexible parsing\n",
    "    try:\n",
    "        df['Created At'] = pd.to_datetime(df['Created At'], format='mixed')\n",
    "        df['Shift Start Logs'] = pd.to_datetime(df['Shift Start Logs'], format='mixed')\n",
    "    except Exception as e:\n",
    "        print(f\"Error in datetime conversion: {str(e)}\")\n",
    "        # Try to identify problematic rows\n",
    "        prob_rows = df[pd.to_datetime(df['Shift Start Logs'], format='mixed', errors='coerce').isna()]\n",
    "        if not prob_rows.empty:\n",
    "            print(\"\\nSample of problematic date formats:\")\n",
    "            print(prob_rows['Shift Start Logs'].head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def categorize_lead_time(hours):\n",
    "    \"\"\"\n",
    "    Categorize lead times based on business rules.\n",
    "    \n",
    "    Parameters:\n",
    "        hours (float): Lead time in hours\n",
    "        \n",
    "    Returns:\n",
    "        str: Category of lead time\n",
    "    \"\"\"\n",
    "    if hours < 0:\n",
    "        return 'No-Show'  # Cancelled after shift start\n",
    "    elif hours < 4:\n",
    "        return 'Late (<4hrs)'\n",
    "    elif hours < 24:\n",
    "        return 'Same Day'\n",
    "    elif hours < 72:\n",
    "        return 'Advance (<3 days)'\n",
    "    return 'Early (3+ days)'\n",
    "\n",
    "def clean_lead_times(cancellations_df):\n",
    "    \"\"\"\n",
    "    Clean and categorize lead times in cancellation data\n",
    "    \n",
    "    Parameters:\n",
    "        cancellations_df (pd.DataFrame): Raw cancellations dataframe\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned cancellations data with categorized lead times\n",
    "        pd.Series: Statistics about removed records for quality control\n",
    "    \"\"\"\n",
    "    df = cancellations_df.copy()\n",
    "    \n",
    "    # Track data quality issues\n",
    "    quality_stats = {\n",
    "        'original_rows': len(df),\n",
    "        'null_lead_times': df['Lead Time'].isnull().sum(),\n",
    "        'infinite_values': (~np.isfinite(df['Lead Time'])).sum()\n",
    "    }\n",
    "    \n",
    "    # Only remove truly invalid data\n",
    "    mask = df['Lead Time'].notnull() & np.isfinite(df['Lead Time'])\n",
    "    df = df[mask]\n",
    "    \n",
    "    # Add cleaned lead time without filtering extremes\n",
    "    df['clean_lead_time'] = df['Lead Time']\n",
    "    \n",
    "    # Categorize all lead times\n",
    "    df['cancellation_category'] = df['clean_lead_time'].apply(categorize_lead_time)\n",
    "    \n",
    "    # Add flags for extreme values for analysis\n",
    "    df['is_extreme_negative'] = df['Lead Time'] < -72  # Flag cancellations >3 days after\n",
    "    df['is_extreme_positive'] = df['Lead Time'] > 1000 # Flag cancellations >41 days before\n",
    "    \n",
    "    quality_stats['final_rows'] = len(df)\n",
    "    quality_stats['removed_rows'] = quality_stats['original_rows'] - quality_stats['final_rows']\n",
    "    \n",
    "    return df, pd.Series(quality_stats)\n",
    "\n",
    "# Data Summary Storage Class\n",
    "class DataSummary:\n",
    "    \"\"\"Class to store and manage analysis results\"\"\"\n",
    "    def __init__(self):\n",
    "        self.summaries = {}\n",
    "    \n",
    "    def add_summary(self, dataset_name, summary_type, data):\n",
    "        \"\"\"Add summary statistics to storage\"\"\"\n",
    "        if dataset_name not in self.summaries:\n",
    "            self.summaries[dataset_name] = {}\n",
    "        self.summaries[dataset_name][summary_type] = data\n",
    "    \n",
    "    def get_summary(self, dataset_name, summary_type=None):\n",
    "        \"\"\"Retrieve stored summary statistics\"\"\"\n",
    "        if summary_type:\n",
    "            return self.summaries.get(dataset_name, {}).get(summary_type)\n",
    "        return self.summaries.get(dataset_name)\n",
    "    \n",
    "    def print_summary(self, dataset_name):\n",
    "        \"\"\"Print stored summaries for a dataset\"\"\"\n",
    "        if dataset_name in self.summaries:\n",
    "            print(f\"\\nSummary for {dataset_name}:\")\n",
    "            for summary_type, data in self.summaries[dataset_name].items():\n",
    "                print(f\"\\n{summary_type}:\")\n",
    "                print(data)\n",
    "\n",
    "# Initialize summary storage\n",
    "summary = DataSummary()\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# Summary explorer helper functions \n",
    "def explore_summary(summary, indent=0, max_display_length=100):\n",
    "    \"\"\"\n",
    "    Recursively explore and display the contents of the DataSummary object.\n",
    "    \n",
    "    Parameters:\n",
    "        summary: The DataSummary object or a nested dictionary/value to explore\n",
    "        indent: Current indentation level (default: 0)\n",
    "        max_display_length: Maximum length for displayed values (default: 100)\n",
    "    \"\"\"\n",
    "    def format_value(value):\n",
    "        \"\"\"Format a value for display, truncating if too long\"\"\"\n",
    "        str_value = str(value)\n",
    "        if len(str_value) > max_display_length:\n",
    "            return str_value[:max_display_length] + '...'\n",
    "        return str_value\n",
    "    \n",
    "    def print_indented(text, indent):\n",
    "        \"\"\"Print text with proper indentation\"\"\"\n",
    "        print('    ' * indent + text)\n",
    "\n",
    "    if isinstance(summary, DataSummary):\n",
    "        # If we're starting with a DataSummary object, explore its summaries\n",
    "        print(\"\\n=== Complete Summary Contents ===\\n\")\n",
    "        explore_summary(summary.summaries, indent)\n",
    "    \n",
    "    elif isinstance(summary, dict):\n",
    "        # Recursively explore dictionary contents\n",
    "        for key, value in summary.items():\n",
    "            if isinstance(value, dict):\n",
    "                print_indented(f\"{key}:\", indent)\n",
    "                explore_summary(value, indent + 1)\n",
    "            elif isinstance(value, pd.Series) or isinstance(value, pd.DataFrame):\n",
    "                print_indented(f\"{key}: [pandas {type(value).__name__}]\", indent)\n",
    "                str_repr = str(value)\n",
    "                for line in str_repr.split('\\n')[:5]:  # Show first 5 lines\n",
    "                    print_indented(line, indent + 1)\n",
    "                if len(str_repr.split('\\n')) > 5:\n",
    "                    print_indented('...', indent + 1)\n",
    "            else:\n",
    "                print_indented(f\"{key}: {format_value(value)}\", indent)\n",
    "    \n",
    "    else:\n",
    "        # Base case: print the value\n",
    "        print_indented(format_value(summary), indent)\n",
    "\n",
    "def get_summary_structure(summary):\n",
    "    \"\"\"\n",
    "    Print just the structure of the summary without all the data values.\n",
    "    \n",
    "    Parameters:\n",
    "        summary: The DataSummary object\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Summary Structure ===\\n\")\n",
    "    for dataset_name in summary.summaries:\n",
    "        print(f\"\\nDataset: {dataset_name}\")\n",
    "        for summary_type in summary.summaries[dataset_name]:\n",
    "            print(f\"  └── {summary_type}\")\n",
    "\n",
    "# Example usage:\n",
    "print(\"First, let's see the overall structure of your summary:\")\n",
    "get_summary_structure(summary)\n",
    "\n",
    "print(\"\\nWould you like to see the complete contents of any specific dataset?\")\n",
    "print(\"You can view them using:\")\n",
    "print(\"explore_summary(summary.get_summary('dataset_name'))\")\n",
    "\n",
    "# To view everything:\n",
    "print(\"\\nOr view all contents with:\")\n",
    "print(\"explore_summary(summary)\")\n",
    "```\n",
    "\n",
    "    First, let's see the overall structure of your summary:\n",
    "    \n",
    "    === Summary Structure ===\n",
    "    \n",
    "    \n",
    "    Dataset: data_quality\n",
    "      └── audit_results\n",
    "      └── coverage_analysis\n",
    "      └── missing_data_impact\n",
    "    \n",
    "    Dataset: bookings\n",
    "      └── time_to_fill\n",
    "      └── role_patterns\n",
    "      └── rebooking_stats\n",
    "      └── shape\n",
    "      └── dtypes\n",
    "      └── missing_values\n",
    "      └── date_range\n",
    "      └── unique_ids\n",
    "    \n",
    "    Dataset: economic\n",
    "      └── overall_impact\n",
    "      └── role_impact\n",
    "      └── type_impact\n",
    "    \n",
    "    Dataset: complete_analysis\n",
    "      └── shifts\n",
    "      └── behavior\n",
    "      └── missing_agents\n",
    "    \n",
    "    Dataset: cancellations\n",
    "      └── quality_stats\n",
    "      └── extreme_values\n",
    "      └── action_types\n",
    "      └── role_patterns\n",
    "      └── shift_patterns\n",
    "      └── role_impact\n",
    "      └── shape\n",
    "      └── dtypes\n",
    "      └── missing_values\n",
    "      └── date_range\n",
    "      └── unique_ids\n",
    "    \n",
    "    Dataset: data_filtering\n",
    "      └── overlap_period\n",
    "    \n",
    "    Dataset: shifts\n",
    "      └── shape\n",
    "      └── dtypes\n",
    "      └── missing_values\n",
    "      └── numeric_stats\n",
    "      └── shift_types\n",
    "      └── agent_types\n",
    "      └── hour_distribution\n",
    "      └── day_distribution\n",
    "      └── facility_stats\n",
    "      └── date_range\n",
    "      └── unique_ids\n",
    "    \n",
    "    Dataset: cross_validation\n",
    "      └── id_overlaps\n",
    "    \n",
    "    Would you like to see the complete contents of any specific dataset?\n",
    "    You can view them using:\n",
    "    explore_summary(summary.get_summary('dataset_name'))\n",
    "    \n",
    "    Or view all contents with:\n",
    "    explore_summary(summary)\n",
    "    \n",
    "\n",
    "# Initial Data Loading and Validation\n",
    "\n",
    "\n",
    "```python\n",
    "# === Load and Prepare All Datasets ===\n",
    "print(\"Loading and preparing all datasets...\")\n",
    "\n",
    "# Load all datasets\n",
    "shifts_df = pd.read_csv('data/cleveland_shifts_large.csv')\n",
    "bookings_df = pd.read_csv('data/booking_logs_large.csv')\n",
    "cancellations_df = pd.read_csv('data/cancel_logs_large.csv')\n",
    "\n",
    "def get_overlapping_date_range(shifts_df, bookings_df, cancellations_df):\n",
    "    \"\"\"\n",
    "    Determine the overlapping date range across all three datasets.\n",
    "    Returns the start and end dates that represent the period where we have complete data.\n",
    "    \n",
    "    The overlapping range is determined by:\n",
    "    - Latest start date among all datasets (to ensure we have data from all sources)\n",
    "    - Earliest end date among all datasets (to ensure we don't exceed any dataset's range)\n",
    "    \"\"\"\n",
    "    # Get date ranges for each dataset\n",
    "    shifts_range = {\n",
    "        'start': shifts_df['Created At'].min(),\n",
    "        'end': shifts_df['Created At'].max()\n",
    "    }\n",
    "    bookings_range = {\n",
    "        'start': bookings_df['Created At'].min(),\n",
    "        'end': bookings_df['Created At'].max()\n",
    "    }\n",
    "    cancellations_range = {\n",
    "        'start': cancellations_df['Created At'].min(),\n",
    "        'end': cancellations_df['Created At'].max()\n",
    "    }\n",
    "    \n",
    "    # Find overlapping range\n",
    "    overlap_start = max(\n",
    "        shifts_range['start'],\n",
    "        bookings_range['start'],\n",
    "        cancellations_range['start']\n",
    "    )\n",
    "    \n",
    "    overlap_end = min(\n",
    "        shifts_range['end'],\n",
    "        bookings_range['end'],\n",
    "        cancellations_range['end']\n",
    "    )\n",
    "    \n",
    "    return overlap_start, overlap_end\n",
    "\n",
    "def filter_to_overlap_period(df, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Filter a dataframe to only include rows within the overlapping date range.\n",
    "    \"\"\"\n",
    "    return df[\n",
    "        (df['Created At'] >= start_date) & \n",
    "        (df['Created At'] <= end_date)\n",
    "    ]\n",
    "\n",
    "# After your existing data loading code, add:\n",
    "# Find overlapping period\n",
    "overlap_start, overlap_end = get_overlapping_date_range(shifts_df, bookings_df, cancellations_df)\n",
    "\n",
    "# Filter all datasets to overlapping period\n",
    "shifts_df = filter_to_overlap_period(shifts_df, overlap_start, overlap_end)\n",
    "bookings_df = filter_to_overlap_period(bookings_df, overlap_start, overlap_end)\n",
    "cancellations_df = filter_to_overlap_period(cancellations_df, overlap_start, overlap_end)\n",
    "\n",
    "# Print information about the filtering\n",
    "print(\"\\n=== Data Filtering Summary ===\")\n",
    "print(f\"Analysis Period: {overlap_start} to {overlap_end}\")\n",
    "print(\"\\nDataset sizes after filtering:\")\n",
    "print(f\"Shifts: {len(shifts_df):,} records\")\n",
    "print(f\"Bookings: {len(bookings_df):,} records\")\n",
    "print(f\"Cancellations: {len(cancellations_df):,} records\")\n",
    "\n",
    "# Store filtering info in summary\n",
    "summary.add_summary('data_filtering', 'overlap_period', {\n",
    "    'start': overlap_start,\n",
    "    'end': overlap_end,\n",
    "    'original_sizes': {\n",
    "        'shifts': len(shifts_df),\n",
    "        'bookings': len(bookings_df),\n",
    "        'cancellations': len(cancellations_df)\n",
    "    }\n",
    "})\n",
    "```\n",
    "\n",
    "    Loading and preparing all datasets...\n",
    "    \n",
    "    === Data Filtering Summary ===\n",
    "    Analysis Period: 2021-09-06 11:06:36 to 2022-04-04 19:50:32\n",
    "    \n",
    "    Dataset sizes after filtering:\n",
    "    Shifts: 40,989 records\n",
    "    Bookings: 126,849 records\n",
    "    Cancellations: 78,056 records\n",
    "    \n",
    "\n",
    "\n",
    "```python\n",
    "# Clean and prepare the data\n",
    "shifts_df = load_and_clean_shifts(shifts_df)\n",
    "bookings_df = load_and_clean_bookings(bookings_df)\n",
    "cancellations_df = load_and_clean_cancellations(cancellations_df)\n",
    "\n",
    "# Function to analyze and summarize a dataset\n",
    "def analyze_dataset(df, dataset_name, summary):\n",
    "    print(f\"\\n=== {dataset_name} Data Overview ===\")\n",
    "    print(\"Dataset Shape:\", df.shape)\n",
    "    print(\"\\nColumns:\", df.columns.tolist())\n",
    "    print(\"\\nData Types:\\n\", df.dtypes)\n",
    "    \n",
    "    # Missing value analysis\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(\"\\nMissing Values:\\n\", missing_values)\n",
    "    \n",
    "    # Display sample data\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Store findings in summary\n",
    "    summary.add_summary(dataset_name.lower(), 'shape', df.shape)\n",
    "    summary.add_summary(dataset_name.lower(), 'dtypes', df.dtypes)\n",
    "    summary.add_summary(dataset_name.lower(), 'missing_values', missing_values)\n",
    "    \n",
    "    # Additional temporal analysis\n",
    "    date_range = {\n",
    "        'start_date': pd.to_datetime(df['Created At']).min(),\n",
    "        'end_date': pd.to_datetime(df['Created At']).max(),\n",
    "        'total_days': (pd.to_datetime(df['Created At']).max() - pd.to_datetime(df['Created At']).min()).days\n",
    "    }\n",
    "    summary.add_summary(dataset_name.lower(), 'date_range', date_range)\n",
    "    \n",
    "    # Record unique IDs count\n",
    "    unique_ids = df['ID'].nunique()\n",
    "    summary.add_summary(dataset_name.lower(), 'unique_ids', unique_ids)\n",
    "    \n",
    "    return date_range, unique_ids\n",
    "\n",
    "# Analyze each dataset\n",
    "datasets = {\n",
    "    'Shifts': shifts_df,\n",
    "    'Bookings': bookings_df,\n",
    "    'Cancellations': cancellations_df\n",
    "}\n",
    "\n",
    "print(\"\\n=== Dataset Analysis ===\")\n",
    "for name, df in datasets.items():\n",
    "    date_range, unique_ids = analyze_dataset(df, name, summary)\n",
    "    print(f\"\\n{name} Dataset Summary:\")\n",
    "    print(f\"Date Range: {date_range['start_date']} to {date_range['end_date']} ({date_range['total_days']} days)\")\n",
    "    print(f\"Unique IDs: {unique_ids}\")\n",
    "\n",
    "# Cross-dataset validation\n",
    "print(\"\\n=== Cross-Dataset Validation ===\")\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{name} Dataset:\")\n",
    "    print(f\"Total Records: {len(df)}\")\n",
    "    print(f\"Records per Day: {len(df) / (pd.to_datetime(df['Created At']).max() - pd.to_datetime(df['Created At']).min()).days:.2f}\")\n",
    "\n",
    "# Check for overlapping IDs between datasets\n",
    "print(\"\\n=== ID Overlap Analysis ===\")\n",
    "shifts_ids = set(shifts_df['ID'])\n",
    "bookings_ids = set(bookings_df['ID'])\n",
    "cancellations_ids = set(cancellations_df['ID'])\n",
    "\n",
    "overlap_analysis = {\n",
    "    'shifts_bookings': len(shifts_ids.intersection(bookings_ids)),\n",
    "    'shifts_cancellations': len(shifts_ids.intersection(cancellations_ids)),\n",
    "    'bookings_cancellations': len(bookings_ids.intersection(cancellations_ids))\n",
    "}\n",
    "\n",
    "summary.add_summary('cross_validation', 'id_overlaps', overlap_analysis)\n",
    "\n",
    "print(\"ID Overlaps:\")\n",
    "print(f\"Shifts-Bookings: {overlap_analysis['shifts_bookings']}\")\n",
    "print(f\"Shifts-Cancellations: {overlap_analysis['shifts_cancellations']}\")\n",
    "print(f\"Bookings-Cancellations: {overlap_analysis['bookings_cancellations']}\")\n",
    "```\n",
    "\n",
    "    \n",
    "    === Dataset Analysis ===\n",
    "    \n",
    "    === Shifts Data Overview ===\n",
    "    Dataset Shape: (40989, 16)\n",
    "    \n",
    "    Columns: ['ID', 'Agent ID', 'Facility ID', 'Start', 'Agent Req', 'End', 'Deleted', 'Shift Type', 'Created At', 'Verified', 'Charge', 'Time', 'Hour', 'Day', 'Month', 'Shift_Length']\n",
    "    \n",
    "    Data Types:\n",
    "     ID                      object\n",
    "    Agent ID                object\n",
    "    Facility ID             object\n",
    "    Start           datetime64[ns]\n",
    "    Agent Req               object\n",
    "    End             datetime64[ns]\n",
    "    Deleted                 object\n",
    "    Shift Type              object\n",
    "    Created At      datetime64[ns]\n",
    "    Verified                  bool\n",
    "    Charge                 float64\n",
    "    Time                   float64\n",
    "    Hour                     int32\n",
    "    Day                     object\n",
    "    Month                    int32\n",
    "    Shift_Length           float64\n",
    "    dtype: object\n",
    "    \n",
    "    Missing Values:\n",
    "     ID                  0\n",
    "    Agent ID        20010\n",
    "    Facility ID         0\n",
    "    Start               0\n",
    "    Agent Req           0\n",
    "    End                 0\n",
    "    Deleted         27028\n",
    "    Shift Type          0\n",
    "    Created At          0\n",
    "    Verified            0\n",
    "    Charge              0\n",
    "    Time                0\n",
    "    Hour                0\n",
    "    Day                 0\n",
    "    Month               0\n",
    "    Shift_Length        0\n",
    "    dtype: int64\n",
    "    \n",
    "    First few rows:\n",
    "                             ID                  Agent ID  \\\n",
    "    0  61732a2ad690c401690cf273  614627661afb050166fecd99   \n",
    "    1  61732a4fd690c401690cf307  60d5f4c8a9b88a0166aedaca   \n",
    "    2  61732ab7d6e633016acd05e2  5d7fb6319b671100167be1f1   \n",
    "    3  61732c33d6e633016acd11fc  613503b78b28c60166060efe   \n",
    "    4  61759081801438016ae23301  6099c93f2a957601669549c3   \n",
    "    \n",
    "                    Facility ID               Start Agent Req                 End  \\\n",
    "    0  5f9c169622c5c50016d5ba32 2021-10-27 23:00:00       LVN 2021-10-28 03:00:00   \n",
    "    1  5f9c169622c5c50016d5ba32 2021-10-28 11:00:00       CNA 2021-10-28 19:00:00   \n",
    "    2  5f9c169622c5c50016d5ba32 2021-10-30 19:00:00       LVN 2021-10-31 03:00:00   \n",
    "    3  5e6bd68c8d921f0016325d09 2021-10-25 18:30:00       CNA 2021-10-26 03:00:00   \n",
    "    4  61704cc1f19e4d0169ec4ac5 2021-11-03 02:00:00       CNA 2021-11-03 10:00:00   \n",
    "    \n",
    "      Deleted Shift Type          Created At  Verified  Charge  Time  Hour  \\\n",
    "    0     NaN         pm 2021-10-22 21:16:26      True  43.000 4.170    23   \n",
    "    1     NaN         am 2021-10-22 21:17:04      True  25.000 5.000    11   \n",
    "    2     NaN         pm 2021-10-22 21:18:48      True  64.750 8.000    19   \n",
    "    3     NaN         pm 2021-10-22 21:25:08      True  28.000 8.440    18   \n",
    "    4     NaN        noc 2021-10-24 16:57:37      True  30.000 7.500     2   \n",
    "    \n",
    "             Day  Month  Shift_Length  \n",
    "    0  Wednesday     10         4.000  \n",
    "    1   Thursday     10         8.000  \n",
    "    2   Saturday     10         8.000  \n",
    "    3     Monday     10         8.500  \n",
    "    4  Wednesday     11         8.000  \n",
    "    \n",
    "    Shifts Dataset Summary:\n",
    "    Date Range: 2021-09-07 01:25:57 to 2022-04-04 19:50:32 (209 days)\n",
    "    Unique IDs: 40989\n",
    "    \n",
    "    === Bookings Data Overview ===\n",
    "    Dataset Shape: (126849, 7)\n",
    "    \n",
    "    Columns: ['ID', 'Created At', 'Shift ID', 'Action', 'Worker ID', 'Facility ID', 'Lead Time']\n",
    "    \n",
    "    Data Types:\n",
    "     ID                     object\n",
    "    Created At     datetime64[ns]\n",
    "    Shift ID               object\n",
    "    Action                 object\n",
    "    Worker ID              object\n",
    "    Facility ID            object\n",
    "    Lead Time             float64\n",
    "    dtype: object\n",
    "    \n",
    "    Missing Values:\n",
    "     ID               0\n",
    "    Created At       0\n",
    "    Shift ID         0\n",
    "    Action           0\n",
    "    Worker ID      140\n",
    "    Facility ID      0\n",
    "    Lead Time        0\n",
    "    dtype: int64\n",
    "    \n",
    "    First few rows:\n",
    "                             ID          Created At                  Shift ID  \\\n",
    "    0  615f58b997538d018b1163e0 2021-10-07 20:29:46  615e1de54502b9016c9a5af1   \n",
    "    1  61608ce36790e5016acaf149 2021-10-08 18:24:35  616060338917ec016965f9a0   \n",
    "    2  61677eea7dce1d016a9b7f95 2021-10-14 00:50:51  61645d1dfdd396016a697466   \n",
    "    3  6166159a8186c5016a54dd6d 2021-10-12 23:09:15  616093710daae0016926f60d   \n",
    "    4  616391cec03fa3016a12a316 2021-10-11 01:22:23  614b6df64cd99701667bb099   \n",
    "    \n",
    "            Action                 Worker ID               Facility ID  Lead Time  \n",
    "    0  SHIFT_CLAIM  5c993c695d096c00167e6845  615b46a1c7135401876cdd06    144.504  \n",
    "    1  SHIFT_CLAIM  61587d5a2ced390187554556  5fbd2a01582f640016a4cb09    200.590  \n",
    "    2  SHIFT_CLAIM  60898a05cd5d9c016675a996  5c3cc2e3a2ae5c0016bef82c     73.653  \n",
    "    3  SHIFT_CLAIM  60ba35153c4b4f016610c5cd  60c1495470d2440166e23cb0    118.846  \n",
    "    4  SHIFT_CLAIM  60de429441198701669b529a  614900f003c26b0166e08737    178.627  \n",
    "    \n",
    "    Bookings Dataset Summary:\n",
    "    Date Range: 2021-09-06 01:19:57 to 2022-02-18 20:29:52 (165 days)\n",
    "    Unique IDs: 126849\n",
    "    \n",
    "    === Cancellations Data Overview ===\n",
    "    Dataset Shape: (78056, 8)\n",
    "    \n",
    "    Columns: ['ID', 'Created At', 'Shift ID', 'Action', 'Worker ID', 'Shift Start Logs', 'Facility ID', 'Lead Time']\n",
    "    \n",
    "    Data Types:\n",
    "     ID                          object\n",
    "    Created At          datetime64[ns]\n",
    "    Shift ID                    object\n",
    "    Action                      object\n",
    "    Worker ID                   object\n",
    "    Shift Start Logs    datetime64[ns]\n",
    "    Facility ID                 object\n",
    "    Lead Time                  float64\n",
    "    dtype: object\n",
    "    \n",
    "    Missing Values:\n",
    "     ID                    0\n",
    "    Created At            0\n",
    "    Shift ID              0\n",
    "    Action                0\n",
    "    Worker ID           191\n",
    "    Shift Start Logs      0\n",
    "    Facility ID           0\n",
    "    Lead Time             0\n",
    "    dtype: int64\n",
    "    \n",
    "    First few rows:\n",
    "                             ID          Created At                  Shift ID  \\\n",
    "    0  61a07227f36d1e0186381d10 2021-11-26 05:35:36  619d2f2a5db209018533a507   \n",
    "    1  617bb2e2ae50230185b05985 2021-10-29 08:37:55  614fa1ed22aa37018320e6ed   \n",
    "    2  61c90bb026c4c8018adc0820 2021-12-27 00:41:20  61ba75ff55eb55018586568b   \n",
    "    3  6186ce5dba1046018596162c 2021-11-06 18:50:06  6169aebc4dcbb2016aad106c   \n",
    "    4  618987925daf8001857ffd84 2021-11-08 20:24:50  6179b80d9c79750169c98f54   \n",
    "    \n",
    "                Action                 Worker ID    Shift Start Logs  \\\n",
    "    0  NO_CALL_NO_SHOW  5e1fa8d8170f34001633e511 2021-11-25 06:00:00   \n",
    "    1    WORKER_CANCEL  5cf573381648900016c41377 2021-10-29 13:00:00   \n",
    "    2    WORKER_CANCEL  61c24b6132278b018511e941 2021-12-27 23:45:00   \n",
    "    3    WORKER_CANCEL  60f9a88678a10501661b36d0 2021-11-13 12:00:00   \n",
    "    4    WORKER_CANCEL  615d413f21f03c016c7523ff 2021-11-23 20:00:00   \n",
    "    \n",
    "                    Facility ID  Lead Time  \n",
    "    0  6182c3fb79773801854c081d    -23.593  \n",
    "    1  5ff4f626909f7a00160d06fd      4.368  \n",
    "    2  5f9888997f5dee0016f777d4     23.061  \n",
    "    3  5f9b189a7ecb880016516a52    161.165  \n",
    "    4  5f9ad22ae3a95f0016090f97    359.586  \n",
    "    \n",
    "    Cancellations Dataset Summary:\n",
    "    Date Range: 2021-09-06 11:06:36 to 2022-04-01 19:06:44 (207 days)\n",
    "    Unique IDs: 78056\n",
    "    \n",
    "    === Cross-Dataset Validation ===\n",
    "    \n",
    "    Shifts Dataset:\n",
    "    Total Records: 40989\n",
    "    Records per Day: 196.12\n",
    "    \n",
    "    Bookings Dataset:\n",
    "    Total Records: 126849\n",
    "    Records per Day: 768.78\n",
    "    \n",
    "    Cancellations Dataset:\n",
    "    Total Records: 78056\n",
    "    Records per Day: 377.08\n",
    "    \n",
    "    === ID Overlap Analysis ===\n",
    "    ID Overlaps:\n",
    "    Shifts-Bookings: 0\n",
    "    Shifts-Cancellations: 0\n",
    "    Bookings-Cancellations: 0\n",
    "    \n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CBH_CS2)",
   "language": "python",
   "name": "cbh_cs2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
